[
    {
        "id": "181",
        "c_code": "#include <stdio.h>   void kmeans_set_zero(int *means, int size);  int main() {          int means[] = {1, 2, 3, 4, 5};     int size = sizeof(means) / sizeof(means[0]);           kmeans_set_zero(means, size);           printf(\"Array after setting all elements to zero:\\n\");     for (int i = 0; i < size; i++) {         printf(\"%d \", means[i]);     }      return 0; }   void kmeans_set_zero(int *means, int size) {     for (int id = 0; id < size; id++) {         means[id] = 0;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void kmeans_set_zero(int *means) {     means[blockIdx.x * blockDim.x + threadIdx.x] = 0; }  int main() {          const int data_size = 100;           int *means_host = (int *)malloc(data_size * sizeof(int));     for (int i = 0; i < data_size; ++i) {         means_host[i] = i;     }           int *means_device;     cudaMalloc((void**)&means_device, data_size * sizeof(int));     cudaMemcpy(means_device, means_host, data_size * sizeof(int), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((data_size + blockDim.x - 1) / blockDim.x);            kmeans_set_zero<<<gridDim, blockDim>>>(means_device);           cudaMemcpy(means_host, means_device, data_size * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < data_size; ++i) {         printf(\"%d \", means_host[i]);     }     printf(\"\\n\");           free(means_host);     cudaFree(means_device);      return 0; }   "
    },
    {
        "id": "182",
        "c_code": "#include <stdio.h>   void Mul_half_cpu(float *src, float *dst);  int main() {          float src[] = {2.0, 4.0, 6.0};     float dst[3];           Mul_half_cpu(src, dst);           printf(\"Array after multiplying each element by 0.5:\\n\");     for (int i = 0; i < 3; i++) {         printf(\"%f \", dst[i]);     }      return 0; }   void Mul_half_cpu(float *src, float *dst) {     for (int index = 0; index < 3; index++) {         dst[index] = src[index] * 0.5;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void Mul_half(float *src, float *dst) {     int index = threadIdx.x;     if (index < 3) {         dst[index] = src[index] * 0.5;     } }  int main() {          const int data_size = 3;           float *src_host = (float *)malloc(data_size * sizeof(float));     for (int i = 0; i < data_size; ++i) {         src_host[i] = i + 1.0;      }           float *src_device, *dst_device;     cudaMalloc((void**)&src_device, data_size * sizeof(float));     cudaMalloc((void**)&dst_device, data_size * sizeof(float));     cudaMemcpy(src_device, src_host, data_size * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim(1);               Mul_half<<<gridDim, blockDim>>>(src_device, dst_device);           float *dst_host = (float *)malloc(data_size * sizeof(float));     cudaMemcpy(dst_host, dst_device, data_size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < data_size; ++i) {         printf(\"%.2f \", dst_host[i]);     }     printf(\"\\n\");           free(src_host);     free(dst_host);     cudaFree(src_device);     cudaFree(dst_device);      return 0; }   "
    },
    {
        "id": "183",
        "c_code": "#include <stdio.h>   void resetIndices_cpu(long *vec_out, const long N);  int main() {          const long N = 5;     long vec_out[N];           resetIndices_cpu(vec_out, N);           printf(\"Array after resetting indices:\\n\");     for (long i = 0; i < N; i++) {         printf(\"%ld \", vec_out[i]);     }      return 0; }   void resetIndices_cpu(long *vec_out, const long N) {     for (long idx = 0; idx < N; idx++) {         vec_out[idx] = idx;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void resetIndices(long *vec_out, const long N) {     int idx = threadIdx.x + blockDim.x * blockIdx.x;     if (idx < N) {         vec_out[idx] = idx;     } }  int main() {          const long data_size = 100;           long *vec_out_device;     cudaMalloc((void**)&vec_out_device, data_size * sizeof(long));           dim3 blockDim(256);      dim3 gridDim((data_size + blockDim.x - 1) / blockDim.x);            resetIndices<<<gridDim, blockDim>>>(vec_out_device, data_size);           long *vec_out_host = (long *)malloc(data_size * sizeof(long));     cudaMemcpy(vec_out_host, vec_out_device, data_size * sizeof(long), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (long i = 0; i < data_size; ++i) {         printf(\"%ld \", vec_out_host[i]);     }     printf(\"\\n\");           free(vec_out_host);     cudaFree(vec_out_device);      return 0; }   "
    },
    {
        "id": "184",
        "c_code": "#include <stdio.h>   void set_offset_kernel(int stride, int size, int *output);  int main() {          int stride = 2;     int size = 5;     int output[size];           set_offset_kernel(stride, size, output);           printf(\"Array after setting offset values:\\n\");     for (int i = 0; i < size; i++) {         printf(\"%d \", output[i]);     }      return 0; }   void set_offset_kernel(int stride, int size, int *output) {     for (int i = 0; i < size; i++) {         output[i] = i * stride;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void set_offset_kernel(int stride, int size, int *output) {     for (int i = threadIdx.x; i < size; i += blockDim.x) {         output[i] = i * stride;     } }  int main() {          const int data_size = 100;     const int stride = 2;           int *output_device;     cudaMalloc((void**)&output_device, data_size * sizeof(int));           dim3 blockDim(256);      dim3 gridDim((data_size + blockDim.x - 1) / blockDim.x);            set_offset_kernel<<<gridDim, blockDim>>>(stride, data_size, output_device);           int *output_host = (int *)malloc(data_size * sizeof(int));     cudaMemcpy(output_host, output_device, data_size * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < data_size; ++i) {         printf(\"%d \", output_host[i]);     }     printf(\"\\n\");           free(output_host);     cudaFree(output_device);      return 0; }   "
    },
    {
        "id": "185",
        "c_code": "#include <stdio.h>   void setSuppressed_cpu(int *suppressed, int dims);  int main() {          int dims = 4;     int suppressed[dims];           setSuppressed_cpu(suppressed, dims);           printf(\"Array after setting all elements to zero:\\n\");     for (int i = 0; i < dims; i++) {         printf(\"%d \", suppressed[i]);     }      return 0; }   void setSuppressed_cpu(int *suppressed, int dims) {     for (int tid = 0; tid < dims; tid++) {         suppressed[tid] = 0;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void setSuppressed(int *suppressed, int dims) {     int tid = blockIdx.x * blockDim.x + threadIdx.x;     if (tid >= dims) {         return;     }     suppressed[tid] = 0; }  int main() {          const int data_size = 100;           int *suppressed_device;     cudaMalloc((void**)&suppressed_device, data_size * sizeof(int));           dim3 blockDim(256);      dim3 gridDim((data_size + blockDim.x - 1) / blockDim.x);            setSuppressed<<<gridDim, blockDim>>>(suppressed_device, data_size);           int *suppressed_host = (int *)malloc(data_size * sizeof(int));     cudaMemcpy(suppressed_host, suppressed_device, data_size * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < data_size; ++i) {         printf(\"%d \", suppressed_host[i]);     }     printf(\"\\n\");           free(suppressed_host);     cudaFree(suppressed_device);      return 0; }   "
    },
    {
        "id": "186",
        "c_code": "#include <stdio.h>   void allDivInplace_cpu(double *arr, double alpha, int n);  int main() {          int n = 6;     double arr[] = {2.0, 4.0, 6.0, 8.0, 10.0, 12.0};     double alpha = 2.0;           allDivInplace_cpu(arr, alpha, n);           printf(\"Array after dividing each element by %f:\\n\", alpha);     for (int i = 0; i < n; i++) {         printf(\"%f \", arr[i]);     }      return 0; }   void allDivInplace_cpu(double *arr, double alpha, int n) {     for (int i = 0; i < n; i++) {         arr[i] /= alpha;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void allDivInplaceKernel(double *arr, double alpha, int n) {     int i = blockIdx.x * blockDim.x + threadIdx.x;     if (i < n) {         arr[i] /= alpha;     } }  int main() {          const int data_size = 100;     const double alpha = 2.0;           double *arr_device;     cudaMalloc((void**)&arr_device, data_size * sizeof(double));           double *arr_host = (double *)malloc(data_size * sizeof(double));     for (int i = 0; i < data_size; ++i) {         arr_host[i] = i + 1.0;      }           cudaMemcpy(arr_device, arr_host, data_size * sizeof(double), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((data_size + blockDim.x - 1) / blockDim.x);            allDivInplaceKernel<<<gridDim, blockDim>>>(arr_device, alpha, data_size);           cudaMemcpy(arr_host, arr_device, data_size * sizeof(double), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < data_size; ++i) {         printf(\"%.2f \", arr_host[i]);     }     printf(\"\\n\");           free(arr_host);     cudaFree(arr_device);      return 0; }   "
    },
    {
        "id": "187",
        "c_code": "#include <stdio.h>   void incrementArrayOnHost(float *a, int N);  int main() {          int N = 5;     float a[] = {1.0, 2.0, 3.0, 4.0, 5.0};           incrementArrayOnHost(a, N);           printf(\"Array after incrementing each element by 1.0:\\n\");     for (int i = 0; i < N; i++) {         printf(\"%f \", a[i]);     }      return 0; }   void incrementArrayOnHost(float *a, int N) {     for (int i = 0; i < N; i++) {         a[i] = a[i] + 1.0f;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void incrementArrayOnDevice(float *a, int N) {     int idx = blockIdx.x * blockDim.x + threadIdx.x;     if (idx < N) {         a[idx] = a[idx] + 1.0f;     } }  int main() {          const int data_size = 100;           float *a_device;     cudaMalloc((void**)&a_device, data_size * sizeof(float));           float *a_host = (float *)malloc(data_size * sizeof(float));     for (int i = 0; i < data_size; ++i) {         a_host[i] = i + 1.0;      }           cudaMemcpy(a_device, a_host, data_size * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((data_size + blockDim.x - 1) / blockDim.x);            incrementArrayOnDevice<<<gridDim, blockDim>>>(a_device, data_size);           cudaMemcpy(a_host, a_device, data_size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < data_size; ++i) {         printf(\"%.2f \", a_host[i]);     }     printf(\"\\n\");           free(a_host);     cudaFree(a_device);      return 0; }   "
    },
    {
        "id": "188",
        "c_code": "#include <stdio.h>   void allMulInplace_cpu(double *arr, double alpha, int n);  int main() {          int n = 6;     double arr[] = {2.0, 4.0, 6.0, 8.0, 10.0, 12.0};     double alpha = 1.5;           allMulInplace_cpu(arr, alpha, n);           printf(\"Array after multiplying each element by %f:\\n\", alpha);     for (int i = 0; i < n; i++) {         printf(\"%f \", arr[i]);     }      return 0; }   void allMulInplace_cpu(double *arr, double alpha, int n) {     for (int i = 0; i < n; i++) {         arr[i] *= alpha;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void allMulInplaceKernel(double *arr, double alpha, int n) {     int i = blockIdx.x * blockDim.x + threadIdx.x;     if (i < n) {         arr[i] *= alpha;     } }  int main() {          const int data_size = 100;     const double alpha = 2.0;           double *arr_device;     cudaMalloc((void**)&arr_device, data_size * sizeof(double));           double *arr_host = (double *)malloc(data_size * sizeof(double));     for (int i = 0; i < data_size; ++i) {         arr_host[i] = i + 1.0;      }           cudaMemcpy(arr_device, arr_host, data_size * sizeof(double), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((data_size + blockDim.x - 1) / blockDim.x);            allMulInplaceKernel<<<gridDim, blockDim>>>(arr_device, alpha, data_size);           cudaMemcpy(arr_host, arr_device, data_size * sizeof(double), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < data_size; ++i) {         printf(\"%.2f \", arr_host[i]);     }     printf(\"\\n\");           free(arr_host);     cudaFree(arr_device);      return 0; }   "
    },
    {
        "id": "189",
        "c_code": "#include <stdio.h>   void Init(const long long size, const double *in, double *out);  int main() {          const long long size = 5;     double in[] = {1.0, 2.0, 3.0, 4.0, 5.0};     double out[size];           Init(size, in, out);           printf(\"Array after initialization:\\n\");     for (long long i = 0; i < size; i++) {         printf(\"%f \", out[i]);     }      return 0; }   void Init(const long long size, const double *in, double *out) {     for (long long i = 0; i < size; i++) {         out[i] = in[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void Init(const long long size, const double *in, double *out) {     int i = blockIdx.x * blockDim.x + threadIdx.x;     if (i < size) {         out[i] = in[i];     } }  int main() {          const long long data_size = 100;           double *in_device, *out_device;     cudaMalloc((void**)&in_device, data_size * sizeof(double));     cudaMalloc((void**)&out_device, data_size * sizeof(double));           double *in_host = (double *)malloc(data_size * sizeof(double));     for (long long i = 0; i < data_size; ++i) {         in_host[i] = i + 1.0;      }           cudaMemcpy(in_device, in_host, data_size * sizeof(double), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((data_size + blockDim.x - 1) / blockDim.x);            Init<<<gridDim, blockDim>>>(data_size, in_device, out_device);           double *out_host = (double *)malloc(data_size * sizeof(double));     cudaMemcpy(out_host, out_device, data_size * sizeof(double), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (long long i = 0; i < data_size; ++i) {         printf(\"%.2f \", out_host[i]);     }     printf(\"\\n\");           free(in_host);     free(out_host);     cudaFree(in_device);     cudaFree(out_device);      return 0; }   "
    },
    {
        "id": "190",
        "c_code": "#include <stdio.h>   void subAvg_cpu(int *input, int count, int avg);  int main() {          int count = 5;     int input[] = {2, 4, 6, 8, 10};     int avg = 6;           subAvg_cpu(input, count, avg);           printf(\"Array after subtracting average value %d:\\n\", avg);     for (int i = 0; i < count; i++) {         printf(\"%d \", input[i]);     }      return 0; }   void subAvg_cpu(int *input, int count, int avg) {     for (int index = 0; index < count; index++) {         input[index] = input[index] - avg;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void subAvg(int *input, int count, int avg) {     int index = blockDim.x * blockIdx.x + threadIdx.x;     if (index < count) {         input[index] = input[index] - avg;     } }  int main() {          const int data_size = 100;           int *input_device;     cudaMalloc((void**)&input_device, data_size * sizeof(int));           int *input_host = (int *)malloc(data_size * sizeof(int));     for (int i = 0; i < data_size; ++i) {         input_host[i] = i + 1;      }           cudaMemcpy(input_device, input_host, data_size * sizeof(int), cudaMemcpyHostToDevice);           int sum = 0;     for (int i = 0; i < data_size; ++i) {         sum += input_host[i];     }     int avg = sum / data_size;           dim3 blockDim(256);      dim3 gridDim((data_size + blockDim.x - 1) / blockDim.x);            subAvg<<<gridDim, blockDim>>>(input_device, data_size, avg);           cudaMemcpy(input_host, input_device, data_size * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < data_size; ++i) {         printf(\"%d \", input_host[i]);     }     printf(\"\\n\");           free(input_host);     cudaFree(input_device);      return 0; }   "
    },
    {
        "id": "191",
        "c_code": "#include <stdio.h>   void allExp2Inplace_cpu(double *arr, int n);  int main() {          int n = 4;     double arr[] = {1.0, 2.0, 3.0, 4.0};           allExp2Inplace_cpu(arr, n);           printf(\"Array after multiplying each element by 9:\\n\");     for (int i = 0; i < n; i++) {         printf(\"%f \", arr[i]);     }      return 0; }   void allExp2Inplace_cpu(double *arr, int n) {     for (int i = 0; i < n; i++) {         arr[i] = arr[i] * 9.0;     } }   ",
        "cuda_code": "#include <stdio.h> #include <math.h>   __global__ void allExp2InplaceKernel(double *arr, int n) {     int i = blockIdx.x * blockDim.x + threadIdx.x;     if (i < n) {         arr[i] = exp2(arr[i]) * 9.0;     } }  int main() {          const int data_size = 100;           double *arr_device;     cudaMalloc((void**)&arr_device, data_size * sizeof(double));           double *arr_host = (double *)malloc(data_size * sizeof(double));     for (int i = 0; i < data_size; ++i) {         arr_host[i] = i + 1.0;      }           cudaMemcpy(arr_device, arr_host, data_size * sizeof(double), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((data_size + blockDim.x - 1) / blockDim.x);            allExp2InplaceKernel<<<gridDim, blockDim>>>(arr_device, data_size);           cudaMemcpy(arr_host, arr_device, data_size * sizeof(double), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < data_size; ++i) {         printf(\"%.2f \", arr_host[i]);     }     printf(\"\\n\");           free(arr_host);     cudaFree(arr_device);      return 0; }   "
    },
    {
        "id": "192",
        "c_code": "#include <stdio.h>   void vector_add_cpu(float a[], float b[], float *c);  int main() {          float a[10000], b[10000], c[10000];           for (int i = 0; i < 10000; i++) {         a[i] = i;         b[i] = i * 2;     }           vector_add_cpu(a, b, c);           printf(\"Resultant array after vector addition:\\n\");     for (int i = 0; i < 10; i++) {         printf(\"%f \", c[i]);     }      return 0; }   void vector_add_cpu(float a[], float b[], float *c) {     for (int i = 0; i < 10000; i++) {         c[i] = a[i] + b[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void vector_add(float *a, float *b, float *c) {     int index = threadIdx.x + blockDim.x * blockIdx.x;     c[index] = a[index] + b[index]; }  int main() {          const int data_size = 100;           float *a_device, *b_device, *c_device;     cudaMalloc((void**)&a_device, data_size * sizeof(float));     cudaMalloc((void**)&b_device, data_size * sizeof(float));     cudaMalloc((void**)&c_device, data_size * sizeof(float));           float *a_host = (float *)malloc(data_size * sizeof(float));     float *b_host = (float *)malloc(data_size * sizeof(float));     for (int i = 0; i < data_size; ++i) {         a_host[i] = i + 1.0;          b_host[i] = (i + 1.0) * 2.0;      }           cudaMemcpy(a_device, a_host, data_size * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(b_device, b_host, data_size * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((data_size + blockDim.x - 1) / blockDim.x);            vector_add<<<gridDim, blockDim>>>(a_device, b_device, c_device);           float *c_host = (float *)malloc(data_size * sizeof(float));     cudaMemcpy(c_host, c_device, data_size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < data_size; ++i) {         printf(\"%.2f \", c_host[i]);     }     printf(\"\\n\");           free(a_host);     free(b_host);     free(c_host);     cudaFree(a_device);     cudaFree(b_device);     cudaFree(c_device);      return 0; }   "
    },
    {
        "id": "193",
        "c_code": "#include <stdio.h>   void setLabels_cpu(int *output, int dims, int clsNum);  int main() {          int dims = 10;     int clsNum = 3;     int output[10];           setLabels_cpu(output, dims, clsNum);           printf(\"Array after setting labels:\\n\");     for (int i = 0; i < dims; i++) {         printf(\"%d \", output[i]);     }      return 0; }   void setLabels_cpu(int *output, int dims, int clsNum) {     for (int tid = 0; tid < dims; tid++) {         output[tid] = tid % clsNum;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void setLabels(int *output, int dims, int clsNum) {     int tid = blockIdx.x * blockDim.x + threadIdx.x;     if (tid >= dims) {         return;     }     output[tid] = tid % clsNum; }  int main() {          const int data_size = 100;     const int clsNum = 5;           int *output_device;     cudaMalloc((void**)&output_device, data_size * sizeof(int));           dim3 blockDim(256);      dim3 gridDim((data_size + blockDim.x - 1) / blockDim.x);            setLabels<<<gridDim, blockDim>>>(output_device, data_size, clsNum);           int *output_host = (int *)malloc(data_size * sizeof(int));     cudaMemcpy(output_host, output_device, data_size * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < data_size; ++i) {         printf(\"%d \", output_host[i]);     }     printf(\"\\n\");           free(output_host);     cudaFree(output_device);      return 0; }   "
    },
    {
        "id": "194",
        "c_code": "#include <stdio.h>   void histogram_cpu(int n, int *color, int *bucket);  int main() {          int n = 8;     int color[] = {1, 2, 1, 3, 2, 3, 1, 2};     int bucket[4] = {0};            histogram_cpu(n, color, bucket);           printf(\"Histogram:\\n\");     for (int i = 0; i < 4; i++) {         printf(\"Bucket %d: %d\\n\", i, bucket[i]);     }      return 0; }   void histogram_cpu(int n, int *color, int *bucket) {     for (int i = 0; i < n; i++) {         int c = color[i];         bucket[c] += 1;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void histogram(int n, int *color, int *bucket) {     int i = threadIdx.x + blockDim.x * blockIdx.x;     if (i < n) {         int c = color[i];         atomicAdd(&bucket[c], 1);     } }  int main() {          const int data_size = 100;     const int num_bins = 256;            int *color_device, *bucket_device;     cudaMalloc((void**)&color_device, data_size * sizeof(int));     cudaMalloc((void**)&bucket_device, num_bins * sizeof(int));           int *color_host = (int *)malloc(data_size * sizeof(int));     for (int i = 0; i < data_size; ++i) {         color_host[i] = i % num_bins;      }           cudaMemcpy(color_device, color_host, data_size * sizeof(int), cudaMemcpyHostToDevice);           int *bucket_host = (int *)malloc(num_bins * sizeof(int));     memset(bucket_host, 0, num_bins * sizeof(int));            cudaMemcpy(bucket_device, bucket_host, num_bins * sizeof(int), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((data_size + blockDim.x - 1) / blockDim.x);            histogram<<<gridDim, blockDim>>>(data_size, color_device, bucket_device);           cudaMemcpy(bucket_host, bucket_device, num_bins * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Histogram Result after CUDA kernel execution:\\n\");     for (int i = 0; i < num_bins; ++i) {         printf(\"Bucket %d: %d\\n\", i, bucket_host[i]);     }           free(color_host);     free(bucket_host);     cudaFree(color_device);     cudaFree(bucket_device);      return 0; }   "
    },
    {
        "id": "195",
        "c_code": "#include <stdio.h> #include <math.h>   void sigmoid_kernel(float *input, float *output, int n);  int main() {          int n = 5;     float input[] = {1.0, 2.0, 3.0, 4.0, 5.0};     float output[5];           sigmoid_kernel(input, output, n);           printf(\"Array after applying sigmoid function:\\n\");     for (int i = 0; i < n; i++) {         printf(\"%f \", output[i]);     }      return 0; }   void sigmoid_kernel(float *input, float *output, int n) {     for (int tid = 0; tid < n; tid++) {         output[tid] = 1.0 / (1.0 + expf(-input[tid]));     } }   ",
        "cuda_code": "#include <stdio.h> #include <math.h>   __global__ void sigmoid_kernel(float *input, float *output) {     int tid = threadIdx.x + blockIdx.x * blockDim.x;     output[tid] = 1.0 / (1.0 + expf(-input[tid])); }  int main() {          const int data_size = 100;           float *input_device, *output_device;     cudaMalloc((void**)&input_device, data_size * sizeof(float));     cudaMalloc((void**)&output_device, data_size * sizeof(float));           float *input_host = (float *)malloc(data_size * sizeof(float));     for (int i = 0; i < data_size; ++i) {         input_host[i] = i + 1.0;      }           cudaMemcpy(input_device, input_host, data_size * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((data_size + blockDim.x - 1) / blockDim.x);            sigmoid_kernel<<<gridDim, blockDim>>>(input_device, output_device);           float *output_host = (float *)malloc(data_size * sizeof(float));     cudaMemcpy(output_host, output_device, data_size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < data_size; ++i) {         printf(\"%.6f \", output_host[i]);     }     printf(\"\\n\");           free(input_host);     free(output_host);     cudaFree(input_device);     cudaFree(output_device);      return 0; }   "
    },
    {
        "id": "196",
        "c_code": "#include <stdio.h>   void kernelUpdateHead(int *head, int *d_idxs_out, int n);  int main() {          int n = 5;     int head[10] = {0};      int d_idxs_out[] = {1, 3, 5, 7, 9};           kernelUpdateHead(head, d_idxs_out, n);           printf(\"Head array after updating:\\n\");     for (int i = 0; i < 10; i++) {         printf(\"%d \", head[i]);     }      return 0; }   void kernelUpdateHead(int *head, int *d_idxs_out, int n) {     for (int i = 0; i < n; i++) {         head[d_idxs_out[i]] = 1;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void kernelUpdateHead(int *head, int *d_idxs_out, int n) {     int i = threadIdx.x + blockDim.x * blockIdx.x;     if (i < n) {         head[d_idxs_out[i]] = 1;     } }  int main() {          const int data_size = 100;           int *head_device, *d_idxs_out_device;     cudaMalloc((void**)&head_device, data_size * sizeof(int));     cudaMalloc((void**)&d_idxs_out_device, data_size * sizeof(int));           int *d_idxs_out_host = (int *)malloc(data_size * sizeof(int));     for (int i = 0; i < data_size; ++i) {         d_idxs_out_host[i] = i % data_size;      }           cudaMemcpy(d_idxs_out_device, d_idxs_out_host, data_size * sizeof(int), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((data_size + blockDim.x - 1) / blockDim.x);            kernelUpdateHead<<<gridDim, blockDim>>>(head_device, d_idxs_out_device, data_size);           int *head_host = (int *)malloc(data_size * sizeof(int));     cudaMemcpy(head_host, head_device, data_size * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < data_size; ++i) {         printf(\"%d \", head_host[i]);     }     printf(\"\\n\");           free(d_idxs_out_host);     free(head_host);     cudaFree(d_idxs_out_device);     cudaFree(head_device);      return 0; }   "
    },
    {
        "id": "197",
        "c_code": "#include <stdio.h>   void const_cpu(int N, float ALPHA, float *X, int INCX);  int main() {          int N = 5;     float ALPHA = 2.0;     float X[] = {1.0, 2.0, 3.0, 4.0, 5.0};     int INCX = 1;           const_cpu(N, ALPHA, X, INCX);           printf(\"Array after setting each element to ALPHA:\\n\");     for (int i = 0; i < N; i++) {         printf(\"%f \", X[i]);     }      return 0; }   void const_cpu(int N, float ALPHA, float *X, int INCX) {     for (int i = 0; i < N; ++i) {         X[i * INCX] = ALPHA;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void const_kernel(int N, float ALPHA, float *X, int INCX) {     int i = (blockIdx.x + blockIdx.y * gridDim.x) * blockDim.x + threadIdx.x;     if (i < N) {         X[i * INCX] = ALPHA;     } }  int main() {          const int data_size = 100;     const float ALPHA = 2.0;            float *X_device;     cudaMalloc((void**)&X_device, data_size * sizeof(float));           dim3 blockDim(256);      dim3 gridDim((data_size + blockDim.x - 1) / blockDim.x);            const_kernel<<<gridDim, blockDim>>>(data_size, ALPHA, X_device, 1);            float *X_host = (float *)malloc(data_size * sizeof(float));     cudaMemcpy(X_host, X_device, data_size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < data_size; ++i) {         printf(\"%.2f \", X_host[i]);     }     printf(\"\\n\");           free(X_host);     cudaFree(X_device);      return 0; }   "
    },
    {
        "id": "198",
        "c_code": "#include <stdio.h>   void allLog2_cpu(const double *arr, double *buf, int n);  int main() {          int n = 4;     double arr[] = {2.0, 4.0, 8.0, 16.0};     double buf[4];           allLog2_cpu(arr, buf, n);           printf(\"Array after dividing each element by 2:\\n\");     for (int i = 0; i < n; i++) {         printf(\"%f \", buf[i]);     }      return 0; }   void allLog2_cpu(const double *arr, double *buf, int n) {     for (int i = 0; i < n; i++) {         buf[i] = arr[i] / 2.0;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void allLog2Kernel(const double *arr, double *buf, int n) {     int i = blockIdx.x * blockDim.x + threadIdx.x;     if (i < n) {         buf[i] = arr[i] / 2.0;     } }  int main() {          const int data_size = 100;           double *arr_device, *buf_device;     cudaMalloc((void**)&arr_device, data_size * sizeof(double));     cudaMalloc((void**)&buf_device, data_size * sizeof(double));           double *arr_host = (double *)malloc(data_size * sizeof(double));     for (int i = 0; i < data_size; ++i) {         arr_host[i] = i + 1.0;      }           cudaMemcpy(arr_device, arr_host, data_size * sizeof(double), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((data_size + blockDim.x - 1) / blockDim.x);            allLog2Kernel<<<gridDim, blockDim>>>(arr_device, buf_device, data_size);           double *buf_host = (double *)malloc(data_size * sizeof(double));     cudaMemcpy(buf_host, buf_device, data_size * sizeof(double), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < data_size; ++i) {         printf(\"%.2f \", buf_host[i]);     }     printf(\"\\n\");           free(arr_host);     free(buf_host);     cudaFree(arr_device);     cudaFree(buf_device);      return 0; }   "
    },
    {
        "id": "199",
        "c_code": "#include <stdio.h>   void clearArray_cpu(unsigned char *arr, const unsigned int length);  int main() {          const unsigned int length = 6;     unsigned char arr[] = {1, 2, 3, 4, 5, 6};           clearArray_cpu(arr, length);           printf(\"Array after clearing elements:\\n\");     for (unsigned int i = 0; i < length; i++) {         printf(\"%u \", arr[i]);     }      return 0; }   void clearArray_cpu(unsigned char *arr, const unsigned int length) {     unsigned int offset = 0;     while (offset < length) {         arr[offset] = 0;         offset += 1;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void clearArray(unsigned char *arr, const unsigned int length) {     unsigned int offset = blockDim.x * blockIdx.x + threadIdx.x;     unsigned int skip = gridDim.x * blockDim.x;          while (offset < length) {         arr[offset] = 0;         offset += skip;     } }  int main() {          const unsigned int data_size = 100;           unsigned char *arr_device;     cudaMalloc((void**)&arr_device, data_size * sizeof(unsigned char));           dim3 blockDim(256);      dim3 gridDim((data_size + blockDim.x - 1) / blockDim.x);            clearArray<<<gridDim, blockDim>>>(arr_device, data_size);           unsigned char *arr_host = (unsigned char *)malloc(data_size * sizeof(unsigned char));     cudaMemcpy(arr_host, arr_device, data_size * sizeof(unsigned char), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (unsigned int i = 0; i < data_size; ++i) {         printf(\"%d \", arr_host[i]);     }     printf(\"\\n\");           free(arr_host);     cudaFree(arr_device);      return 0; }   "
    },
    {
        "id": "200",
        "c_code": "#include <stdio.h>   void Copy_List_cpu(const int element_numbers, const float *origin_list, float *list);  int main() {          const int element_numbers = 5;     float origin_list[] = {1.0, 2.0, 3.0, 4.0, 5.0};     float list[5];           Copy_List_cpu(element_numbers, origin_list, list);           printf(\"Copied list:\\n\");     for (int i = 0; i < element_numbers; i++) {         printf(\"%f \", list[i]);     }      return 0; }   void Copy_List_cpu(const int element_numbers, const float *origin_list, float *list) {     for (int i = 0; i < element_numbers; i++) {         list[i] = origin_list[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void Copy_List(const int element_numbers, const float *origin_list, float *list) {     int i = blockDim.x * blockIdx.x + threadIdx.x;     if (i < element_numbers) {         list[i] = origin_list[i];     } }  int main() {          const int data_size = 100;           float *origin_list_device, *list_device;     cudaMalloc((void**)&origin_list_device, data_size * sizeof(float));     cudaMalloc((void**)&list_device, data_size * sizeof(float));           float *origin_list_host = (float *)malloc(data_size * sizeof(float));     for (int i = 0; i < data_size; ++i) {         origin_list_host[i] = i + 1.0;      }           cudaMemcpy(origin_list_device, origin_list_host, data_size * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((data_size + blockDim.x - 1) / blockDim.x);            Copy_List<<<gridDim, blockDim>>>(data_size, origin_list_device, list_device);           float *list_host = (float *)malloc(data_size * sizeof(float));     cudaMemcpy(list_host, list_device, data_size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < data_size; ++i) {         printf(\"%.2f \", list_host[i]);     }     printf(\"\\n\");           free(origin_list_host);     free(list_host);     cudaFree(origin_list_device);     cudaFree(list_device);      return 0; }   "
    },
    {
        "id": "201",
        "c_code": "#include <stdio.h>   void add(int n, float *x, float *y);  int main() {          int n = 3;     float x[] = {1.0, 2.0, 3.0};     float y[] = {4.0, 5.0, 6.0};           add(n, x, y);           printf(\"Array y after adding elements from x:\\n\");     for (int i = 0; i < n; i++) {         printf(\"%f \", y[i]);     }      return 0; }   void add(int n, float *x, float *y) {     for (int i = 0; i < n; i++) {         y[i] = x[i] + y[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void add(int n, float *x, float *y) {     int index = blockIdx.x * blockDim.x + threadIdx.x;     for (int i = index; i < n; i++) {         y[i] = x[i] + y[i];     } }  int main() {          const int data_size = 100;           float *x_device, *y_device;     cudaMalloc((void**)&x_device, data_size * sizeof(float));     cudaMalloc((void**)&y_device, data_size * sizeof(float));           float *x_host = (float *)malloc(data_size * sizeof(float));     float *y_host = (float *)malloc(data_size * sizeof(float));     for (int i = 0; i < data_size; ++i) {         x_host[i] = i + 1.0;          y_host[i] = (i + 1.0) * 2.0;      }           cudaMemcpy(x_device, x_host, data_size * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(y_device, y_host, data_size * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((data_size + blockDim.x - 1) / blockDim.x);            add<<<gridDim, blockDim>>>(data_size, x_device, y_device);           float *y_result = (float *)malloc(data_size * sizeof(float));     cudaMemcpy(y_result, y_device, data_size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < data_size; ++i) {         printf(\"%.2f \", y_result[i]);     }     printf(\"\\n\");           free(x_host);     free(y_host);     free(y_result);     cudaFree(x_device);     cudaFree(y_device);      return 0; }   "
    },
    {
        "id": "202",
        "c_code": "#include <stdio.h>   void host_add(float *c, float *a, float *b, int n);  int main() {          int n = 4;     float a[] = {1.0, 2.0, 3.0, 4.0};     float b[] = {5.0, 6.0, 7.0, 8.0};     float c[4];           host_add(c, a, b, n);           printf(\"Array c after adding elements from a and b:\\n\");     for (int i = 0; i < n; i++) {         printf(\"%f \", c[i]);     }      return 0; }   void host_add(float *c, float *a, float *b, int n) {     for (int k = 0; k < n; k++) {         c[k] = a[k] + b[k];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void gpu_add(float *c, float *a, float *b, int n) {     for (int k = threadIdx.x; k < n; k += blockDim.x) {         c[k] = a[k] + b[k];     } }  int main() {          const int data_size = 100;           float *a_device, *b_device, *c_device;     cudaMalloc((void**)&a_device, data_size * sizeof(float));     cudaMalloc((void**)&b_device, data_size * sizeof(float));     cudaMalloc((void**)&c_device, data_size * sizeof(float));           float *a_host = (float *)malloc(data_size * sizeof(float));     float *b_host = (float *)malloc(data_size * sizeof(float));     for (int i = 0; i < data_size; ++i) {         a_host[i] = i + 1.0;          b_host[i] = (i + 1.0) * 2.0;      }           cudaMemcpy(a_device, a_host, data_size * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(b_device, b_host, data_size * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim(1);            gpu_add<<<gridDim, blockDim>>>(c_device, a_device, b_device, data_size);           float *c_result = (float *)malloc(data_size * sizeof(float));     cudaMemcpy(c_result, c_device, data_size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < data_size; ++i) {         printf(\"%.2f \", c_result[i]);     }     printf(\"\\n\");           free(a_host);     free(b_host);     free(c_result);     cudaFree(a_device);     cudaFree(b_device);     cudaFree(c_device);      return 0; }   "
    },
    {
        "id": "203",
        "c_code": "#include <stdio.h>   void subtract_matrix(float *a, float *b, float *c, int N);  int main() {          int N = 9;     float a[] = {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0};     float b[] = {9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0};     float c[9];           subtract_matrix(a, b, c, N);           printf(\"Resultant matrix after subtraction:\\n\");     for (int i = 0; i < N; i++) {         printf(\"%f \", c[i]);         if ((i + 1) % 3 == 0) {             printf(\"\\n\");         }     }      return 0; }   void subtract_matrix(float *a, float *b, float *c, int N) {     for (int idx = 0; idx < N; idx++) {         c[idx] = a[idx] - b[idx];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void subtract_matrix(float *a, float *b, float *c, int N) {     int idx = blockIdx.x * blockDim.x + threadIdx.x;     if (idx < N) {         c[idx] = a[idx] - b[idx];     } }  int main() {          const int matrix_size = 100;           float *a_device, *b_device, *c_device;     cudaMalloc((void**)&a_device, matrix_size * sizeof(float));     cudaMalloc((void**)&b_device, matrix_size * sizeof(float));     cudaMalloc((void**)&c_device, matrix_size * sizeof(float));           float *a_host = (float *)malloc(matrix_size * sizeof(float));     float *b_host = (float *)malloc(matrix_size * sizeof(float));     for (int i = 0; i < matrix_size; ++i) {         a_host[i] = i + 1.0;          b_host[i] = (i + 1.0) * 2.0;      }           cudaMemcpy(a_device, a_host, matrix_size * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(b_device, b_host, matrix_size * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((matrix_size + blockDim.x - 1) / blockDim.x);            subtract_matrix<<<gridDim, blockDim>>>(a_device, b_device, c_device, matrix_size);           float *c_result = (float *)malloc(matrix_size * sizeof(float));     cudaMemcpy(c_result, c_device, matrix_size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < matrix_size; ++i) {         printf(\"%.2f \", c_result[i]);     }     printf(\"\\n\");           free(a_host);     free(b_host);     free(c_result);     cudaFree(a_device);     cudaFree(b_device);     cudaFree(c_device);      return 0; }   "
    },
    {
        "id": "204",
        "c_code": "#include <stdio.h>   void add_matrix_cpu(float *a, float *b, float *c, int N);  int main() {          int N = 9;     float a[] = {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0};     float b[] = {9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0};     float c[9];           add_matrix_cpu(a, b, c, N);           printf(\"Resultant matrix after addition:\\n\");     for (int i = 0; i < N; i++) {         printf(\"%f \", c[i]);         if ((i + 1) % 3 == 0) {             printf(\"\\n\");         }     }      return 0; }   void add_matrix_cpu(float *a, float *b, float *c, int N) {     for (int idx = 0; idx < N; idx++) {         c[idx] = a[idx] + b[idx];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void add_matrix(float *a, float *b, float *c, int N) {     int idx = blockIdx.x * blockDim.x + threadIdx.x;     if (idx < N) {         c[idx] = a[idx] + b[idx];     } }  int main() {          const int matrix_size = 100;           float *a_device, *b_device, *c_device;     cudaMalloc((void**)&a_device, matrix_size * sizeof(float));     cudaMalloc((void**)&b_device, matrix_size * sizeof(float));     cudaMalloc((void**)&c_device, matrix_size * sizeof(float));           float *a_host = (float *)malloc(matrix_size * sizeof(float));     float *b_host = (float *)malloc(matrix_size * sizeof(float));     for (int i = 0; i < matrix_size; ++i) {         a_host[i] = i + 1.0;          b_host[i] = (i + 1.0) * 2.0;      }           cudaMemcpy(a_device, a_host, matrix_size * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(b_device, b_host, matrix_size * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((matrix_size + blockDim.x - 1) / blockDim.x);            add_matrix<<<gridDim, blockDim>>>(a_device, b_device, c_device, matrix_size);           float *c_result = (float *)malloc(matrix_size * sizeof(float));     cudaMemcpy(c_result, c_device, matrix_size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < matrix_size; ++i) {         printf(\"%.2f \", c_result[i]);     }     printf(\"\\n\");           free(a_host);     free(b_host);     free(c_result);     cudaFree(a_device);     cudaFree(b_device);     cudaFree(c_device);      return 0; }   "
    },
    {
        "id": "205",
        "c_code": "#include <stdio.h>   void vecAdd_cpu(float *in1, float *in2, float *out, int len);  int main() {          int len = 5;     float in1[] = {1.0, 2.0, 3.0, 4.0, 5.0};     float in2[] = {5.0, 4.0, 3.0, 2.0, 1.0};     float out[5];           vecAdd_cpu(in1, in2, out, len);           printf(\"Resultant vector after addition:\\n\");     for (int i = 0; i < len; i++) {         printf(\"%f \", out[i]);     }      return 0; }   void vecAdd_cpu(float *in1, float *in2, float *out, int len) {     for (int i = 0; i < len; i++) {         out[i] = in1[i] + in2[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void vecAdd(float *in1, float *in2, float *out, int len) {     int i = threadIdx.x + blockDim.x * blockIdx.x;     if (i < len) {         out[i] = in1[i] + in2[i];     } }  int main() {          const int vector_size = 100;           float *in1_device, *in2_device, *out_device;     cudaMalloc((void**)&in1_device, vector_size * sizeof(float));     cudaMalloc((void**)&in2_device, vector_size * sizeof(float));     cudaMalloc((void**)&out_device, vector_size * sizeof(float));           float *in1_host = (float *)malloc(vector_size * sizeof(float));     float *in2_host = (float *)malloc(vector_size * sizeof(float));     for (int i = 0; i < vector_size; ++i) {         in1_host[i] = i + 1.0;          in2_host[i] = (i + 1.0) * 2.0;      }           cudaMemcpy(in1_device, in1_host, vector_size * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(in2_device, in2_host, vector_size * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((vector_size + blockDim.x - 1) / blockDim.x);            vecAdd<<<gridDim, blockDim>>>(in1_device, in2_device, out_device, vector_size);           float *out_result = (float *)malloc(vector_size * sizeof(float));     cudaMemcpy(out_result, out_device, vector_size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < vector_size; ++i) {         printf(\"%.2f \", out_result[i]);     }     printf(\"\\n\");           free(in1_host);     free(in2_host);     free(out_result);     cudaFree(in1_device);     cudaFree(in2_device);     cudaFree(out_device);      return 0; }   "
    },
    {
        "id": "206",
        "c_code": "#include <stdio.h>   void doubleArrayScalarAdd_cpu(double *d_in, double *d_out, int length, double scalar);  int main() {          int length = 5;     double d_in[] = {1.0, 2.0, 3.0, 4.0, 5.0};     double d_out[5];     double scalar = 10.0;           doubleArrayScalarAdd_cpu(d_in, d_out, length, scalar);           printf(\"Resultant array after adding scalar:\\n\");     for (int i = 0; i < length; i++) {         printf(\"%f \", d_out[i]);     }      return 0; }   void doubleArrayScalarAdd_cpu(double *d_in, double *d_out, int length, double scalar) {     for (int idx = 0; idx < length; idx++) {         d_out[idx] = d_in[idx] + scalar;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void doubleArrayScalarAddKernel(double *d_in, double *d_out, int length, double scalar) {     int tid = (blockIdx.x * blockDim.x) + threadIdx.x;     if (tid < length) {         d_out[tid] = d_in[tid] + scalar;     } }  int main() {          const int array_size = 100;           double *d_in_device, *d_out_device;     cudaMalloc((void**)&d_in_device, array_size * sizeof(double));     cudaMalloc((void**)&d_out_device, array_size * sizeof(double));           double *d_in_host = (double *)malloc(array_size * sizeof(double));     for (int i = 0; i < array_size; ++i) {         d_in_host[i] = i + 1.0;      }           cudaMemcpy(d_in_device, d_in_host, array_size * sizeof(double), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((array_size + blockDim.x - 1) / blockDim.x);            double scalar = 5.0;           doubleArrayScalarAddKernel<<<gridDim, blockDim>>>(d_in_device, d_out_device, array_size, scalar);           double *d_out_result = (double *)malloc(array_size * sizeof(double));     cudaMemcpy(d_out_result, d_out_device, array_size * sizeof(double), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < array_size; ++i) {         printf(\"%.2f \", d_out_result[i]);     }     printf(\"\\n\");           free(d_in_host);     free(d_out_result);     cudaFree(d_in_device);     cudaFree(d_out_device);      return 0; }   "
    },
    {
        "id": "207",
        "c_code": "#include <stdio.h>   void add_matrix_cpu(double *a, double *b, double *c, int N);  int main() {          int N = 9;     double a[] = {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0};     double b[] = {9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0};     double c[9];           add_matrix_cpu(a, b, c, N);           printf(\"Resultant matrix after addition:\\n\");     for (int i = 0; i < N; i++) {         printf(\"%f \", c[i]);         if ((i + 1) % 3 == 0) {             printf(\"\\n\");         }     }      return 0; }   void add_matrix_cpu(double *a, double *b, double *c, int N) {     for (int idx = 0; idx < N; idx++) {         c[idx] = a[idx] + b[idx];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void dadd_matrix(double *a, double *b, double *c, int N) {     int idx = blockIdx.x * blockDim.x + threadIdx.x;     if (idx < N) {         c[idx] = a[idx] + b[idx];     } }  int main() {          const int matrix_size = 100;           double *a_device, *b_device, *c_device;     cudaMalloc((void**)&a_device, matrix_size * sizeof(double));     cudaMalloc((void**)&b_device, matrix_size * sizeof(double));     cudaMalloc((void**)&c_device, matrix_size * sizeof(double));           double *a_host = (double *)malloc(matrix_size * sizeof(double));     double *b_host = (double *)malloc(matrix_size * sizeof(double));     for (int i = 0; i < matrix_size; ++i) {         a_host[i] = i + 1.0;          b_host[i] = (i + 1.0) * 2.0;      }           cudaMemcpy(a_device, a_host, matrix_size * sizeof(double), cudaMemcpyHostToDevice);     cudaMemcpy(b_device, b_host, matrix_size * sizeof(double), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((matrix_size + blockDim.x - 1) / blockDim.x);            dadd_matrix<<<gridDim, blockDim>>>(a_device, b_device, c_device, matrix_size);           double *c_result = (double *)malloc(matrix_size * sizeof(double));     cudaMemcpy(c_result, c_device, matrix_size * sizeof(double), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < matrix_size; ++i) {         printf(\"%.2f \", c_result[i]);     }     printf(\"\\n\");           free(a_host);     free(b_host);     free(c_result);     cudaFree(a_device);     cudaFree(b_device);     cudaFree(c_device);      return 0; }   "
    },
    {
        "id": "208",
        "c_code": "#include <stdio.h>   void test1_cpu(float *input, int dims);  int main() {          int dims = 5;     float input[] = {1.0, 2.0, 3.0, 4.0, 5.0, 0.0, 0.0, 0.0, 0.0, 10.0};           test1_cpu(input, dims);           printf(\"Resultant array after test1_cpu:\\n\");     for (int i = 0; i < dims * 4; i++) {         printf(\"%f \", input[i]);     }      return 0; }   void test1_cpu(float *input, int dims) {     for (int tid = 0; tid < dims; tid++) {         if (input[tid * 4] != 0) {             input[tid * 4] = 0;         }     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void test1(float *input, int dims) {     int tid = blockIdx.x * blockDim.x + threadIdx.x;     if (tid >= dims) {         return;     }     if (input[tid * 4] != 0) {         input[tid * 4] = 0;     } }  int main() {          const int array_size = 100;           float *input_device;     cudaMalloc((void**)&input_device, array_size * 4 * sizeof(float));           float *input_host = (float *)malloc(array_size * 4 * sizeof(float));     for (int i = 0; i < array_size * 4; ++i) {         input_host[i] = i + 1.0;      }           cudaMemcpy(input_device, input_host, array_size * 4 * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((array_size + blockDim.x - 1) / blockDim.x);            test1<<<gridDim, blockDim>>>(input_device, array_size);           float *output_result = (float *)malloc(array_size * 4 * sizeof(float));     cudaMemcpy(output_result, input_device, array_size * 4 * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < array_size * 4; ++i) {         printf(\"%.2f \", output_result[i]);     }     printf(\"\\n\");           free(input_host);     free(output_result);     cudaFree(input_device);      return 0; }   "
    },
    {
        "id": "209",
        "c_code": "#include <stdio.h>   #define VecSize 5   void vecAddCPU(double *pdbA, double *pdbB, double *pdbC);  int main() {          double pdbA[VecSize] = {1.0, 2.0, 3.0, 4.0, 5.0};     double pdbB[VecSize] = {5.0, 4.0, 3.0, 2.0, 1.0};     double pdbC[VecSize];           vecAddCPU(pdbA, pdbB, pdbC);           printf(\"Resultant vector after addition:\\n\");     for (int i = 0; i < VecSize; i++) {         printf(\"%f \", pdbC[i]);     }      return 0; }   void vecAddCPU(double *pdbA, double *pdbB, double *pdbC) {     for (int i = 0; i < VecSize; ++i) {         pdbC[i] = pdbA[i] + pdbB[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void vecAddGPU(double *pdbA, double *pdbB, double *pdbC) {     int i = blockDim.x * blockIdx.x + threadIdx.x;     pdbC[i] = pdbA[i] + pdbB[i]; }  int main() {          const int vector_size = 100;           double *pdbA_device, *pdbB_device, *pdbC_device;     cudaMalloc((void**)&pdbA_device, vector_size * sizeof(double));     cudaMalloc((void**)&pdbB_device, vector_size * sizeof(double));     cudaMalloc((void**)&pdbC_device, vector_size * sizeof(double));           double *pdbA_host = (double *)malloc(vector_size * sizeof(double));     double *pdbB_host = (double *)malloc(vector_size * sizeof(double));     for (int i = 0; i < vector_size; ++i) {         pdbA_host[i] = i + 1.0;          pdbB_host[i] = (i + 1.0) * 2.0;      }           cudaMemcpy(pdbA_device, pdbA_host, vector_size * sizeof(double), cudaMemcpyHostToDevice);     cudaMemcpy(pdbB_device, pdbB_host, vector_size * sizeof(double), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((vector_size + blockDim.x - 1) / blockDim.x);            vecAddGPU<<<gridDim, blockDim>>>(pdbA_device, pdbB_device, pdbC_device);           double *pdbC_result = (double *)malloc(vector_size * sizeof(double));     cudaMemcpy(pdbC_result, pdbC_device, vector_size * sizeof(double), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < vector_size; ++i) {         printf(\"%.2f \", pdbC_result[i]);     }     printf(\"\\n\");           free(pdbA_host);     free(pdbB_host);     free(pdbC_result);     cudaFree(pdbA_device);     cudaFree(pdbB_device);     cudaFree(pdbC_device);      return 0; }   "
    },
    {
        "id": "210",
        "c_code": "#include <stdio.h>   void doubleArrayScalarMultiply_cpu(double *d_in, double *d_out, int length, double scalar);  int main() {          int length = 5;     double d_in[] = {1.0, 2.0, 3.0, 4.0, 5.0};     double d_out[5];     double scalar = 2.0;           doubleArrayScalarMultiply_cpu(d_in, d_out, length, scalar);           printf(\"Resultant array after scalar multiplication:\\n\");     for (int i = 0; i < length; i++) {         printf(\"%f \", d_out[i]);     }      return 0; }   void doubleArrayScalarMultiply_cpu(double *d_in, double *d_out, int length, double scalar) {     for (int idx = 0; idx < length; idx++) {         d_out[idx] = d_in[idx] * scalar;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void doubleArrayScalarMultiplyKernel(double *d_in, double *d_out, int length, double scalar) {     int tid = (blockIdx.x * blockDim.x) + threadIdx.x;     if (tid < length) {         d_out[tid] = d_in[tid] * scalar;     } }  int main() {          const int array_size = 100;           double *d_in_device, *d_out_device;     cudaMalloc((void**)&d_in_device, array_size * sizeof(double));     cudaMalloc((void**)&d_out_device, array_size * sizeof(double));           double *d_in_host = (double *)malloc(array_size * sizeof(double));     for (int i = 0; i < array_size; ++i) {         d_in_host[i] = i + 1.0;      }           cudaMemcpy(d_in_device, d_in_host, array_size * sizeof(double), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((array_size + blockDim.x - 1) / blockDim.x);            double scalar = 2.0;           doubleArrayScalarMultiplyKernel<<<gridDim, blockDim>>>(d_in_device, d_out_device, array_size, scalar);           double *d_out_result = (double *)malloc(array_size * sizeof(double));     cudaMemcpy(d_out_result, d_out_device, array_size * sizeof(double), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < array_size; ++i) {         printf(\"%.2f \", d_out_result[i]);     }     printf(\"\\n\");           free(d_in_host);     free(d_out_result);     cudaFree(d_in_device);     cudaFree(d_out_device);      return 0; }   "
    },
    {
        "id": "211",
        "c_code": "#include <stdio.h>   void addV_cpu(int *a, int *b, int *c, int N);  int main() {          int N = 5;     int a[] = {1, 2, 3, 4, 5};     int b[] = {5, 4, 3, 2, 1};     int c[5];           addV_cpu(a, b, c, N);           printf(\"Resultant array after addition:\\n\");     for (int i = 0; i < N; i++) {         printf(\"%d \", c[i]);     }      return 0; }   void addV_cpu(int *a, int *b, int *c, int N) {     for (int index = 0; index < N; index++) {         c[index] = a[index] + b[index];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void addV(int *a, int *b, int *c, int N) {     int index = threadIdx.x + blockIdx.x * blockDim.x;     if (index < N) {         c[index] = a[index] + b[index];     } }  int main() {          const int array_size = 100;           int *a_device, *b_device, *c_device;     cudaMalloc((void**)&a_device, array_size * sizeof(int));     cudaMalloc((void**)&b_device, array_size * sizeof(int));     cudaMalloc((void**)&c_device, array_size * sizeof(int));           int *a_host = (int *)malloc(array_size * sizeof(int));     int *b_host = (int *)malloc(array_size * sizeof(int));     for (int i = 0; i < array_size; ++i) {         a_host[i] = i + 1;          b_host[i] = (i + 1) * 2;      }           cudaMemcpy(a_device, a_host, array_size * sizeof(int), cudaMemcpyHostToDevice);     cudaMemcpy(b_device, b_host, array_size * sizeof(int), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((array_size + blockDim.x - 1) / blockDim.x);            addV<<<gridDim, blockDim>>>(a_device, b_device, c_device, array_size);           int *c_result = (int *)malloc(array_size * sizeof(int));     cudaMemcpy(c_result, c_device, array_size * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < array_size; ++i) {         printf(\"%d \", c_result[i]);     }     printf(\"\\n\");           free(a_host);     free(b_host);     free(c_result);     cudaFree(a_device);     cudaFree(b_device);     cudaFree(c_device);      return 0; }   "
    },
    {
        "id": "212",
        "c_code": "#include <stdio.h>   void VecAdd_cpu(float *A, float *B, float *C, int N);  int main() {          int N = 5;     float A[] = {1.0, 2.0, 3.0, 4.0, 5.0};     float B[] = {5.0, 4.0, 3.0, 2.0, 1.0};     float C[5];           VecAdd_cpu(A, B, C, N);           printf(\"Resultant array after addition:\\n\");     for (int i = 0; i < N; i++) {         printf(\"%f \", C[i]);     }      return 0; }   void VecAdd_cpu(float *A, float *B, float *C, int N) {     for (int i = 0; i < N; i++) {         C[i] = A[i] + B[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void VecAdd(float *A, float *B, float *C, int N) {     int i = blockDim.x * blockIdx.x + threadIdx.x;     if (i < N) {         C[i] = A[i] + B[i];     } }  int main() {          const int array_size = 100;           float *A_device, *B_device, *C_device;     cudaMalloc((void**)&A_device, array_size * sizeof(float));     cudaMalloc((void**)&B_device, array_size * sizeof(float));     cudaMalloc((void**)&C_device, array_size * sizeof(float));           float *A_host = (float *)malloc(array_size * sizeof(float));     float *B_host = (float *)malloc(array_size * sizeof(float));     for (int i = 0; i < array_size; ++i) {         A_host[i] = i + 1.0;          B_host[i] = (i + 1.0) * 2.0;      }           cudaMemcpy(A_device, A_host, array_size * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(B_device, B_host, array_size * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((array_size + blockDim.x - 1) / blockDim.x);            VecAdd<<<gridDim, blockDim>>>(A_device, B_device, C_device, array_size);           float *C_result = (float *)malloc(array_size * sizeof(float));     cudaMemcpy(C_result, C_device, array_size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < array_size; ++i) {         printf(\"%.2f \", C_result[i]);     }     printf(\"\\n\");           free(A_host);     free(B_host);     free(C_result);     cudaFree(A_device);     cudaFree(B_device);     cudaFree(C_device);      return 0; }   "
    },
    {
        "id": "213",
        "c_code": "#include <stdio.h>   void saxpy_cpu(float *x, float *y, float alpha, int n);  int main() {          int n = 5;     float x[] = {1.0, 2.0, 3.0, 4.0, 5.0};     float y[] = {5.0, 4.0, 3.0, 2.0, 1.0};     float alpha = 2.0;           saxpy_cpu(x, y, alpha, n);           printf(\"Resultant vector after saxpy operation:\\n\");     for (int i = 0; i < n; i++) {         printf(\"%f \", y[i]);     }      return 0; }   void saxpy_cpu(float *x, float *y, float alpha, int n) {     for (int i = 0; i < n; i++) {         y[i] = alpha * x[i] + y[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void saxpy_gpu_kernel(float *x, float *y, float alpha, int n) {     int i = blockIdx.x * blockDim.x + threadIdx.x;     if (i < n) {         y[i] = alpha * x[i] + y[i];     } }  int main() {          const int array_size = 100;           float *x_device, *y_device;     cudaMalloc((void**)&x_device, array_size * sizeof(float));     cudaMalloc((void**)&y_device, array_size * sizeof(float));           float *x_host = (float *)malloc(array_size * sizeof(float));     float *y_host = (float *)malloc(array_size * sizeof(float));     for (int i = 0; i < array_size; ++i) {         x_host[i] = i + 1.0;          y_host[i] = (i + 1.0) * 2.0;      }           cudaMemcpy(x_device, x_host, array_size * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(y_device, y_host, array_size * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((array_size + blockDim.x - 1) / blockDim.x);            float alpha = 0.5;           saxpy_gpu_kernel<<<gridDim, blockDim>>>(x_device, y_device, alpha, array_size);           float *y_result = (float *)malloc(array_size * sizeof(float));     cudaMemcpy(y_result, y_device, array_size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < array_size; ++i) {         printf(\"%.2f \", y_result[i]);     }     printf(\"\\n\");           free(x_host);     free(y_host);     free(y_result);     cudaFree(x_device);     cudaFree(y_device);      return 0; }   "
    },
    {
        "id": "214",
        "c_code": "#include <stdio.h>   void sumArrays_cpu(float *A, float *B, float *C, const int N);  int main() {          const int N = 5;     float A[] = {1.0, 2.0, 3.0, 4.0, 5.0};     float B[] = {5.0, 4.0, 3.0, 2.0, 1.0};     float C[5];           sumArrays_cpu(A, B, C, N);           printf(\"Resultant array after sumArrays_cpu:\\n\");     for (int i = 0; i < N; i++) {         printf(\"%f \", C[i]);     }      return 0; }   void sumArrays_cpu(float *A, float *B, float *C, const int N) {     for (int i = 0; i < N; i++) {         C[i] = A[i] + B[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void sumArrays(float *A, float *B, float *C, const int N) {     int i = blockIdx.x * blockDim.x + threadIdx.x;     if (i < N) {         C[i] = A[i] + B[i];     } }  int main() {          const int array_size = 100;           float *A_device, *B_device, *C_device;     cudaMalloc((void**)&A_device, array_size * sizeof(float));     cudaMalloc((void**)&B_device, array_size * sizeof(float));     cudaMalloc((void**)&C_device, array_size * sizeof(float));           float *A_host = (float *)malloc(array_size * sizeof(float));     float *B_host = (float *)malloc(array_size * sizeof(float));     for (int i = 0; i < array_size; ++i) {         A_host[i] = i + 1.0;          B_host[i] = (i + 1.0) * 2.0;      }           cudaMemcpy(A_device, A_host, array_size * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(B_device, B_host, array_size * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((array_size + blockDim.x - 1) / blockDim.x);            sumArrays<<<gridDim, blockDim>>>(A_device, B_device, C_device, array_size);           float *C_result = (float *)malloc(array_size * sizeof(float));     cudaMemcpy(C_result, C_device, array_size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < array_size; ++i) {         printf(\"%.2f \", C_result[i]);     }     printf(\"\\n\");           free(A_host);     free(B_host);     free(C_result);     cudaFree(A_device);     cudaFree(B_device);     cudaFree(C_device);      return 0; }   "
    },
    {
        "id": "215",
        "c_code": "#include <stdio.h>   void doubleArrayScalarSubstract_cpu(double *d_in, double *d_out, int length, double scalar);  int main() {          int length = 5;     double d_in[] = {1.0, 2.0, 3.0, 4.0, 5.0};     double d_out[5];     double scalar = 2.0;           doubleArrayScalarSubstract_cpu(d_in, d_out, length, scalar);           printf(\"Resultant array after scalar subtraction:\\n\");     for (int i = 0; i < length; i++) {         printf(\"%f \", d_out[i]);     }      return 0; }   void doubleArrayScalarSubstract_cpu(double *d_in, double *d_out, int length, double scalar) {     for (int idx = 0; idx < length; idx++) {         d_out[idx] = d_in[idx] - scalar;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void doubleArrayScalarSubtractKernel(double *d_in, double *d_out, int length, double scalar) {     int tid = (blockIdx.x * blockDim.x) + threadIdx.x;     if (tid < length) {         d_out[tid] = d_in[tid] - scalar;     } }  int main() {          const int array_size = 100;           double *d_in_device, *d_out_device;     cudaMalloc((void**)&d_in_device, array_size * sizeof(double));     cudaMalloc((void**)&d_out_device, array_size * sizeof(double));           double *d_in_host = (double *)malloc(array_size * sizeof(double));     for (int i = 0; i < array_size; ++i) {         d_in_host[i] = i + 1.0;      }           cudaMemcpy(d_in_device, d_in_host, array_size * sizeof(double), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((array_size + blockDim.x - 1) / blockDim.x);            double scalar = 2.0;           doubleArrayScalarSubtractKernel<<<gridDim, blockDim>>>(d_in_device, d_out_device, array_size, scalar);           double *d_out_result = (double *)malloc(array_size * sizeof(double));     cudaMemcpy(d_out_result, d_out_device, array_size * sizeof(double), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < array_size; ++i) {         printf(\"%.2f \", d_out_result[i]);     }     printf(\"\\n\");           free(d_in_host);     free(d_out_result);     cudaFree(d_in_device);     cudaFree(d_out_device);      return 0; }   "
    },
    {
        "id": "216",
        "c_code": "#include <stdio.h>   void doubleArrayElementwiseSquare_cpu(double *d_in, double *d_out, int length);  int main() {          int length = 5;     double d_in[] = {1.0, 2.0, 3.0, 4.0, 5.0};     double d_out[5];           doubleArrayElementwiseSquare_cpu(d_in, d_out, length);           printf(\"Resultant array after elementwise square:\\n\");     for (int i = 0; i < length; i++) {         printf(\"%f \", d_out[i]);     }      return 0; }   void doubleArrayElementwiseSquare_cpu(double *d_in, double *d_out, int length) {     for (int idx = 0; idx < length; idx++) {         d_out[idx] = d_in[idx] * d_in[idx];     } }   ",
        "cuda_code": "#include <stdio.h> #include <math.h>   __global__ void doubleArrayElementwiseSquareKernel(double *d_in, double *d_out, int length) {     int tid = (blockIdx.x * blockDim.x) + threadIdx.x;     if (tid < length) {         d_out[tid] = pow(d_in[tid], 2);     } }  int main() {          const int array_size = 100;           double *d_in_device, *d_out_device;     cudaMalloc((void**)&d_in_device, array_size * sizeof(double));     cudaMalloc((void**)&d_out_device, array_size * sizeof(double));           double *d_in_host = (double *)malloc(array_size * sizeof(double));     for (int i = 0; i < array_size; ++i) {         d_in_host[i] = i + 1.0;      }           cudaMemcpy(d_in_device, d_in_host, array_size * sizeof(double), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((array_size + blockDim.x - 1) / blockDim.x);            doubleArrayElementwiseSquareKernel<<<gridDim, blockDim>>>(d_in_device, d_out_device, array_size);           double *d_out_result = (double *)malloc(array_size * sizeof(double));     cudaMemcpy(d_out_result, d_out_device, array_size * sizeof(double), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < array_size; ++i) {         printf(\"%.2f \", d_out_result[i]);     }     printf(\"\\n\");           free(d_in_host);     free(d_out_result);     cudaFree(d_in_device);     cudaFree(d_out_device);      return 0; }   "
    },
    {
        "id": "217",
        "c_code": "#include <stdio.h>   void sumArraysOnHostx(int *A, int *B, int *C, const int N);  int main() {          const int N = 5;     int A[] = {1, 2, 3, 4, 5};     int B[] = {5, 4, 3, 2, 1};     int C[5];           sumArraysOnHostx(A, B, C, N);           printf(\"Resultant array after sumArraysOnHostx:\\n\");     for (int i = 0; i < N; i++) {         printf(\"%d \", C[i]);     }      return 0; }   void sumArraysOnHostx(int *A, int *B, int *C, const int N) {     for (int idx = 0; idx < N; idx++) {         C[idx] = A[idx] + B[idx];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void sum_array_overlap(int *a, int *b, int *c, int N) {     int gid = blockIdx.x * blockDim.x + threadIdx.x;     if (gid < N) {         c[gid] = a[gid] + b[gid];     } }  int main() {          const int array_size = 100;           int *a_device, *b_device, *c_device;     cudaMalloc((void**)&a_device, array_size * sizeof(int));     cudaMalloc((void**)&b_device, array_size * sizeof(int));     cudaMalloc((void**)&c_device, array_size * sizeof(int));           int *a_host = (int *)malloc(array_size * sizeof(int));     int *b_host = (int *)malloc(array_size * sizeof(int));     for (int i = 0; i < array_size; ++i) {         a_host[i] = i + 1;          b_host[i] = (i + 1) * 2;      }           cudaMemcpy(a_device, a_host, array_size * sizeof(int), cudaMemcpyHostToDevice);     cudaMemcpy(b_device, b_host, array_size * sizeof(int), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((array_size + blockDim.x - 1) / blockDim.x);            sum_array_overlap<<<gridDim, blockDim>>>(a_device, b_device, c_device, array_size);           int *c_result = (int *)malloc(array_size * sizeof(int));     cudaMemcpy(c_result, c_device, array_size * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < array_size; ++i) {         printf(\"%d \", c_result[i]);     }     printf(\"\\n\");           free(a_host);     free(b_host);     free(c_result);     cudaFree(a_device);     cudaFree(b_device);     cudaFree(c_device);      return 0; }   "
    },
    {
        "id": "218",
        "c_code": "#include <stdio.h>   void k_vec_divide(float *vec1, float *vec2, int max_size);  int main() {          const int max_size = 5;     float vec1[] = {10.0, 20.0, 30.0, 40.0, 50.0};     float vec2[] = {2.0, 4.0, 5.0, 8.0, 10.0};           k_vec_divide(vec1, vec2, max_size);           printf(\"Resultant vector after elementwise division:\\n\");     for (int i = 0; i < max_size; i++) {         printf(\"%f \", vec1[i]);     }      return 0; }   void k_vec_divide(float *vec1, float *vec2, int max_size) {     for (int i = 0; i < max_size; i++) {         vec1[i] = vec1[i] / vec2[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void k_vec_divide(float *vec1, float *vec2, size_t max_size) {     for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < max_size; i += blockDim.x * gridDim.x) {         vec1[i] = vec1[i] / vec2[i];     } }  int main() {          const size_t array_size = 100;           float *vec1_device, *vec2_device;     cudaMalloc((void**)&vec1_device, array_size * sizeof(float));     cudaMalloc((void**)&vec2_device, array_size * sizeof(float));           float *vec1_host = (float *)malloc(array_size * sizeof(float));     float *vec2_host = (float *)malloc(array_size * sizeof(float));     for (size_t i = 0; i < array_size; ++i) {         vec1_host[i] = i + 1.0;          vec2_host[i] = (i + 1.0) * 2.0;      }           cudaMemcpy(vec1_device, vec1_host, array_size * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(vec2_device, vec2_host, array_size * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((array_size + blockDim.x - 1) / blockDim.x);            k_vec_divide<<<gridDim, blockDim>>>(vec1_device, vec2_device, array_size);           float *vec1_result = (float *)malloc(array_size * sizeof(float));     cudaMemcpy(vec1_result, vec1_device, array_size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (size_t i = 0; i < array_size; ++i) {         printf(\"%.2f \", vec1_result[i]);     }     printf(\"\\n\");           free(vec1_host);     free(vec2_host);     free(vec1_result);     cudaFree(vec1_device);     cudaFree(vec2_device);      return 0; }   "
    },
    {
        "id": "219",
        "c_code": "#include <stdio.h>   void saxpi_c(int n, float a, float *x, float *y);  int main() {          const int n = 5;     float a = 2.0;     float x[] = {1.0, 2.0, 3.0, 4.0, 5.0};     float y[] = {5.0, 4.0, 3.0, 2.0, 1.0};           saxpi_c(n, a, x, y);           printf(\"Resultant vector after saxpi_c operation:\\n\");     for (int i = 0; i < n; i++) {         printf(\"%f \", y[i]);     }      return 0; }   void saxpi_c(int n, float a, float *x, float *y) {     for (int i = 0; i < n; i++) {         y[i] = a * x[i] + y[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void saxpi_nBlock(int n, float a, float *x, float *y) {     int idx = threadIdx.x + (blockIdx.x * blockDim.x);     if (idx < n) {         y[idx] = a * x[idx] + y[idx];     } }  int main() {          const int array_size = 100;           float *x_device, *y_device;     cudaMalloc((void**)&x_device, array_size * sizeof(float));     cudaMalloc((void**)&y_device, array_size * sizeof(float));           float *x_host = (float *)malloc(array_size * sizeof(float));     float *y_host = (float *)malloc(array_size * sizeof(float));     for (int i = 0; i < array_size; ++i) {         x_host[i] = i + 1.0;          y_host[i] = (i + 1.0) * 2.0;      }           cudaMemcpy(x_device, x_host, array_size * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(y_device, y_host, array_size * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((array_size + blockDim.x - 1) / blockDim.x);            float a = 0.5;           saxpi_nBlock<<<gridDim, blockDim>>>(array_size, a, x_device, y_device);           float *y_result = (float *)malloc(array_size * sizeof(float));     cudaMemcpy(y_result, y_device, array_size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < array_size; ++i) {         printf(\"%.2f \", y_result[i]);     }     printf(\"\\n\");           free(x_host);     free(y_host);     free(y_result);     cudaFree(x_device);     cudaFree(y_device);      return 0; }   "
    },
    {
        "id": "220",
        "c_code": "#include <stdio.h>   void cpu_record(float *p, float *seis_kt, int *Gxz, int ng);  int main() {          const int ng = 5;     float p[] = {1.0, 2.0, 3.0, 4.0, 5.0};     int Gxz[] = {0, 2, 4, 1, 3};     float seis_kt[5];           cpu_record(p, seis_kt, Gxz, ng);           printf(\"Resultant array after cpu_record:\\n\");     for (int i = 0; i < ng; i++) {         printf(\"%f \", seis_kt[i]);     }      return 0; }   void cpu_record(float *p, float *seis_kt, int *Gxz, int ng) {     for (int id = 0; id < ng; id++) {         seis_kt[id] = p[Gxz[id]];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void cuda_record(float *p, float *seis_kt, int *Gxz, int ng) {     int id = threadIdx.x + blockDim.x * blockIdx.x;     if (id < ng) {         seis_kt[id] = p[Gxz[id]];     } }  int main() {          const int array_size = 100;           float *p_device, *seis_kt_device;     int *Gxz_device;      cudaMalloc((void**)&p_device, array_size * sizeof(float));     cudaMalloc((void**)&seis_kt_device, array_size * sizeof(float));     cudaMalloc((void**)&Gxz_device, array_size * sizeof(int));           float *p_host = (float *)malloc(array_size * sizeof(float));     float *seis_kt_host = (float *)malloc(array_size * sizeof(float));     int *Gxz_host = (int *)malloc(array_size * sizeof(int));      for (int i = 0; i < array_size; ++i) {         p_host[i] = i + 1.0;          seis_kt_host[i] = 0.0;          Gxz_host[i] = i % array_size;      }           cudaMemcpy(p_device, p_host, array_size * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(seis_kt_device, seis_kt_host, array_size * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(Gxz_device, Gxz_host, array_size * sizeof(int), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((array_size + blockDim.x - 1) / blockDim.x);            cuda_record<<<gridDim, blockDim>>>(p_device, seis_kt_device, Gxz_device, array_size);           float *seis_kt_result = (float *)malloc(array_size * sizeof(float));     cudaMemcpy(seis_kt_result, seis_kt_device, array_size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < array_size; ++i) {         printf(\"%.2f \", seis_kt_result[i]);     }     printf(\"\\n\");           free(p_host);     free(seis_kt_host);     free(Gxz_host);     free(seis_kt_result);     cudaFree(p_device);     cudaFree(seis_kt_device);     cudaFree(Gxz_device);      return 0; }   "
    },
    {
        "id": "221",
        "c_code": "#include <stdio.h>   void vectorDiv(const float *A, const float *B, float *C, int numElements);  int main() {          const int numElements = 5;     float A[] = {10.0, 20.0, 30.0, 40.0, 50.0};     float B[] = {2.0, 4.0, 5.0, 8.0, 10.0};     float C[5];           vectorDiv(A, B, C, numElements);           printf(\"Resultant array after elementwise division:\\n\");     for (int i = 0; i < numElements; i++) {         printf(\"%f \", C[i]);     }      return 0; }   void vectorDiv(const float *A, const float *B, float *C, int numElements) {     for (int i = 0; i < numElements; i++) {         C[i] = A[i] / B[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void vectorDiv(const float *A, const float *B, float *C, int numElements) {     int i = blockDim.x * blockIdx.x + threadIdx.x;     if (i < numElements) {         C[i] = A[i] / B[i];     } }  int main() {          const int array_size = 100;           float *A_device, *B_device, *C_device;      cudaMalloc((void**)&A_device, array_size * sizeof(float));     cudaMalloc((void**)&B_device, array_size * sizeof(float));     cudaMalloc((void**)&C_device, array_size * sizeof(float));           float *A_host = (float *)malloc(array_size * sizeof(float));     float *B_host = (float *)malloc(array_size * sizeof(float));     float *C_host = (float *)malloc(array_size * sizeof(float));      for (int i = 0; i < array_size; ++i) {         A_host[i] = i + 1.0;          B_host[i] = (i + 1.0) * 2.0;          C_host[i] = 0.0;      }           cudaMemcpy(A_device, A_host, array_size * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(B_device, B_host, array_size * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((array_size + blockDim.x - 1) / blockDim.x);            vectorDiv<<<gridDim, blockDim>>>(A_device, B_device, C_device, array_size);           cudaMemcpy(C_host, C_device, array_size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < array_size; ++i) {         printf(\"%.2f \", C_host[i]);     }     printf(\"\\n\");           free(A_host);     free(B_host);     free(C_host);     cudaFree(A_device);     cudaFree(B_device);     cudaFree(C_device);      return 0; }   "
    },
    {
        "id": "222",
        "c_code": "#include <stdio.h>   void cpuSAXPY(int len, float a, float *x, float *y);  int main() {          const int len = 5;     float a = 2.0;     float x[] = {1.0, 2.0, 3.0, 4.0, 5.0};     float y[] = {5.0, 4.0, 3.0, 2.0, 1.0};           cpuSAXPY(len, a, x, y);           printf(\"Resultant vector after cpuSAXPY operation:\\n\");     for (int i = 0; i < len; i++) {         printf(\"%f \", y[i]);     }      return 0; }   void cpuSAXPY(int len, float a, float *x, float *y) {     for (int i = 0; i < len; i++) {         y[i] = x[i] * a + y[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void kernelSAXPY(int len, float a, float *d_x, float *d_y) {     const int i = blockIdx.x * blockDim.x + threadIdx.x;     if (i < len) {         d_y[i] = d_x[i] * a + d_y[i];     } }  int main() {          const int array_size = 100;           float *d_x, *d_y;      cudaMalloc((void**)&d_x, array_size * sizeof(float));     cudaMalloc((void**)&d_y, array_size * sizeof(float));           float *h_x = (float *)malloc(array_size * sizeof(float));     float *h_y = (float *)malloc(array_size * sizeof(float));      for (int i = 0; i < array_size; ++i) {         h_x[i] = i + 1.0;          h_y[i] = (i + 1.0) * 2.0;      }           cudaMemcpy(d_x, h_x, array_size * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_y, h_y, array_size * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((array_size + blockDim.x - 1) / blockDim.x);            float a = 0.5;           kernelSAXPY<<<gridDim, blockDim>>>(array_size, a, d_x, d_y);           float *h_result = (float *)malloc(array_size * sizeof(float));     cudaMemcpy(h_result, d_y, array_size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < array_size; ++i) {         printf(\"%.2f \", h_result[i]);     }     printf(\"\\n\");           free(h_x);     free(h_y);     free(h_result);     cudaFree(d_x);     cudaFree(d_y);      return 0; }   "
    },
    {
        "id": "223",
        "c_code": "#include <stdio.h>   void vectorAdd(const float *A, const float *B, float *C, int numElements);  int main() {          const int numElements = 5;     float A[] = {1.0, 2.0, 3.0, 4.0, 5.0};     float B[] = {5.0, 4.0, 3.0, 2.0, 1.0};     float C[5];           vectorAdd(A, B, C, numElements);           printf(\"Resultant array after elementwise addition:\\n\");     for (int i = 0; i < numElements; i++) {         printf(\"%f \", C[i]);     }      return 0; }   void vectorAdd(const float *A, const float *B, float *C, int numElements) {     for (int i = 0; i < numElements; i++) {         C[i] = A[i] + B[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void vectorAdd(const float *A, const float *B, float *C, int numElements) {     int i = blockDim.x * blockIdx.x + threadIdx.x;     if (i < numElements) {         C[i] = A[i] + B[i];     } }  int main() {          const int array_size = 100;           float *A_device, *B_device, *C_device;      cudaMalloc((void**)&A_device, array_size * sizeof(float));     cudaMalloc((void**)&B_device, array_size * sizeof(float));     cudaMalloc((void**)&C_device, array_size * sizeof(float));           float *A_host = (float *)malloc(array_size * sizeof(float));     float *B_host = (float *)malloc(array_size * sizeof(float));      for (int i = 0; i < array_size; ++i) {         A_host[i] = i + 1.0;          B_host[i] = (i + 1.0) * 2.0;      }           cudaMemcpy(A_device, A_host, array_size * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(B_device, B_host, array_size * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockDim(256);      dim3 gridDim((array_size + blockDim.x - 1) / blockDim.x);            vectorAdd<<<gridDim, blockDim>>>(A_device, B_device, C_device, array_size);           float *C_host = (float *)malloc(array_size * sizeof(float));     cudaMemcpy(C_host, C_device, array_size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result after CUDA kernel execution:\\n\");     for (int i = 0; i < array_size; ++i) {         printf(\"%.2f \", C_host[i]);     }     printf(\"\\n\");           free(A_host);     free(B_host);     free(C_host);     cudaFree(A_device);     cudaFree(B_device);     cudaFree(C_device);      return 0; }   "
    },
    {
        "id": "224",
        "c_code": "#include <stdio.h>   void vectorAdd(double *a, double *b, double *c, int vector_size);  int main() {          const int vector_size = 5;     double a[] = {1.0, 2.0, 3.0, 4.0, 5.0};     double b[] = {5.0, 4.0, 3.0, 2.0, 1.0};     double c[5];           vectorAdd(a, b, c, vector_size);           printf(\"Resultant array after elementwise addition:\\n\");     for (int i = 0; i < vector_size; i++) {         printf(\"%f \", c[i]);     }      return 0; }   void vectorAdd(double *a, double *b, double *c, int vector_size) {     for (int idx = 0; idx < vector_size; idx++) {         c[idx] = a[idx] + b[idx];     } }   ",
        "cuda_code": "#include <iostream>   __global__ void vectorAdd(double* a, double* b, double* c, int vector_size) {     int tid = (blockIdx.x * blockDim.x) + threadIdx.x;     if (tid < vector_size) {         c[tid] = a[tid] + b[tid];     } }  int main() {          int vector_size = 100;            double *h_a, *h_b, *h_c;     h_a = new double[vector_size];     h_b = new double[vector_size];     h_c = new double[vector_size];           for (int i = 0; i < vector_size; ++i) {         h_a[i] = i;         h_b[i] = i * 2;     }           double *d_a, *d_b, *d_c;     cudaMalloc((void**)&d_a, vector_size * sizeof(double));     cudaMalloc((void**)&d_b, vector_size * sizeof(double));     cudaMalloc((void**)&d_c, vector_size * sizeof(double));           cudaMemcpy(d_a, h_a, vector_size * sizeof(double), cudaMemcpyHostToDevice);     cudaMemcpy(d_b, h_b, vector_size * sizeof(double), cudaMemcpyHostToDevice);           int block_size = 256;     int grid_size = (vector_size + block_size - 1) / block_size;           vectorAdd<<<grid_size, block_size>>>(d_a, d_b, d_c, vector_size);           cudaMemcpy(h_c, d_c, vector_size * sizeof(double), cudaMemcpyDeviceToHost);           for (int i = 0; i < vector_size; ++i) {         std::cout << h_c[i] << \" \";     }     std::cout << std::endl;           delete[] h_a;     delete[] h_b;     delete[] h_c;     cudaFree(d_a);     cudaFree(d_b);     cudaFree(d_c);      return 0; }   "
    },
    {
        "id": "225",
        "c_code": "#include <stdio.h>   void addIntValues(int *destination, int *value1, int *value2, unsigned int end);  int main() {          const unsigned int end = 5;     int value1[] = {1, 2, 3, 4, 5};     int value2[] = {5, 4, 3, 2, 1};     int destination[5];           addIntValues(destination, value1, value2, end);           printf(\"Resultant array after elementwise addition of integers:\\n\");     for (unsigned int i = 0; i < end; i++) {         printf(\"%d \", destination[i]);     }      return 0; }   void addIntValues(int *destination, int *value1, int *value2, unsigned int end) {     for (unsigned int i = 0; i < end; i++) {         destination[i] = value1[i] + value2[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void intAdd(int* c, const int* a, const int* b, const unsigned int d) {     int i = threadIdx.x + blockIdx.x * blockDim.x;     if (i < d) {         c[i] = a[i] + b[i];     } }  int main() {          unsigned int vector_size = 100;            int *h_a, *h_b, *h_c;     h_a = (int*)malloc(vector_size * sizeof(int));     h_b = (int*)malloc(vector_size * sizeof(int));     h_c = (int*)malloc(vector_size * sizeof(int));           for (unsigned int i = 0; i < vector_size; ++i) {         h_a[i] = i;         h_b[i] = i * 2;     }           int *d_a, *d_b, *d_c;     cudaMalloc((void**)&d_a, vector_size * sizeof(int));     cudaMalloc((void**)&d_b, vector_size * sizeof(int));     cudaMalloc((void**)&d_c, vector_size * sizeof(int));           cudaMemcpy(d_a, h_a, vector_size * sizeof(int), cudaMemcpyHostToDevice);     cudaMemcpy(d_b, h_b, vector_size * sizeof(int), cudaMemcpyHostToDevice);           int block_size = 256;     int grid_size = (vector_size + block_size - 1) / block_size;           intAdd<<<grid_size, block_size>>>(d_c, d_a, d_b, vector_size);           cudaMemcpy(h_c, d_c, vector_size * sizeof(int), cudaMemcpyDeviceToHost);           for (unsigned int i = 0; i < vector_size; ++i) {         printf(\"%d \", h_c[i]);     }     printf(\"\\n\");           free(h_a);     free(h_b);     free(h_c);     cudaFree(d_a);     cudaFree(d_b);     cudaFree(d_c);      return 0; }   "
    },
    {
        "id": "226",
        "c_code": "#include <stdio.h>   void histo_cpu(const unsigned int *const vals, unsigned int *const histo, int numVals);  int main() {          const int numVals = 10;     unsigned int vals[] = {1, 2, 3, 4, 5, 1, 2, 3, 4, 5};     unsigned int histo[6] = {0};           histo_cpu(vals, histo, numVals);           printf(\"Histogram result:\\n\");     for (int i = 0; i < 6; i++) {         printf(\"Value %d: %d occurrences\\n\", i, histo[i]);     }      return 0; }   void histo_cpu(const unsigned int *const vals, unsigned int *const histo, int numVals) {     for (int i = 0; i < numVals; i++) {         histo[vals[i]] = histo[vals[i]] + 1;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void histo_atomic(const unsigned int* const vals, unsigned int* const histo, int numVals) {     int i = threadIdx.x + blockIdx.x * blockDim.x;     if (i >= numVals)         return;     atomicAdd(&histo[vals[i]], 1); }  int main() {          int numVals = 100;            unsigned int* h_vals;     unsigned int* h_histo;     h_vals = (unsigned int*)malloc(numVals * sizeof(unsigned int));     h_histo = (unsigned int*)malloc(numVals * sizeof(unsigned int));           for (int i = 0; i < numVals; ++i) {         h_vals[i] = i % numVals;      }           unsigned int* d_vals;     unsigned int* d_histo;     cudaMalloc((void**)&d_vals, numVals * sizeof(unsigned int));     cudaMalloc((void**)&d_histo, numVals * sizeof(unsigned int));           cudaMemcpy(d_vals, h_vals, numVals * sizeof(unsigned int), cudaMemcpyHostToDevice);           cudaMemset(d_histo, 0, numVals * sizeof(unsigned int));           int block_size = 256;     dim3 grid_size((numVals + block_size - 1) / block_size, 1);           histo_atomic<<<grid_size, block_size>>>(d_vals, d_histo, numVals);           cudaMemcpy(h_histo, d_histo, numVals * sizeof(unsigned int), cudaMemcpyDeviceToHost);           for (int i = 0; i < numVals; ++i) {         printf(\"%u \", h_histo[i]);     }     printf(\"\\n\");           free(h_vals);     free(h_histo);     cudaFree(d_vals);     cudaFree(d_histo);      return 0; } "
    },
    {
        "id": "227",
        "c_code": "#include <stdio.h>   void vadd(const float *a, const float *b, float *c, const unsigned int count);  int main() {          const unsigned int count = 5;     float a[] = {1.0, 2.0, 3.0, 4.0, 5.0};     float b[] = {5.0, 4.0, 3.0, 2.0, 1.0};     float c[5];           vadd(a, b, c, count);           printf(\"Resultant array after elementwise addition:\\n\");     for (unsigned int i = 0; i < count; i++) {         printf(\"%f \", c[i]);     }      return 0; }   void vadd(const float *a, const float *b, float *c, const unsigned int count) {     for (unsigned int i = 0; i < count; i++) {         c[i] = a[i] + b[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void vadd(const float* a, const float* b, float* c, const unsigned int count) {     int i = blockDim.x * blockIdx.x + threadIdx.x;     if (i < count) {         c[i] = a[i] + b[i];     } }  int main() {          unsigned int vector_size = 100;            float *h_a, *h_b, *h_c;     h_a = (float*)malloc(vector_size * sizeof(float));     h_b = (float*)malloc(vector_size * sizeof(float));     h_c = (float*)malloc(vector_size * sizeof(float));           for (unsigned int i = 0; i < vector_size; ++i) {         h_a[i] = i * 1.5f;          h_b[i] = i * 2.0f;      }           float *d_a, *d_b, *d_c;     cudaMalloc((void**)&d_a, vector_size * sizeof(float));     cudaMalloc((void**)&d_b, vector_size * sizeof(float));     cudaMalloc((void**)&d_c, vector_size * sizeof(float));           cudaMemcpy(d_a, h_a, vector_size * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_b, h_b, vector_size * sizeof(float), cudaMemcpyHostToDevice);           int block_size = 256;     int grid_size = (vector_size + block_size - 1) / block_size;           vadd<<<grid_size, block_size>>>(d_a, d_b, d_c, vector_size);           cudaMemcpy(h_c, d_c, vector_size * sizeof(float), cudaMemcpyDeviceToHost);           for (unsigned int i = 0; i < vector_size; ++i) {         printf(\"%f \", h_c[i]);     }     printf(\"\\n\");           free(h_a);     free(h_b);     free(h_c);     cudaFree(d_a);     cudaFree(d_b);     cudaFree(d_c);      return 0; }   "
    },
    {
        "id": "228",
        "c_code": "#include <stdio.h>   void subtractIntValues(int *destination, int *value1, int *value2, unsigned int end);  int main() {          const unsigned int end = 5;     int value1[] = {10, 8, 6, 4, 2};     int value2[] = {5, 4, 3, 2, 1};     int destination[5];           subtractIntValues(destination, value1, value2, end);           printf(\"Resultant array after elementwise subtraction of integers:\\n\");     for (unsigned int i = 0; i < end; i++) {         printf(\"%d \", destination[i]);     }      return 0; }   void subtractIntValues(int *destination, int *value1, int *value2, unsigned int end) {     for (unsigned int i = 0; i < end; i++) {         destination[i] = value1[i] - value2[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void intSubtract(int* c, const int* a, const int* b, const unsigned int d) {     int i = threadIdx.x + blockIdx.x * blockDim.x;     if (i < d) {         c[i] = a[i] - b[i];     } }  int main() {          unsigned int vector_size = 100;            int *h_a, *h_b, *h_c;     h_a = (int*)malloc(vector_size * sizeof(int));     h_b = (int*)malloc(vector_size * sizeof(int));     h_c = (int*)malloc(vector_size * sizeof(int));           for (unsigned int i = 0; i < vector_size; ++i) {         h_a[i] = i * 2;          h_b[i] = i;          }           int *d_a, *d_b, *d_c;     cudaMalloc((void**)&d_a, vector_size * sizeof(int));     cudaMalloc((void**)&d_b, vector_size * sizeof(int));     cudaMalloc((void**)&d_c, vector_size * sizeof(int));           cudaMemcpy(d_a, h_a, vector_size * sizeof(int), cudaMemcpyHostToDevice);     cudaMemcpy(d_b, h_b, vector_size * sizeof(int), cudaMemcpyHostToDevice);           int block_size = 256;     int grid_size = (vector_size + block_size - 1) / block_size;           intSubtract<<<grid_size, block_size>>>(d_c, d_a, d_b, vector_size);           cudaMemcpy(h_c, d_c, vector_size * sizeof(int), cudaMemcpyDeviceToHost);           for (unsigned int i = 0; i < vector_size; ++i) {         printf(\"%d \", h_c[i]);     }     printf(\"\\n\");           free(h_a);     free(h_b);     free(h_c);     cudaFree(d_a);     cudaFree(d_b);     cudaFree(d_c);      return 0; }   "
    },
    {
        "id": "229",
        "c_code": "#include <stdio.h>   void transferMBR3_cpu(double *xy_copy, long long *a_copy, int tasks);  int main() {          const int tasks = 5;     double xy_copy[] = {1.5, 2.5, 3.5, 4.5, 5.5};     long long a_copy[5];           transferMBR3_cpu(xy_copy, a_copy, tasks);           printf(\"Resultant array after transferMBR3_cpu:\\n\");     for (int i = 0; i < tasks; i++) {         printf(\"%lld \", a_copy[i]);     }      return 0; }   void transferMBR3_cpu(double *xy_copy, long long *a_copy, int tasks) {     for (int i = 0; i < tasks; i++) {         a_copy[i] = xy_copy[i] * 10000000;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void transferMBR3(double* xy_copy, long long* a_copy, int tasks) {     for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < tasks; i += blockDim.x * gridDim.x) {         a_copy[i] = static_cast<long long>(xy_copy[i] * 10000000);     } }  int main() {          int tasks = 100;            double* h_xy_copy;     long long* h_a_copy;     h_xy_copy = (double*)malloc(tasks * sizeof(double));     h_a_copy = (long long*)malloc(tasks * sizeof(long long));           for (int i = 0; i < tasks; ++i) {         h_xy_copy[i] = i * 1.5;      }           double* d_xy_copy;     long long* d_a_copy;     cudaMalloc((void**)&d_xy_copy, tasks * sizeof(double));     cudaMalloc((void**)&d_a_copy, tasks * sizeof(long long));           cudaMemcpy(d_xy_copy, h_xy_copy, tasks * sizeof(double), cudaMemcpyHostToDevice);           int block_size = 256;     int grid_size = (tasks + block_size - 1) / block_size;           transferMBR3<<<grid_size, block_size>>>(d_xy_copy, d_a_copy, tasks);           cudaMemcpy(h_a_copy, d_a_copy, tasks * sizeof(long long), cudaMemcpyDeviceToHost);           for (int i = 0; i < tasks; ++i) {         printf(\"%lld \", h_a_copy[i]);     }     printf(\"\\n\");           free(h_xy_copy);     free(h_a_copy);     cudaFree(d_xy_copy);     cudaFree(d_a_copy);      return 0; }   "
    },
    {
        "id": "230",
        "c_code": "#include <stdio.h>   void binarize_cpu(float *input, int n, float *binary);  int main() {          const int n = 5;     float input[] = {1.0, -2.0, 3.0, -4.0, 5.0};     float binary[5];           binarize_cpu(input, n, binary);           printf(\"Resultant array after binarization:\\n\");     for (int i = 0; i < n; i++) {         printf(\"%f \", binary[i]);     }      return 0; }   void binarize_cpu(float *input, int n, float *binary) {     for (int i = 0; i < n; i++) {         binary[i] = (input[i] > 0) ? 1.0f : -1.0f;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void binarize_kernel(float* x, int n, float* binary) {     int i = (blockIdx.x + blockIdx.y * gridDim.x) * blockDim.x + threadIdx.x;     if (i >= n)         return;     binary[i] = (x[i] >= 0) ? 1 : -1; }  int main() {          int n = 100;            float* h_x;     float* h_binary;     h_x = (float*)malloc(n * sizeof(float));     h_binary = (float*)malloc(n * sizeof(float));           for (int i = 0; i < n; ++i) {         h_x[i] = i - 50;      }           float* d_x;     float* d_binary;     cudaMalloc((void**)&d_x, n * sizeof(float));     cudaMalloc((void**)&d_binary, n * sizeof(float));           cudaMemcpy(d_x, h_x, n * sizeof(float), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((n + block_size - 1) / block_size, 1);           binarize_kernel<<<grid_size, block_size>>>(d_x, n, d_binary);           cudaMemcpy(h_binary, d_binary, n * sizeof(float), cudaMemcpyDeviceToHost);           for (int i = 0; i < n; ++i) {         printf(\"%f \", h_binary[i]);     }     printf(\"\\n\");           free(h_x);     free(h_binary);     cudaFree(d_x);     cudaFree(d_binary);      return 0; }   "
    },
    {
        "id": "231",
        "c_code": "#include <stdio.h>   void add_vec_scalaire_cpu(int *vec, int *res, int a, long N);  int main() {          const long N = 5;     int vec[] = {1, 2, 3, 4, 5};     int res[5];     int scalar = 10;           add_vec_scalaire_cpu(vec, res, scalar, N);           printf(\"Resultant array after adding scalar to vector:\\n\");     for (long i = 0; i < N; i++) {         printf(\"%d \", res[i]);     }      return 0; }   void add_vec_scalaire_cpu(int *vec, int *res, int a, long N) {     for (long i = 0; i < N; i++) {         res[i] = vec[i] + a;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void add_vec_scalaire_gpu(int* vec, int* res, int a, long N) {     long i = (long)blockIdx.x * (long)blockDim.x + (long)threadIdx.x;     if (i < N) {         res[i] = vec[i] + a;     } }  int main() {          long N = 100;            int* h_vec;     int* h_res;     h_vec = (int*)malloc(N * sizeof(int));     h_res = (int*)malloc(N * sizeof(int));           for (long i = 0; i < N; ++i) {         h_vec[i] = i;      }           int* d_vec;     int* d_res;     cudaMalloc((void**)&d_vec, N * sizeof(int));     cudaMalloc((void**)&d_res, N * sizeof(int));           cudaMemcpy(d_vec, h_vec, N * sizeof(int), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((N + block_size - 1) / block_size, 1);           add_vec_scalaire_gpu<<<grid_size, block_size>>>(d_vec, d_res, 5, N);            cudaMemcpy(h_res, d_res, N * sizeof(int), cudaMemcpyDeviceToHost);           for (long i = 0; i < N; ++i) {         printf(\"%d \", h_res[i]);     }     printf(\"\\n\");           free(h_vec);     free(h_res);     cudaFree(d_vec);     cudaFree(d_res);      return 0; }     "
    },
    {
        "id": "232",
        "c_code": "#include <stdio.h> #include <string.h>   void memcpy_kernel(int *dst, int *src, int n);  int main() {          const int n = 5;     int src[] = {1, 2, 3, 4, 5};     int dst[5];           memcpy_kernel(dst, src, n * sizeof(int));           printf(\"Resultant array after memcpy_kernel:\\n\");     for (int i = 0; i < n; i++) {         printf(\"%d \", dst[i]);     }      return 0; }   void memcpy_kernel(int *dst, int *src, int n) {     memcpy(dst, src, n); }   ",
        "cuda_code": "#include <stdio.h>   __global__ void memcpy_kernel(int* dst, int* src, size_t n) {     int num = gridDim.x * blockDim.x;     int id = blockDim.x * blockIdx.x + threadIdx.x;     for (int i = id; i < n / sizeof(int); i += num) {         dst[i] = src[i];     } }  int main() {          size_t n = 100;            int* h_src;     int* h_dst;     h_src = (int*)malloc(n * sizeof(int));     h_dst = (int*)malloc(n * sizeof(int));           for (size_t i = 0; i < n; ++i) {         h_src[i] = i;      }           int* d_src;     int* d_dst;     cudaMalloc((void**)&d_src, n * sizeof(int));     cudaMalloc((void**)&d_dst, n * sizeof(int));           cudaMemcpy(d_src, h_src, n * sizeof(int), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((n + block_size - 1) / block_size, 1);           memcpy_kernel<<<grid_size, block_size>>>(d_dst, d_src, n * sizeof(int));           cudaMemcpy(h_dst, d_dst, n * sizeof(int), cudaMemcpyDeviceToHost);           for (size_t i = 0; i < n; ++i) {         printf(\"%d \", h_dst[i]);     }     printf(\"\\n\");           free(h_src);     free(h_dst);     cudaFree(d_src);     cudaFree(d_dst);      return 0; }   "
    },
    {
        "id": "233",
        "c_code": "#include <stdio.h>   void add_cpu(int N, int offset, float *X, int INCX);  int main() {          const int N = 5;     int offset = 10;     float X[] = {-130.0, -129.0, -128.0, -127.0, -126.0};           add_cpu(N, offset, X, 1);           printf(\"Resultant array after add_cpu:\\n\");     for (int i = 0; i < N; i++) {         printf(\"%f \", X[i]);     }      return 0; }   void add_cpu(int N, int offset, float *X, int INCX) {     for (int i = 0; i < N; i++) {         X[i * INCX] += offset;         if (X[i * INCX] == -128.0f) {             X[i * INCX] = -127.0f;         }     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void add_kernel(int N, float ALPHA, float* X, int INCX) {     int i = (blockIdx.x + blockIdx.y * gridDim.x) * blockDim.x + threadIdx.x;     if (i < N)         X[i * INCX] += ALPHA; }  int main() {          int N = 100;            float* h_X;     h_X = (float*)malloc(N * sizeof(float));           for (int i = 0; i < N; ++i) {         h_X[i] = i;      }           float* d_X;     cudaMalloc((void**)&d_X, N * sizeof(float));           cudaMemcpy(d_X, h_X, N * sizeof(float), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((N + block_size - 1) / block_size, 1);           add_kernel<<<grid_size, block_size>>>(N, 2.0f, d_X, 1);            cudaMemcpy(h_X, d_X, N * sizeof(float), cudaMemcpyDeviceToHost);           for (int i = 0; i < N; ++i) {         printf(\"%f \", h_X[i]);     }     printf(\"\\n\");           free(h_X);     cudaFree(d_X);      return 0; }   "
    },
    {
        "id": "234",
        "c_code": " #include <stdio.h>   void doubleArraySign_cpu(double *d_in, double *d_out, int length);  int main() {          const int length = 5;     double d_in[] = {-2.5, 0.0, 1.5, -3.0, 2.0};     double d_out[5];           doubleArraySign_cpu(d_in, d_out, length);           printf(\"Resultant array after doubleArraySign_cpu:\\n\");     for (int i = 0; i < length; i++) {         printf(\"%f \", d_out[i]);     }      return 0; }   void doubleArraySign_cpu(double *d_in, double *d_out, int length) {     for (int idx = 0; idx < length; idx++) {         d_out[idx] = (0 < d_in[idx]) - (d_in[idx] < 0);     } }  ",
        "cuda_code": "#include <stdio.h>   __global__ void doubleArraySignKernel(double* d_in, double* d_out, int length) {     int tid = (blockIdx.x * blockDim.x) + threadIdx.x;     if (tid < length) {         d_out[tid] = (0 < d_in[tid]) - (d_in[tid] < 0);     } }  int main() {          int length = 100;            double* h_d_in;     double* h_d_out;     h_d_in = (double*)malloc(length * sizeof(double));     h_d_out = (double*)malloc(length * sizeof(double));           for (int i = 0; i < length; ++i) {         h_d_in[i] = i - 50.0;      }           double* d_d_in;     double* d_d_out;     cudaMalloc((void**)&d_d_in, length * sizeof(double));     cudaMalloc((void**)&d_d_out, length * sizeof(double));           cudaMemcpy(d_d_in, h_d_in, length * sizeof(double), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((length + block_size - 1) / block_size, 1);           doubleArraySignKernel<<<grid_size, block_size>>>(d_d_in, d_d_out, length);           cudaMemcpy(h_d_out, d_d_out, length * sizeof(double), cudaMemcpyDeviceToHost);           for (int i = 0; i < length; ++i) {         printf(\"%f \", h_d_out[i]);     }     printf(\"\\n\");           free(h_d_in);     free(h_d_out);     cudaFree(d_d_in);     cudaFree(d_d_out);      return 0; }   "
    },
    {
        "id": "235",
        "c_code": "#include <stdio.h>   void find_max_cpu(int *data, int N);  int main() {          const int N = 5;     int data[] = {8, 3, 12, 5, 7};           find_max_cpu(data, N);           printf(\"Maximum value in the array: %d\\n\", data[0]);      return 0; }   void find_max_cpu(int *data, int N) {     int m = data[0];      for (int i = 0; i < N; i++) {         if (data[i] > m) {             m = data[i];         }     }      data[0] = m; }   ",
        "cuda_code": "#include <stdio.h>   __global__ void find_max_among_blocks(int* data, int blockSize, int nbBlocks) {     for (int i = 0; i < nbBlocks; ++i) {         if (data[0] < data[i * blockSize]) {             data[0] = data[i * blockSize];         }     } }  int main() {          int dataSize = 100;            int nbBlocks = 10;     int blockSize = dataSize / nbBlocks;           int* h_data;     h_data = (int*)malloc(dataSize * sizeof(int));           for (int i = 0; i < dataSize; ++i) {         h_data[i] = i;      }           int* d_data;     cudaMalloc((void**)&d_data, dataSize * sizeof(int));           cudaMemcpy(d_data, h_data, dataSize * sizeof(int), cudaMemcpyHostToDevice);           find_max_among_blocks<<<1, 1>>>(d_data, blockSize, nbBlocks);           cudaMemcpy(h_data, d_data, dataSize * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Maximum value among blocks: %d\\n\", h_data[0]);           free(h_data);     cudaFree(d_data);      return 0; }   "
    },
    {
        "id": "236",
        "c_code": "#include <stdio.h>   void setOffset_cpu(int *offset, int dims, int batchSize);  int main() {          const int dims = 3;     const int batchSize = 4;     int offset[5];            setOffset_cpu(offset, dims, batchSize);           printf(\"Resultant offset array:\\n\");     for (int i = 0; i <= batchSize; i++) {         printf(\"%d \", offset[i]);     }      return 0; }   void setOffset_cpu(int *offset, int dims, int batchSize) {     offset[0] = 0;      for (int i = 1; i <= batchSize; i++) {         offset[i] = i * dims;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void setOffset(int* offset, int dims, int batchSize) {     int tid = blockIdx.x * blockDim.x + threadIdx.x;     if (tid > 0) {         return;     }      offset[0] = 0;      for (int i = 1; i < batchSize + 1; i++) {         offset[i] = i * dims;     } }  int main() {          int batchSize = 5;     int dims = 3;           int* h_offset = (int*)malloc((batchSize + 1) * sizeof(int));           int* d_offset;     cudaMalloc((void**)&d_offset, (batchSize + 1) * sizeof(int));           setOffset<<<1, 1>>>(d_offset, dims, batchSize);           cudaMemcpy(h_offset, d_offset, (batchSize + 1) * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Offsets: \");     for (int i = 0; i <= batchSize; i++) {         printf(\"%d \", h_offset[i]);     }     printf(\"\\n\");           free(h_offset);     cudaFree(d_offset);      return 0; }   "
    },
    {
        "id": "237",
        "c_code": "#include <stdio.h>   void expandScoreFactors_cpu(const float *input, float *output, int dims, int clsNum);  int main() {          const int dims = 8;     const int clsNum = 2;     float input[] = {1.0, 2.0, 3.0, 4.0};     float output[8];            expandScoreFactors_cpu(input, output, dims, clsNum);           printf(\"Resultant expanded array:\\n\");     for (int i = 0; i < dims; i++) {         printf(\"%f \", output[i]);     }      return 0; }   void expandScoreFactors_cpu(const float *input, float *output, int dims, int clsNum) {     for (int tid = 0; tid < dims; tid++) {         int k = tid / clsNum;         output[tid] = input[k];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void expandScoreFactors(const float* input, float* output, int dims, int clsNum) {     int tid = blockIdx.x * blockDim.x + threadIdx.x;     if (tid >= dims) {         return;     }      int k = tid / clsNum;     output[tid] = input[k]; }  int main() {          int dims = 15;     int clsNum = 3;           float* h_input = (float*)malloc(clsNum * sizeof(float));     float* h_output = (float*)malloc(dims * sizeof(float));           for (int i = 0; i < clsNum; ++i) {         h_input[i] = static_cast<float>(i + 1);       }           float* d_input;     float* d_output;     cudaMalloc((void**)&d_input, clsNum * sizeof(float));     cudaMalloc((void**)&d_output, dims * sizeof(float));           cudaMemcpy(d_input, h_input, clsNum * sizeof(float), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((dims + block_size - 1) / block_size, 1);           expandScoreFactors<<<grid_size, block_size>>>(d_input, d_output, dims, clsNum);           cudaMemcpy(h_output, d_output, dims * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Expanded Output: \");     for (int i = 0; i < dims; ++i) {         printf(\"%f \", h_output[i]);     }     printf(\"\\n\");           free(h_input);     free(h_output);     cudaFree(d_input);     cudaFree(d_output);      return 0; }   "
    },
    {
        "id": "238",
        "c_code": "#include <stdio.h>   void kernelIsFirst_cpu(int *head, int *first_pts, int n);  int main() {          const int n = 5;     int head[] = {1, 0, 1, 0, 1};     int first_pts[5];           kernelIsFirst_cpu(head, first_pts, n);           printf(\"Resultant first_pts array:\\n\");     for (int i = 0; i < n; i++) {         printf(\"%d \", first_pts[i]);     }      return 0; }   void kernelIsFirst_cpu(int *head, int *first_pts, int n) {     for (int i = 0; i < n; i++) {         if (head[i] == 1) {             first_pts[i] = i;         } else {             first_pts[i] = 0;         }     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void kernelIsFirst(int* head, int* first_pts, int n) {     int i = threadIdx.x + blockDim.x * blockIdx.x;     if (i < n) {         if (head[i] == 1)             first_pts[i] = i;         else             first_pts[i] = 0;     } }  int main() {          int n = 10;            int* h_head = (int*)malloc(n * sizeof(int));     int* h_first_pts = (int*)malloc(n * sizeof(int));           for (int i = 0; i < n; ++i) {         h_head[i] = (i % 2 == 0) ? 1 : 0;       }           int* d_head;     int* d_first_pts;     cudaMalloc((void**)&d_head, n * sizeof(int));     cudaMalloc((void**)&d_first_pts, n * sizeof(int));           cudaMemcpy(d_head, h_head, n * sizeof(int), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((n + block_size - 1) / block_size, 1);           kernelIsFirst<<<grid_size, block_size>>>(d_head, d_first_pts, n);           cudaMemcpy(h_first_pts, d_first_pts, n * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"First Points: \");     for (int i = 0; i < n; ++i) {         printf(\"%d \", h_first_pts[i]);     }     printf(\"\\n\");           free(h_head);     free(h_first_pts);     cudaFree(d_head);     cudaFree(d_first_pts);      return 0; }   "
    },
    {
        "id": "239",
        "c_code": "#include <stdio.h>   void sumRowKernel_cpu(const int *d_in, int *d_out, int DIM);  int main() {          const int DIM = 4;     int d_in[] = {1, 2, 3, 4};     int d_out;           sumRowKernel_cpu(d_in, &d_out, DIM);           printf(\"Sum of elements in the row: %d\\n\", d_out);      return 0; }   void sumRowKernel_cpu(const int *d_in, int *d_out, int DIM) {     int sum = 0;      for (int i = 0; i < DIM; i++) {         sum += d_in[i];     }      *d_out = sum; }   ",
        "cuda_code": "#include <stdio.h>   __global__ void sumRowKernel(int* d_in, int* d_out, int DIM) {     for (int bid = blockIdx.x; bid < DIM; bid += gridDim.x) {         int sum = 0;         for (int tid = threadIdx.x; tid < DIM; tid += blockDim.x) {             sum += d_in[tid + bid * DIM];         }         atomicAdd(&d_out[bid], sum);     } }  int main() {          int DIM = 5;            int* h_in = (int*)malloc(DIM * DIM * sizeof(int));     int* h_out = (int*)malloc(DIM * sizeof(int));           for (int i = 0; i < DIM * DIM; ++i) {         h_in[i] = i + 1;      }           int* d_in;     int* d_out;     cudaMalloc((void**)&d_in, DIM * DIM * sizeof(int));     cudaMalloc((void**)&d_out, DIM * sizeof(int));           cudaMemcpy(d_in, h_in, DIM * DIM * sizeof(int), cudaMemcpyHostToDevice);           dim3 block_size(256);     dim3 grid_size((DIM + block_size.x - 1) / block_size.x);     sumRowKernel<<<grid_size, block_size>>>(d_in, d_out, DIM);           cudaMemcpy(h_out, d_out, DIM * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Sum of Rows: \");     for (int i = 0; i < DIM; ++i) {         printf(\"%d \", h_out[i]);     }     printf(\"\\n\");           free(h_in);     free(h_out);     cudaFree(d_in);     cudaFree(d_out);      return 0; }   "
    },
    {
        "id": "240",
        "c_code": "#include <stdio.h>   void addVectorsInto_cpu(float *result, const float *a, const float *b, int N);  int main() {          const int N = 5;     float a[] = {1.0, 2.0, 3.0, 4.0, 5.0};     float b[] = {5.0, 4.0, 3.0, 2.0, 1.0};     float result[5];           addVectorsInto_cpu(result, a, b, N);           printf(\"Resultant vector after addition:\\n\");     for (int i = 0; i < N; i++) {         printf(\"%f \", result[i]);     }      return 0; }   void addVectorsInto_cpu(float *result, const float *a, const float *b, int N) {     for (int i = 0; i < N; i++) {         result[i] = a[i] + b[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void addVectorsInto(float* result, float* a, float* b, int N) {     int index = threadIdx.x + blockIdx.x * blockDim.x;     int stride = blockDim.x * gridDim.x;      for (int i = index; i < N; i += stride) {         result[i] = a[i] + b[i];     } }  int main() {          int N = 100;            float* h_result = (float*)malloc(N * sizeof(float));     float* h_a = (float*)malloc(N * sizeof(float));     float* h_b = (float*)malloc(N * sizeof(float));           for (int i = 0; i < N; ++i) {         h_a[i] = i + 1;          h_b[i] = 2 * (i + 1);     }           float* d_result;     float* d_a;     float* d_b;     cudaMalloc((void**)&d_result, N * sizeof(float));     cudaMalloc((void**)&d_a, N * sizeof(float));     cudaMalloc((void**)&d_b, N * sizeof(float));           cudaMemcpy(d_a, h_a, N * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_b, h_b, N * sizeof(float), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((N + block_size - 1) / block_size, 1);           addVectorsInto<<<grid_size, block_size>>>(d_result, d_a, d_b, N);           cudaMemcpy(h_result, d_result, N * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result: \");     for (int i = 0; i < N; ++i) {         printf(\"%f \", h_result[i]);     }     printf(\"\\n\");           free(h_result);     free(h_a);     free(h_b);     cudaFree(d_result);     cudaFree(d_a);     cudaFree(d_b);      return 0; }   "
    },
    {
        "id": "241",
        "c_code": "#include <stdio.h>   void setIndexYolov3_cpu(int *input, int dims, int batchSize);  int main() {          const int dims = 3;     const int batchSize = 2;     int input[6];            setIndexYolov3_cpu(input, dims, batchSize);           printf(\"Resultant input array:\\n\");     for (int i = 0; i < batchSize; i++) {         for (int j = 0; j < dims; j++) {             printf(\"%d \", input[i * dims + j]);         }         printf(\"\\n\");     }      return 0; }   void setIndexYolov3_cpu(int *input, int dims, int batchSize) {     for (int tid = 0; tid < dims; tid++) {         for (int i = 0; i < batchSize; i++) {             input[i * dims + tid] = tid;         }     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void setIndexYolov3(int* input, int dims, int batchSize) {     int tid = blockIdx.x * blockDim.x + threadIdx.x;     if (tid >= dims) {         return;     }      for (int i = 0; i < batchSize; i++) {         input[i * dims + tid] = tid;     } }  int main() {          int dims = 5;     int batchSize = 3;           int* h_input = (int*)malloc(batchSize * dims * sizeof(int));           int* d_input;     cudaMalloc((void**)&d_input, batchSize * dims * sizeof(int));           dim3 block_size(256);     dim3 grid_size((dims + block_size.x - 1) / block_size.x);      setIndexYolov3<<<grid_size, block_size>>>(d_input, dims, batchSize);           cudaMemcpy(h_input, d_input, batchSize * dims * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Result:\\n\");     for (int i = 0; i < batchSize; ++i) {         for (int j = 0; j < dims; ++j) {             printf(\"%d \", h_input[i * dims + j]);         }         printf(\"\\n\");     }           free(h_input);     cudaFree(d_input);      return 0; }   "
    },
    {
        "id": "242",
        "c_code": "#include <stdio.h>   void shiftIndices(long *vec_out, const long by, const long imageSize, const long N);  int main() {          const long imageSize = 10;     const long N = 5;     const long by = 3;     long vec_out[5];           shiftIndices(vec_out, by, imageSize, N);           printf(\"Resultant vec_out array:\\n\");     for (int i = 0; i < N; i++) {         printf(\"%ld \", vec_out[i]);     }      return 0; }   void shiftIndices(long *vec_out, const long by, const long imageSize, const long N) {     for (int idx = 0; idx < N; idx++) {         vec_out[idx] = (imageSize + ((idx - N / 2 + by) % imageSize)) % imageSize;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void shiftIndices(long* vec_out, const long by, const long imageSize, const long N) {     long idx = threadIdx.x + blockDim.x * blockIdx.x;     if (idx < N) {         vec_out[idx] = (imageSize + ((idx - N / 2 + by) % imageSize)) % imageSize;     } }  int main() {          long N = 10;     long imageSize = 8;     long shiftAmount = 3;           long* h_vec_out = (long*)malloc(N * sizeof(long));           long* d_vec_out;     cudaMalloc((void**)&d_vec_out, N * sizeof(long));           dim3 block_size(256);     dim3 grid_size((N + block_size.x - 1) / block_size.x);      shiftIndices<<<grid_size, block_size>>>(d_vec_out, shiftAmount, imageSize, N);           cudaMemcpy(h_vec_out, d_vec_out, N * sizeof(long), cudaMemcpyDeviceToHost);           printf(\"Result:\\n\");     for (long i = 0; i < N; ++i) {         printf(\"%ld \", h_vec_out[i]);     }     printf(\"\\n\");           free(h_vec_out);     cudaFree(d_vec_out);      return 0; }   "
    },
    {
        "id": "243",
        "c_code": "#include <stdio.h>   void doubleArrayVectorSubtract_cpu(double *d_in_a, double *d_in_b, double *d_out, int length);  int main() {          const int length = 5;     double d_in_a[] = {1.0, 2.0, 3.0, 4.0, 5.0};     double d_in_b[] = {5.0, 4.0, 3.0, 2.0, 1.0};     double d_out[5];           doubleArrayVectorSubtract_cpu(d_in_a, d_in_b, d_out, length);           printf(\"Resultant d_out array:\\n\");     for (int i = 0; i < length; i++) {         printf(\"%f \", d_out[i]);     }      return 0; }   void doubleArrayVectorSubtract_cpu(double *d_in_a, double *d_in_b, double *d_out, int length) {     for (int idx = 0; idx < length; idx++) {         d_out[idx] = d_in_a[idx] - d_in_b[idx];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void doubleArrayVectorSubtractKernel(double* d_in_a, double* d_in_b, double* d_out, int length) {     int tid = blockIdx.x * blockDim.x + threadIdx.x;     if (tid < length) {         d_out[tid] = d_in_a[tid] - d_in_b[tid];     } }  int main() {          int length = 10;            double* h_in_a = (double*)malloc(length * sizeof(double));     double* h_in_b = (double*)malloc(length * sizeof(double));     double* h_out = (double*)malloc(length * sizeof(double));           for (int i = 0; i < length; ++i) {         h_in_a[i] = static_cast<double>(i + 1);          h_in_b[i] = static_cast<double>(i);     }           double* d_in_a;     double* d_in_b;     double* d_out;     cudaMalloc((void**)&d_in_a, length * sizeof(double));     cudaMalloc((void**)&d_in_b, length * sizeof(double));     cudaMalloc((void**)&d_out, length * sizeof(double));           cudaMemcpy(d_in_a, h_in_a, length * sizeof(double), cudaMemcpyHostToDevice);     cudaMemcpy(d_in_b, h_in_b, length * sizeof(double), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((length + block_size - 1) / block_size, 1);           doubleArrayVectorSubtractKernel<<<grid_size, block_size>>>(d_in_a, d_in_b, d_out, length);           cudaMemcpy(h_out, d_out, length * sizeof(double), cudaMemcpyDeviceToHost);           printf(\"Result: \");     for (int i = 0; i < length; ++i) {         printf(\"%f \", h_out[i]);     }     printf(\"\\n\");           free(h_in_a);     free(h_in_b);     free(h_out);     cudaFree(d_in_a);     cudaFree(d_in_b);     cudaFree(d_out);      return 0; }   "
    },
    {
        "id": "244",
        "c_code": "#include <stdio.h>   void doubleArrayVectorElementwiseMultiply_cpu(double *d_in_a, double *d_in_b, double *d_out, int length);  int main() {          const int length = 5;     double d_in_a[] = {1.0, 2.0, 3.0, 4.0, 5.0};     double d_in_b[] = {5.0, 4.0, 3.0, 2.0, 1.0};     double d_out[5];           doubleArrayVectorElementwiseMultiply_cpu(d_in_a, d_in_b, d_out, length);           printf(\"Resultant d_out array:\\n\");     for (int i = 0; i < length; i++) {         printf(\"%f \", d_out[i]);     }      return 0; }   void doubleArrayVectorElementwiseMultiply_cpu(double *d_in_a, double *d_in_b, double *d_out, int length) {     for (int idx = 0; idx < length; idx++) {         d_out[idx] = d_in_a[idx] * d_in_b[idx];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void doubleArrayVectorElementwiseMultiplyKernel(double* d_in_a, double* d_in_b, double* d_out, int length) {     int tid = blockIdx.x * blockDim.x + threadIdx.x;     if (tid < length) {         d_out[tid] = d_in_a[tid] * d_in_b[tid];     } }  int main() {          int length = 10;            double* h_in_a = (double*)malloc(length * sizeof(double));     double* h_in_b = (double*)malloc(length * sizeof(double));     double* h_out = (double*)malloc(length * sizeof(double));           for (int i = 0; i < length; ++i) {         h_in_a[i] = static_cast<double>(i + 1);          h_in_b[i] = static_cast<double>(i);     }           double* d_in_a;     double* d_in_b;     double* d_out;     cudaMalloc((void**)&d_in_a, length * sizeof(double));     cudaMalloc((void**)&d_in_b, length * sizeof(double));     cudaMalloc((void**)&d_out, length * sizeof(double));           cudaMemcpy(d_in_a, h_in_a, length * sizeof(double), cudaMemcpyHostToDevice);     cudaMemcpy(d_in_b, h_in_b, length * sizeof(double), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((length + block_size - 1) / block_size, 1);           doubleArrayVectorElementwiseMultiplyKernel<<<grid_size, block_size>>>(d_in_a, d_in_b, d_out, length);           cudaMemcpy(h_out, d_out, length * sizeof(double), cudaMemcpyDeviceToHost);           printf(\"Result: \");     for (int i = 0; i < length; ++i) {         printf(\"%f \", h_out[i]);     }     printf(\"\\n\");           free(h_in_a);     free(h_in_b);     free(h_out);     cudaFree(d_in_a);     cudaFree(d_in_b);     cudaFree(d_out);      return 0; }   "
    },
    {
        "id": "245",
        "c_code": "#include <stdio.h>   void fill_idx(int N, int *device_input, int *device_output);  int main() {          const int N = 6;     int device_input[] = {1, 2, 4, 5, 7, 8};     int device_output[8] = {0};            fill_idx(N, device_input, device_output);           printf(\"Resultant device_output array:\\n\");     for (int i = 0; i < N; i++) {         printf(\"%d \", device_output[i]);     }      return 0; }   void fill_idx(int N, int *device_input, int *device_output) {     int idx;     for (idx = 0; idx + 1 < N; idx++) {         if (device_input[idx] + 1 == device_input[idx + 1]) {             device_output[device_input[idx]] = idx;         }     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void fill_idx(int N, int* device_input, int* device_output) {     int idx = blockDim.x * blockIdx.x + threadIdx.x;     if (idx + 1 < N && device_input[idx] + 1 == device_input[idx + 1]) {         device_output[device_input[idx]] = idx;     } }  int main() {          int N = 10;            int* h_input = (int*)malloc(N * sizeof(int));     int* h_output = (int*)malloc(N * sizeof(int));           for (int i = 0; i < N; ++i) {         h_input[i] = i;      }           int* d_input;     int* d_output;     cudaMalloc((void**)&d_input, N * sizeof(int));     cudaMalloc((void**)&d_output, N * sizeof(int));           cudaMemcpy(d_input, h_input, N * sizeof(int), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((N + block_size - 1) / block_size, 1);           fill_idx<<<grid_size, block_size>>>(N, d_input, d_output);           cudaMemcpy(h_output, d_output, N * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Result: \");     for (int i = 0; i < N; ++i) {         printf(\"%d \", h_output[i]);     }     printf(\"\\n\");           free(h_input);     free(h_output);     cudaFree(d_input);     cudaFree(d_output);      return 0; }   "
    },
    {
        "id": "246",
        "c_code": "#include <stdio.h>   void cpuSearchPosShmem1EQ(int key, int *devKey, int *devPos, int size);  int main() {          const int size = 5;     int key = 3;     int devKey[] = {1, 2, 3, 4, 3};     int devPos[1] = {-1};            cpuSearchPosShmem1EQ(key, devKey, devPos, size);           printf(\"Position of key %d: %d\\n\", key, devPos[0]);      return 0; }   void cpuSearchPosShmem1EQ(int key, int *devKey, int *devPos, int size) {     for (int globalTx = 0; globalTx < size; globalTx++) {         if (devKey[globalTx] == key) {             devPos[0] = globalTx;         }     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void gpuSearchPosShmem1EQ(int key, int* devKey, int* devPos, int size) {     int globalTx = blockIdx.x * blockDim.x + threadIdx.x;     if (globalTx < size) {         if (devKey[globalTx] == key) {             devPos[0] = globalTx;         }     } }  int main() {          int size = 10;      int key = 5;              int* h_devKey = (int*)malloc(size * sizeof(int));     int* h_devPos = (int*)malloc(sizeof(int));           for (int i = 0; i < size; ++i) {         h_devKey[i] = i;      }           int* d_devKey;     int* d_devPos;     cudaMalloc((void**)&d_devKey, size * sizeof(int));     cudaMalloc((void**)&d_devPos, sizeof(int));           cudaMemcpy(d_devKey, h_devKey, size * sizeof(int), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((size + block_size - 1) / block_size, 1);           gpuSearchPosShmem1EQ<<<grid_size, block_size>>>(key, d_devKey, d_devPos, size);           cudaMemcpy(h_devPos, d_devPos, sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Position of key %d: %d\\n\", key, h_devPos[0]);           free(h_devKey);     free(h_devPos);     cudaFree(d_devKey);     cudaFree(d_devPos);      return 0; }   "
    },
    {
        "id": "247",
        "c_code": "#include <stdio.h>   void mathKernel1(float *c, int size);  int main() {          const int size = 5;     float c[5];           mathKernel1(c, size);           printf(\"Resultant array:\\n\");     for (int i = 0; i < size; i++) {         printf(\"%f \", c[i]);     }      return 0; }   void mathKernel1(float *c, int size) {     int tid;     float ia, ib;     ia = ib = 0.0f;      for (tid = 0; tid < size; tid++) {         if (tid % 2 == 0) {             ia = 100.0f;         } else {             ib = 200.0f;         }         c[tid] = ia + ib;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void mathKernel1(float* c) {     int tid = blockIdx.x * blockDim.x + threadIdx.x;     float ia, ib;     ia = ib = 0.0f;      if (tid % 2 == 0) {         ia = 100.0f;     } else {         ib = 200.0f;     }      c[tid] = ia + ib; }  int main() {          int size = 10;            float* h_c = (float*)malloc(size * sizeof(float));           float* d_c;     cudaMalloc((void**)&d_c, size * sizeof(float));           int block_size = 256;     dim3 grid_size((size + block_size - 1) / block_size, 1);           mathKernel1<<<grid_size, block_size>>>(d_c);           cudaMemcpy(h_c, d_c, size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result: \");     for (int i = 0; i < size; ++i) {         printf(\"%f \", h_c[i]);     }     printf(\"\\n\");           free(h_c);     cudaFree(d_c);      return 0; }   "
    },
    {
        "id": "248",
        "c_code": "#include <stdio.h>   void Reverse(int *d_in, int *d_out, int size);  int main() {          const int size = 5;     int d_in[] = {1, 2, 3, 4, 5};     int d_out[5];           Reverse(d_in, d_out, size);           printf(\"Reversed array:\\n\");     for (int i = 0; i < size; i++) {         printf(\"%d \", d_out[i]);     }      return 0; }   void Reverse(int *d_in, int *d_out, int size) {     for (int i = 0; i < size; i++) {         d_out[i] = d_in[size - 1 - i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void reverseArrayBlock(int* d_out, int* d_in) {     extern __shared__ int s_data[];      int i = blockIdx.x * blockDim.x + threadIdx.x;     int j = (gridDim.x - 1 - blockIdx.x) * blockDim.x + threadIdx.x;      s_data[blockDim.x - 1 - threadIdx.x] = d_in[i];      __syncthreads();      d_out[j] = s_data[threadIdx.x]; }  int main() {          int size = 10;            int* h_in = (int*)malloc(size * sizeof(int));     int* h_out = (int*)malloc(size * sizeof(int));           for (int i = 0; i < size; ++i) {         h_in[i] = i;      }           int* d_in;     int* d_out;     cudaMalloc((void**)&d_in, size * sizeof(int));     cudaMalloc((void**)&d_out, size * sizeof(int));           cudaMemcpy(d_in, h_in, size * sizeof(int), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((size + block_size - 1) / block_size, 1);           reverseArrayBlock<<<grid_size, block_size, block_size * sizeof(int)>>>(d_out, d_in);           cudaMemcpy(h_out, d_out, size * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Original Array: \");     for (int i = 0; i < size; ++i) {         printf(\"%d \", h_in[i]);     }     printf(\"\\n\");      printf(\"Reversed Array: \");     for (int i = 0; i < size; ++i) {         printf(\"%d \", h_out[i]);     }     printf(\"\\n\");           free(h_in);     free(h_out);     cudaFree(d_in);     cudaFree(d_out);      return 0; }   "
    },
    {
        "id": "249",
        "c_code": "#include <stdio.h>   void gaussianPass(int patchSize, int dataSize, float *gaussFilter, float *data);  int main() {          const int patchSize = 3;     const int dataSize = 9;     float gaussFilter[] = {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9};     float data[] = {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0};           gaussianPass(patchSize, dataSize, gaussFilter, data);           printf(\"Resultant data array:\\n\");     for (int i = 0; i < dataSize; i++) {         printf(\"%f \", data[i]);     }      return 0; }   void gaussianPass(int patchSize, int dataSize, float *gaussFilter, float *data) {     for (int i = 0; i < dataSize; i++) {         data[i] = gaussFilter[i % (patchSize * patchSize)] * data[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void gaussianPass(int patchSize, int dataSize, float* gaussFilter, float* data) {     int index = blockIdx.x * blockDim.x + threadIdx.x;     int stride = blockDim.x * gridDim.x;      for (int i = index; i < dataSize; i += stride) {         data[i] = gaussFilter[i % (patchSize * patchSize)] * data[i];     } }  int main() {          int dataSize = 1000;      int patchSize = 5;               float* h_gaussFilter = (float*)malloc(patchSize * patchSize * sizeof(float));     float* h_data = (float*)malloc(dataSize * sizeof(float));           for (int i = 0; i < patchSize * patchSize; ++i) {         h_gaussFilter[i] = static_cast<float>(i + 1);      }      for (int i = 0; i < dataSize; ++i) {         h_data[i] = static_cast<float>(i + 1);      }           float* d_gaussFilter;     float* d_data;     cudaMalloc((void**)&d_gaussFilter, patchSize * patchSize * sizeof(float));     cudaMalloc((void**)&d_data, dataSize * sizeof(float));           cudaMemcpy(d_gaussFilter, h_gaussFilter, patchSize * patchSize * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_data, h_data, dataSize * sizeof(float), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((dataSize + block_size - 1) / block_size, 1);           gaussianPass<<<grid_size, block_size>>>(patchSize, dataSize, d_gaussFilter, d_data);           cudaMemcpy(h_data, d_data, dataSize * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result: \");     for (int i = 0; i < dataSize; ++i) {         printf(\"%f \", h_data[i]);     }     printf(\"\\n\");           free(h_gaussFilter);     free(h_data);     cudaFree(d_gaussFilter);     cudaFree(d_data);      return 0; }   "
    },
    {
        "id": "250",
        "c_code": "#include <stdio.h>   void cpuAdd(int *a, int *b, int *c, int vectorSize);  int main() {          const int vectorSize = 5;     int a[] = {1, 2, 3, 4, 5};     int b[] = {5, 4, 3, 2, 1};     int c[5];           cpuAdd(a, b, c, vectorSize);           printf(\"Resultant array:\\n\");     for (int i = 0; i < vectorSize; i++) {         printf(\"%d \", c[i]);     }      return 0; }   void cpuAdd(int *a, int *b, int *c, int vectorSize) {     #pragma omp parallel for     for (int i = 0; i < vectorSize; i++) {         c[i] = a[i] + b[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void addKernel(int* a, int* b, int* c, int vectorSize, int elements_per_thread) {     int start = (blockIdx.x * blockDim.x + threadIdx.x) * elements_per_thread;      for (int i = start; i - start < elements_per_thread && (i < vectorSize); i++) {         c[i] = a[i] + b[i];     } }  int main() {          int vectorSize = 1000;             int elements_per_thread = 10;            int* h_a = (int*)malloc(vectorSize * sizeof(int));     int* h_b = (int*)malloc(vectorSize * sizeof(int));     int* h_c = (int*)malloc(vectorSize * sizeof(int));           for (int i = 0; i < vectorSize; ++i) {         h_a[i] = i;          h_b[i] = i * 2;     }           int* d_a;     int* d_b;     int* d_c;     cudaMalloc((void**)&d_a, vectorSize * sizeof(int));     cudaMalloc((void**)&d_b, vectorSize * sizeof(int));     cudaMalloc((void**)&d_c, vectorSize * sizeof(int));           cudaMemcpy(d_a, h_a, vectorSize * sizeof(int), cudaMemcpyHostToDevice);     cudaMemcpy(d_b, h_b, vectorSize * sizeof(int), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((vectorSize + block_size - 1) / block_size, 1);           addKernel<<<grid_size, block_size>>>(d_a, d_b, d_c, vectorSize, elements_per_thread);           cudaMemcpy(h_c, d_c, vectorSize * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Result: \");     for (int i = 0; i < vectorSize; ++i) {         printf(\"%d \", h_c[i]);     }     printf(\"\\n\");           free(h_a);     free(h_b);     free(h_c);     cudaFree(d_a);     cudaFree(d_b);     cudaFree(d_c);      return 0; }   "
    },
    {
        "id": "251",
        "c_code": "#include <stdio.h> #include <math.h>   void clamp_cpu(int N, float *X, int INCX, float clamp_min, float clamp_max);  int main() {          const int N = 5;     float X[] = {1.0, 2.0, 3.0, 4.0, 5.0};     const int INCX = 1;     const float clamp_min = 2.0;     const float clamp_max = 4.0;           clamp_cpu(N, X, INCX, clamp_min, clamp_max);           printf(\"Resultant array:\\n\");     for (int i = 0; i < N; i++) {         printf(\"%f \", X[i]);     }      return 0; }   void clamp_cpu(int N, float *X, int INCX, float clamp_min, float clamp_max) {     for (int i = 0; i < N; ++i) {         X[i * INCX] = fmin(clamp_max, fmax(clamp_min, X[i * INCX]));     } }   ",
        "cuda_code": "#include <stdio.h> #include <math.h>   __global__ void clamp_kernel(int N, float* X, int INCX, float clamp_min, float clamp_max) {     int i = (blockIdx.x + blockIdx.y * gridDim.x) * blockDim.x + threadIdx.x;          if (i < N) {         X[i * INCX] = fminf(clamp_max, fmaxf(clamp_min, X[i * INCX]));     } }  int main() {          int N = 1000;            float* h_X = (float*)malloc(N * sizeof(float));           for (int i = 0; i < N; ++i) {         h_X[i] = static_cast<float>(i - 500);      }           float* d_X;     cudaMalloc((void**)&d_X, N * sizeof(float));           cudaMemcpy(d_X, h_X, N * sizeof(float), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((N + block_size - 1) / block_size, 1);           float clamp_min = -100.0f;     float clamp_max = 100.0f;           clamp_kernel<<<grid_size, block_size>>>(N, d_X, 1, clamp_min, clamp_max);           cudaMemcpy(h_X, d_X, N * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result: \");     for (int i = 0; i < N; ++i) {         printf(\"%f \", h_X[i]);     }     printf(\"\\n\");           free(h_X);     cudaFree(d_X);      return 0; }   "
    },
    {
        "id": "252",
        "c_code": "#include <stdio.h> #include <omp.h>   double histogram_serial(const int *values, int *bins, const int nbins, const int n);  int main() {          const int n = 10;     int values[] = {1, 2, 3, 4, 5, 1, 2, 3, 4, 5};     const int nbins = 5;     int bins[5];           double time = histogram_serial(values, bins, nbins, n);           printf(\"Histogram bins:\\n\");     for (int i = 0; i < nbins; i++) {         printf(\"Bin %d: %d\\n\", i, bins[i]);     }           printf(\"Time taken: %f seconds\\n\", time);      return 0; }   double histogram_serial(const int *values, int *bins, const int nbins, const int n) {     double time = -omp_get_wtime();           for (int i = 0; i < nbins; ++i) {         bins[i] = 0;     }           for (int i = 0; i < n; ++i) {         bins[values[i]]++;     }      time += omp_get_wtime();     return time; }   ",
        "cuda_code": "#include <stdio.h>   __global__ void histogram(int* x, int* bins, int n) {     auto i = threadIdx.x + blockIdx.x * blockDim.x;      if (i < n) {         const auto c = x[i];         atomicAdd(&bins[c], 1);     } }  int main() {          int n = 1000;            int* h_x = (int*)malloc(n * sizeof(int));     int* h_bins = (int*)calloc(n, sizeof(int));            for (int i = 0; i < n; ++i) {         h_x[i] = i % 10;      }           int* d_x;     int* d_bins;     cudaMalloc((void**)&d_x, n * sizeof(int));     cudaMalloc((void**)&d_bins, n * sizeof(int));           cudaMemcpy(d_x, h_x, n * sizeof(int), cudaMemcpyHostToDevice);     cudaMemcpy(d_bins, h_bins, n * sizeof(int), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((n + block_size - 1) / block_size, 1);           histogram<<<grid_size, block_size>>>(d_x, d_bins, n);           cudaMemcpy(h_bins, d_bins, n * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Histogram Result:\\n\");     for (int i = 0; i < n; ++i) {         printf(\"Bin %d: %d\\n\", i, h_bins[i]);     }           free(h_x);     free(h_bins);     cudaFree(d_x);     cudaFree(d_bins);      return 0; }   "
    },
    {
        "id": "253",
        "c_code": "#include <stdio.h>   void multMat_cpu(int n, int *arrForce_d, int *arrDistance_d, int *arrAnswer_d);  int main() {          const int n = 5;     int arrForce_d[] = {1, 2, 3, 4, 5};     int arrDistance_d[] = {2, 4, 6, 8, 10};     int arrAnswer_d[5];           multMat_cpu(n, arrForce_d, arrDistance_d, arrAnswer_d);           printf(\"Resultant array:\\n\");     for (int i = 0; i < n; i++) {         printf(\"%d \", arrAnswer_d[i]);     }      return 0; }   void multMat_cpu(int n, int *arrForce_d, int *arrDistance_d, int *arrAnswer_d) {     for (int i = 0; i < n; i++) {         arrAnswer_d[i] = arrForce_d[i] * arrDistance_d[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void multMat(int n, int* arrForce_d, int* arrDistance_d, int* arrAnswer_d) {     int i = blockIdx.x * blockDim.x + threadIdx.x;      if (i < n) {         arrAnswer_d[i] = arrForce_d[i] * arrDistance_d[i];     } }  int main() {          int n = 1000;            int* h_arrForce = (int*)malloc(n * sizeof(int));     int* h_arrDistance = (int*)malloc(n * sizeof(int));     int* h_arrAnswer = (int*)malloc(n * sizeof(int));           for (int i = 0; i < n; ++i) {         h_arrForce[i] = i;                    h_arrDistance[i] = i * 2;         }           int* d_arrForce;     int* d_arrDistance;     int* d_arrAnswer;     cudaMalloc((void**)&d_arrForce, n * sizeof(int));     cudaMalloc((void**)&d_arrDistance, n * sizeof(int));     cudaMalloc((void**)&d_arrAnswer, n * sizeof(int));           cudaMemcpy(d_arrForce, h_arrForce, n * sizeof(int), cudaMemcpyHostToDevice);     cudaMemcpy(d_arrDistance, h_arrDistance, n * sizeof(int), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((n + block_size - 1) / block_size, 1);           multMat<<<grid_size, block_size>>>(n, d_arrForce, d_arrDistance, d_arrAnswer);           cudaMemcpy(h_arrAnswer, d_arrAnswer, n * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Result: \");     for (int i = 0; i < n; ++i) {         printf(\"%d \", h_arrAnswer[i]);     }     printf(\"\\n\");           free(h_arrForce);     free(h_arrDistance);     free(h_arrAnswer);     cudaFree(d_arrForce);     cudaFree(d_arrDistance);     cudaFree(d_arrAnswer);      return 0; }   "
    },
    {
        "id": "254",
        "c_code": " #include <stdio.h>   void axpy_cpu(int N, float ALPHA, float *X, int INCX, float *Y, int INCY);  int main() {          const int N = 5;     const float ALPHA = 2.0;     float X[] = {1.0, 2.0, 3.0, 4.0, 5.0};     const int INCX = 1;     float Y[] = {2.0, 4.0, 6.0, 8.0, 10.0};     const int INCY = 1;           axpy_cpu(N, ALPHA, X, INCX, Y, INCY);           printf(\"Resultant array Y after axpy operation:\\n\");     for (int i = 0; i < N; i++) {         printf(\"%f \", Y[i]);     }      return 0; }   void axpy_cpu(int N, float ALPHA, float *X, int INCX, float *Y, int INCY) {     for (int i = 0; i < N; ++i) {         Y[i * INCY] += ALPHA * X[i * INCX];     } }  ",
        "cuda_code": "#include <stdio.h>   __global__ void axpy_kernel(int N, float ALPHA, float* X, int OFFX, int INCX, float* Y, int OFFY, int INCY) {     int i = blockIdx.x * blockDim.x + threadIdx.x;      if (i < N) {         Y[OFFY + i * INCY] += ALPHA * X[OFFX + i * INCX];     } }  int main() {          int N = 1000;            float ALPHA = 2.0f;     float* h_X = (float*)malloc(N * sizeof(float));     float* h_Y = (float*)malloc(N * sizeof(float));           for (int i = 0; i < N; ++i) {         h_X[i] = i;          h_Y[i] = i * 2;     }           float* d_X;     float* d_Y;     cudaMalloc((void**)&d_X, N * sizeof(float));     cudaMalloc((void**)&d_Y, N * sizeof(float));           cudaMemcpy(d_X, h_X, N * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_Y, h_Y, N * sizeof(float), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((N + block_size - 1) / block_size, 1);           axpy_kernel<<<grid_size, block_size>>>(N, ALPHA, d_X, 0, 1, d_Y, 0, 1);           cudaMemcpy(h_Y, d_Y, N * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result: \");     for (int i = 0; i < N; ++i) {         printf(\"%f \", h_Y[i]);     }     printf(\"\\n\");           free(h_X);     free(h_Y);     cudaFree(d_X);     cudaFree(d_Y);      return 0; }   "
    },
    {
        "id": "255",
        "c_code": "#include <stdio.h>   void castImageTofloat(float *deviceOutputImageData, unsigned char *ucharImage,                        int imageWidth, int imageHeight, int channels, int pixelSize);  int main() {          const int imageWidth = 2;     const int imageHeight = 2;     const int channels = 3;     const int pixelSize = channels;      unsigned char ucharImage[] = {255, 128, 0, 200, 100, 50, 0, 0, 255, 128, 255, 0};           const int floatPixelSize = channels;      float deviceOutputImageData[floatPixelSize];           castImageTofloat(deviceOutputImageData, ucharImage, imageWidth, imageHeight, channels, pixelSize);           printf(\"Resultant array after casting to float:\\n\");     for (int i = 0; i < floatPixelSize; i++) {         printf(\"%f \", deviceOutputImageData[i]);     }      return 0; }   void castImageTofloat(float *deviceOutputImageData, unsigned char *ucharImage,                        int imageWidth, int imageHeight, int channels, int pixelSize) {     for (int w = 0; w < pixelSize; w++) {         deviceOutputImageData[w] = (float)(ucharImage[w] / 255.0);     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void castImageTofloat(float* deviceOutputImageData, unsigned char* ucharImage,                                   int imageWidth, int imageHeight, int channels, int pixelSize) {     int w = threadIdx.x + blockDim.x * blockIdx.x;      if (w < pixelSize) {         deviceOutputImageData[w] = static_cast<float>(ucharImage[w]) / 255.0f;     } }  int main() {          int imageWidth = 512;         int imageHeight = 512;        int channels = 3;             int pixelSize = channels * imageWidth * imageHeight;           float* h_deviceOutputImageData = (float*)malloc(pixelSize * sizeof(float));     unsigned char* h_ucharImage = (unsigned char*)malloc(pixelSize * sizeof(unsigned char));           for (int i = 0; i < pixelSize; ++i) {         h_ucharImage[i] = static_cast<unsigned char>(i % 256);       }           float* d_deviceOutputImageData;     unsigned char* d_ucharImage;     cudaMalloc((void**)&d_deviceOutputImageData, pixelSize * sizeof(float));     cudaMalloc((void**)&d_ucharImage, pixelSize * sizeof(unsigned char));           cudaMemcpy(d_ucharImage, h_ucharImage, pixelSize * sizeof(unsigned char), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((pixelSize + block_size - 1) / block_size, 1);           castImageTofloat<<<grid_size, block_size>>>(d_deviceOutputImageData, d_ucharImage,                                                imageWidth, imageHeight, channels, pixelSize);           cudaMemcpy(h_deviceOutputImageData, d_deviceOutputImageData, pixelSize * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result: \");     for (int i = 0; i < pixelSize; ++i) {         printf(\"%f \", h_deviceOutputImageData[i]);     }     printf(\"\\n\");           free(h_deviceOutputImageData);     free(h_ucharImage);     cudaFree(d_deviceOutputImageData);     cudaFree(d_ucharImage);      return 0; }   "
    },
    {
        "id": "256",
        "c_code": "#include <stdio.h>   void zero_centroid_vals_cpu(int k, double *Cx_sum, double *Cy_sum, int *Csize);  int main() {          const int k = 3;     double Cx_sum[k], Cy_sum[k];     int Csize[k];           zero_centroid_vals_cpu(k, Cx_sum, Cy_sum, Csize);           printf(\"Resultant arrays after zeroing centroid values:\\n\");     printf(\"Cx_sum: \");     for (int i = 0; i < k; i++) {         printf(\"%f \", Cx_sum[i]);     }     printf(\"\\nCy_sum: \");     for (int i = 0; i < k; i++) {         printf(\"%f \", Cy_sum[i]);     }     printf(\"\\nCsize: \");     for (int i = 0; i < k; i++) {         printf(\"%d \", Csize[i]);     }      return 0; }   void zero_centroid_vals_cpu(int k, double *Cx_sum, double *Cy_sum, int *Csize) {     for (int index = 0; index < k; index++) {         Cx_sum[index] = 0;         Cy_sum[index] = 0;         Csize[index] = 0;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void zero_centroid_vals(int k, double* Cx_sum, double* Cy_sum, int* Csize) {     int index = blockIdx.x * blockDim.x + threadIdx.x;      if (index < k) {         Cx_sum[index] = 0.0;         Cy_sum[index] = 0.0;         Csize[index] = 0;     } }  int main() {          int k = 10;            double* h_Cx_sum = (double*)malloc(k * sizeof(double));     double* h_Cy_sum = (double*)malloc(k * sizeof(double));     int* h_Csize = (int*)malloc(k * sizeof(int));           double* d_Cx_sum;     double* d_Cy_sum;     int* d_Csize;     cudaMalloc((void**)&d_Cx_sum, k * sizeof(double));     cudaMalloc((void**)&d_Cy_sum, k * sizeof(double));     cudaMalloc((void**)&d_Csize, k * sizeof(int));           int block_size = 256;     dim3 grid_size((k + block_size - 1) / block_size, 1);           zero_centroid_vals<<<grid_size, block_size>>>(k, d_Cx_sum, d_Cy_sum, d_Csize);           cudaMemcpy(h_Cx_sum, d_Cx_sum, k * sizeof(double), cudaMemcpyDeviceToHost);     cudaMemcpy(h_Cy_sum, d_Cy_sum, k * sizeof(double), cudaMemcpyDeviceToHost);     cudaMemcpy(h_Csize, d_Csize, k * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Result:\\n\");     for (int i = 0; i < k; ++i) {         printf(\"Centroid %d: Cx_sum=%f, Cy_sum=%f, Csize=%d\\n\", i, h_Cx_sum[i], h_Cy_sum[i], h_Csize[i]);     }           free(h_Cx_sum);     free(h_Cy_sum);     free(h_Csize);     cudaFree(d_Cx_sum);     cudaFree(d_Cy_sum);     cudaFree(d_Csize);      return 0; }   "
    },
    {
        "id": "257",
        "c_code": "#include <stdio.h>   void HammingDistanceCPU(int *c, const int *a, const int *b, long const int *size);  int main() {          const int size = 5;     int a[] = {1, 0, 1, 1, 0};     int b[] = {0, 1, 1, 1, 1};     int c = 0;           HammingDistanceCPU(&c, a, b, &size);           printf(\"Hamming distance: %d\\n\", c);      return 0; }   void HammingDistanceCPU(int *c, const int *a, const int *b, long const int *size) {     for (int i = 0; i < *size; i += 1) {         if (a[i] != b[i])             *c = *c + 1;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void HammingDistance(int* c, const int* a, const int* b, const long int* size) {     int i = threadIdx.x + blockDim.x * blockIdx.x;     int stride = blockDim.x * gridDim.x;      for (; i < *size; i += stride) {         if (a[i] != b[i]) {             atomicAdd(c, 1);         }     } }  int main() {          long int size = 1000;            int* h_a = (int*)malloc(size * sizeof(int));     int* h_b = (int*)malloc(size * sizeof(int));     int* h_c = (int*)malloc(sizeof(int));           for (int i = 0; i < size; ++i) {         h_a[i] = i % 2;          h_b[i] = (i + 1) % 2;      }      *h_c = 0;           int* d_a;     int* d_b;     int* d_c;     cudaMalloc((void**)&d_a, size * sizeof(int));     cudaMalloc((void**)&d_b, size * sizeof(int));     cudaMalloc((void**)&d_c, sizeof(int));           cudaMemcpy(d_a, h_a, size * sizeof(int), cudaMemcpyHostToDevice);     cudaMemcpy(d_b, h_b, size * sizeof(int), cudaMemcpyHostToDevice);     cudaMemcpy(d_c, h_c, sizeof(int), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((size + block_size - 1) / block_size, 1);           HammingDistance<<<grid_size, block_size>>>(d_c, d_a, d_b, &size);           cudaMemcpy(h_c, d_c, sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Hamming Distance: %d\\n\", *h_c);           free(h_a);     free(h_b);     free(h_c);     cudaFree(d_a);     cudaFree(d_b);     cudaFree(d_c);      return 0; }   "
    },
    {
        "id": "258",
        "c_code": "#include <stdio.h>   void castImageToUchar(float *deviceInputImageData, unsigned char *ucharImage, int imageWidth, int imageHeight, int channels, int pixelSize);  int main() {          const int imageWidth = 2;     const int imageHeight = 2;     const int channels = 3;     const int pixelSize = channels;     float deviceInputImageData[pixelSize] = {0.1f, 0.5f, 0.9f, 0.3f, 0.7f, 1.0f};     unsigned char ucharImage[pixelSize];           castImageToUchar(deviceInputImageData, ucharImage, imageWidth, imageHeight, channels, pixelSize);           printf(\"Resultant ucharImage:\\n\");     for (int i = 0; i < pixelSize; ++i) {         printf(\"%d \", ucharImage[i]);     }      return 0; }   void castImageToUchar(float *deviceInputImageData, unsigned char *ucharImage, int imageWidth, int imageHeight, int channels, int pixelSize) {     int w;     for (w = 0; w < pixelSize; w++) {         ucharImage[w] = (unsigned char)(255 * deviceInputImageData[w]);     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void castImageToUchar(float* deviceInputImageData, unsigned char* ucharImage,                                   int imageWidth, int imageHeight, int channels, int pixelSize) {     int w = threadIdx.x + blockDim.x * blockIdx.x;      if (w < pixelSize) {         ucharImage[w] = static_cast<unsigned char>(255 * deviceInputImageData[w]);     } }  int main() {          int imageWidth = 512;         int imageHeight = 512;        int channels = 3;             int pixelSize = channels * imageWidth * imageHeight;           float* h_deviceInputImageData = (float*)malloc(pixelSize * sizeof(float));     unsigned char* h_ucharImage = (unsigned char*)malloc(pixelSize * sizeof(unsigned char));           for (int i = 0; i < pixelSize; ++i) {         h_deviceInputImageData[i] = static_cast<float>(i % 256) / 255.0f;       }           float* d_deviceInputImageData;     unsigned char* d_ucharImage;     cudaMalloc((void**)&d_deviceInputImageData, pixelSize * sizeof(float));     cudaMalloc((void**)&d_ucharImage, pixelSize * sizeof(unsigned char));           cudaMemcpy(d_deviceInputImageData, h_deviceInputImageData, pixelSize * sizeof(float), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((pixelSize + block_size - 1) / block_size, 1);           castImageToUchar<<<grid_size, block_size>>>(d_deviceInputImageData, d_ucharImage,                                                  imageWidth, imageHeight, channels, pixelSize);           cudaMemcpy(h_ucharImage, d_ucharImage, pixelSize * sizeof(unsigned char), cudaMemcpyDeviceToHost);           printf(\"Result: \");     for (int i = 0; i < pixelSize; ++i) {         printf(\"%d \", h_ucharImage[i]);     }     printf(\"\\n\");           free(h_deviceInputImageData);     free(h_ucharImage);     cudaFree(d_deviceInputImageData);     cudaFree(d_ucharImage);      return 0; }   "
    },
    {
        "id": "259",
        "c_code": "#include <stdio.h>   void compareDoubleArrayToThreshold_cpu(double *d_in, int *d_out, int length, double threshold);  int main() {          const int length = 5;     double d_in[length] = {1.5, -2.0, 0.8, -0.3, 2.7};     int d_out[length];           double threshold = 1.0;     compareDoubleArrayToThreshold_cpu(d_in, d_out, length, threshold);           printf(\"Resultant d_out array:\\n\");     for (int i = 0; i < length; ++i) {         printf(\"%d \", d_out[i]);     }      return 0; }   void compareDoubleArrayToThreshold_cpu(double *d_in, int *d_out, int length, double threshold) {     for (int idx = 0; idx < length; idx++) {         double abs_value = (d_in[idx] > 0) ? d_in[idx] : -d_in[idx];         d_out[idx] = (abs_value < threshold) ? 1 : 0;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void compareDoubleArrayToThresholdKernel(double* d_in, int* d_out, int length, double threshold) {     int tid = blockIdx.x * blockDim.x + threadIdx.x;      if (tid < length) {         double abs_val = (d_in[tid] > 0) ? d_in[tid] : -d_in[tid];         d_out[tid] = (abs_val < threshold) ? 1 : 0;     } }  int main() {          int length = 100;            double* h_d_in = (double*)malloc(length * sizeof(double));     int* h_d_out = (int*)malloc(length * sizeof(int));           for (int i = 0; i < length; ++i) {         h_d_in[i] = (i % 3 == 0) ? 0.5 : -1.0;      }           double* d_d_in;     int* d_d_out;     cudaMalloc((void**)&d_d_in, length * sizeof(double));     cudaMalloc((void**)&d_d_out, length * sizeof(int));           cudaMemcpy(d_d_in, h_d_in, length * sizeof(double), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((length + block_size - 1) / block_size, 1);           double threshold = 1.0;            compareDoubleArrayToThresholdKernel<<<grid_size, block_size>>>(d_d_in, d_d_out, length, threshold);           cudaMemcpy(h_d_out, d_d_out, length * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Result: \");     for (int i = 0; i < length; ++i) {         printf(\"%d \", h_d_out[i]);     }     printf(\"\\n\");           free(h_d_in);     free(h_d_out);     cudaFree(d_d_in);     cudaFree(d_d_out);      return 0; }   "
    },
    {
        "id": "260",
        "c_code": "#include <stdio.h>   void transpose(int A[][10000], int trans[][10000]);  int main() {          const int rows = 10000;     const int cols = 10000;     int A[rows][cols];     int trans[cols][rows];           for (int i = 0; i < rows; i++) {         for (int j = 0; j < cols; j++) {             A[i][j] = i * cols + j + 1;          }     }           transpose(A, trans);           printf(\"Original array A:\\n\");     for (int i = 0; i < 5; i++) {         for (int j = 0; j < 5; j++) {             printf(\"%d \", A[i][j]);         }         printf(\"\\n\");     }      printf(\"\\nTransposed array:\\n\");     for (int i = 0; i < 5; i++) {         for (int j = 0; j < 5; j++) {             printf(\"%d \", trans[i][j]);         }         printf(\"\\n\");     }      return 0; }   void transpose(int A[][10000], int trans[][10000]) {     for (int i = 0; i < 10000; i++) {         for (int j = 0; j < 10000; j++) {             trans[i][j] = A[j][i];         }     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void transpose(float* a, float* b, int width) {     int row = blockIdx.y * blockDim.y + threadIdx.y;     int col = blockIdx.x * blockDim.x + threadIdx.x;      if (row < width && col < width) {         b[col * width + row] = a[row * width + col];     } }  int main() {          int width = 4;            float* h_a = (float*)malloc(width * width * sizeof(float));     float* h_b = (float*)malloc(width * width * sizeof(float));           for (int i = 0; i < width * width; ++i) {         h_a[i] = static_cast<float>(i);      }           float* d_a;     float* d_b;     cudaMalloc((void**)&d_a, width * width * sizeof(float));     cudaMalloc((void**)&d_b, width * width * sizeof(float));           cudaMemcpy(d_a, h_a, width * width * sizeof(float), cudaMemcpyHostToDevice);           dim3 block_size(16, 16);      dim3 grid_size((width + block_size.x - 1) / block_size.x, (width + block_size.y - 1) / block_size.y);           transpose<<<grid_size, block_size>>>(d_a, d_b, width);           cudaMemcpy(h_b, d_b, width * width * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Original Matrix:\\n\");     for (int i = 0; i < width; ++i) {         for (int j = 0; j < width; ++j) {             printf(\"%.2f\\t\", h_a[i * width + j]);         }         printf(\"\\n\");     }      printf(\"\\nTransposed Matrix:\\n\");     for (int i = 0; i < width; ++i) {         for (int j = 0; j < width; ++j) {             printf(\"%.2f\\t\", h_b[i * width + j]);         }         printf(\"\\n\");     }           free(h_a);     free(h_b);     cudaFree(d_a);     cudaFree(d_b);      return 0; }   "
    },
    {
        "id": "261",
        "c_code": "#include <stdio.h>   void ReLU_forward(float *in, int *mask, int datasize, int training);  int main() {          const int datasize = 5;     float in[5] = {-1.5, 2.0, -0.8, 0.3, 1.7};     int mask[5] = {0};             ReLU_forward(in, mask, datasize, 1);             printf(\"Output array after ReLU forward pass:\\n\");     for (int i = 0; i < datasize; ++i) {         printf(\"%f \", in[i]);     }      printf(\"\\nBinary mask (for training): \");     for (int i = 0; i < datasize; ++i) {         printf(\"%d \", mask[i]);     }      return 0; }   void ReLU_forward(float *in, int *mask, int datasize, int training) {     for (int i = 0; i < datasize; ++i) {         int keep = in[i] > 0;         if (training) {             mask[i] = keep;         }         if (!keep) {             in[i] = 0;         }     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void cuda_ReLU_forward_kernel(float* d_in_data, bool* d_mask, const long unsigned int datasize, bool training) {     uint i = blockIdx.x * blockDim.x + threadIdx.x;      if (i >= datasize)         return;      bool keep = d_in_data[i] > 0;      if (training)         d_mask[i] = keep;      if (!keep)         d_in_data[i] = 0; }  int main() {          long unsigned int datasize = 100;            float* h_d_in_data = (float*)malloc(datasize * sizeof(float));     bool* h_d_mask = (bool*)malloc(datasize * sizeof(bool));           for (int i = 0; i < datasize; ++i) {         h_d_in_data[i] = i - datasize / 2;      }           float* d_d_in_data;     bool* d_d_mask;     cudaMalloc((void**)&d_d_in_data, datasize * sizeof(float));     cudaMalloc((void**)&d_d_mask, datasize * sizeof(bool));           cudaMemcpy(d_d_in_data, h_d_in_data, datasize * sizeof(float), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((datasize + block_size - 1) / block_size, 1);           bool training = true;            cuda_ReLU_forward_kernel<<<grid_size, block_size>>>(d_d_in_data, d_d_mask, datasize, training);           cudaMemcpy(h_d_in_data, d_d_in_data, datasize * sizeof(float), cudaMemcpyDeviceToHost);     cudaMemcpy(h_d_mask, d_d_mask, datasize * sizeof(bool), cudaMemcpyDeviceToHost);           printf(\"Input Data:\\n\");     for (int i = 0; i < datasize; ++i) {         printf(\"%.2f \", h_d_in_data[i]);     }      printf(\"\\nMask (for training):\\n\");     for (int i = 0; i < datasize; ++i) {         printf(\"%d \", h_d_mask[i] ? 1 : 0);     }     printf(\"\\n\");           free(h_d_in_data);     free(h_d_mask);     cudaFree(d_d_in_data);     cudaFree(d_d_mask);      return 0; }   "
    },
    {
        "id": "262",
        "c_code": "#include <stdio.h>   void sum_backward(float *db, float *dout, int r, int c);  int main() {          const int r = 3;     const int c = 2;     float db[2] = {0.0, 0.0};     float dout[6] = {1.0, 2.0, 3.0, 4.0, 5.0, 6.0};           sum_backward(db, dout, r, c);           printf(\"Gradients with respect to bias terms (db): \");     for (int i = 0; i < c; ++i) {         printf(\"%f \", db[i]);     }      return 0; }   void sum_backward(float *db, float *dout, int r, int c) {     for (int j = 0; j < c; ++j) {         for (int i = 0; i < r; ++i) {             db[j] += dout[i * c + j];         }     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void kernel_sum_backward(float* db, float* dout, int r, int c) {     unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;     int N = c;      while (tid < N) {         for (int i = 0; i < r; i++) {             db[tid] += dout[i * c + tid];         }         tid += gridDim.x * blockDim.x;     } }  int main() {          int r = 4;      int c = 5;            float* h_db = (float*)malloc(c * sizeof(float));     float* h_dout = (float*)malloc(r * c * sizeof(float));           for (int i = 0; i < c; ++i) {         h_db[i] = 0.0;      }      for (int i = 0; i < r * c; ++i) {         h_dout[i] = i + 1;      }           float* d_db;     float* d_dout;     cudaMalloc((void**)&d_db, c * sizeof(float));     cudaMalloc((void**)&d_dout, r * c * sizeof(float));           cudaMemcpy(d_db, h_db, c * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_dout, h_dout, r * c * sizeof(float), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((c + block_size - 1) / block_size, 1);           kernel_sum_backward<<<grid_size, block_size>>>(d_db, d_dout, r, c);           cudaMemcpy(h_db, d_db, c * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Gradients with respect to input (db):\\n\");     for (int i = 0; i < c; ++i) {         printf(\"%.2f \", h_db[i]);     }     printf(\"\\n\");           free(h_db);     free(h_dout);     cudaFree(d_db);     cudaFree(d_dout);      return 0; }   "
    },
    {
        "id": "263",
        "c_code": "#include <stdio.h>   void clip_cpu(int N, float ALPHA, float *X, int INCX, float *Y, int INCY);  int main() {          const int N = 5;     const float ALPHA = 2.0;     float X[] = {1.0, 3.0, 2.0, -1.0, 4.0};     float Y[N];           clip_cpu(N, ALPHA, X, 1, Y, 1);           printf(\"Clipped array (Y): \");     for (int i = 0; i < N; ++i) {         printf(\"%f \", Y[i]);     }      return 0; }   void clip_cpu(int N, float ALPHA, float *X, int INCX, float *Y, int INCY) {     for (int i = 0; i < N; ++i) {         float val = X[i * INCX];         Y[i * INCY] = val > ALPHA ? val : 0;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void clip_kernel(int N, float ALPHA, float* X, int INCX, float* Y, int INCY) {     int i = (blockIdx.x + blockIdx.y * gridDim.x) * blockDim.x + threadIdx.x;      if (i < N) {         float val = X[i * INCX];         Y[i * INCY] = (val > ALPHA) ? val : 0;     } }  int main() {          int N = 100;            float* h_X = (float*)malloc(N * sizeof(float));     float* h_Y = (float*)malloc(N * sizeof(float));           for (int i = 0; i < N; ++i) {         h_X[i] = i - N / 2.0;      }           float* d_X;     float* d_Y;     cudaMalloc((void**)&d_X, N * sizeof(float));     cudaMalloc((void**)&d_Y, N * sizeof(float));           cudaMemcpy(d_X, h_X, N * sizeof(float), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((N + block_size - 1) / block_size, 1);           float ALPHA = 5.0;            clip_kernel<<<grid_size, block_size>>>(N, ALPHA, d_X, 1, d_Y, 1);           cudaMemcpy(h_Y, d_Y, N * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Input Array:\\n\");     for (int i = 0; i < N; ++i) {         printf(\"%.2f \", h_X[i]);     }      printf(\"\\nClipped Array (ALPHA=%.2f):\\n\", ALPHA);     for (int i = 0; i < N; ++i) {         printf(\"%.2f \", h_Y[i]);     }     printf(\"\\n\");           free(h_X);     free(h_Y);     cudaFree(d_X);     cudaFree(d_Y);      return 0; }   "
    },
    {
        "id": "264",
        "c_code": "#include <stdio.h>   void update_x(double *x, double *a, double *b, int n);  int main() {          const int n = 5;     double x[] = {1.0, 2.0, 3.0, 4.0, 5.0};     double a[] = {2.0, 4.0, 6.0, 8.0, 10.0};     double b[] = {1.0, 2.0, 3.0, 2.0, 1.0};           update_x(x, a, b, n);           printf(\"Updated array (x): \");     for (int i = 0; i < n; ++i) {         printf(\"%f \", x[i]);     }      return 0; }   void update_x(double *x, double *a, double *b, int n) {     for (int i = 0; i < n; ++i) {         x[i] = (2.0 / 3.0) * (a[i] / b[i]) + (1.0 / 3.0) * x[i];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void update_x(double* x, double* a, double* b, int n) {     int index = blockIdx.x * blockDim.x + threadIdx.x;     int stride = gridDim.x * blockDim.x;      for (int i = index; i < n; i += stride) {         x[i] = 2.0 / 3.0 * a[i] / b[i] + 1.0 / 3.0 * x[i];     } }  int main() {          int n = 100;            double* h_x = (double*)malloc(n * sizeof(double));     double* h_a = (double*)malloc(n * sizeof(double));     double* h_b = (double*)malloc(n * sizeof(double));           for (int i = 0; i < n; ++i) {         h_x[i] = i + 1.0;          h_a[i] = 2.0 * i + 1.0;          h_b[i] = i + 2.0;      }           double* d_x;     double* d_a;     double* d_b;     cudaMalloc((void**)&d_x, n * sizeof(double));     cudaMalloc((void**)&d_a, n * sizeof(double));     cudaMalloc((void**)&d_b, n * sizeof(double));           cudaMemcpy(d_x, h_x, n * sizeof(double), cudaMemcpyHostToDevice);     cudaMemcpy(d_a, h_a, n * sizeof(double), cudaMemcpyHostToDevice);     cudaMemcpy(d_b, h_b, n * sizeof(double), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((n + block_size - 1) / block_size, 1);           update_x<<<grid_size, block_size>>>(d_x, d_a, d_b, n);           cudaMemcpy(h_x, d_x, n * sizeof(double), cudaMemcpyDeviceToHost);           printf(\"Initial x Array:\\n\");     for (int i = 0; i < n; ++i) {         printf(\"%.2f \", h_x[i]);     }      printf(\"\\n\");           free(h_x);     free(h_a);     free(h_b);     cudaFree(d_x);     cudaFree(d_a);     cudaFree(d_b);      return 0; }   "
    },
    {
        "id": "265",
        "c_code": "#include <stdio.h>  int seqTrans(float **h_in, float **h_out, int x_size, int y_size) ;  int main() {          int x_size = 3;     int y_size = 2;     float **h_in;       float **h_out;            seqTrans(h_in, h_out, x_size, y_size);           printf(\"Transposed Matrix:\\n\");     for (int y = 0; y < x_size; y++) {         for (int x = 0; x < y_size; x++) {             printf(\"%f \", h_out[y][x]);         }         printf(\"\\n\");     }            return 0; }  int seqTrans(float **h_in, float **h_out, int x_size, int y_size) {     for (int y = 0; y < y_size; y++) {         for (int x = 0; x < x_size; x++) {             h_out[x][y] = h_in[y][x];         }     }     return 1; }   ",
        "cuda_code": "#include <stdio.h>   __global__ void naiveParTrans(float* d_in, float* d_out, int x_size, int y_size) {     int gidx = blockIdx.x * blockDim.x + threadIdx.x;     int gidy = blockIdx.y * blockDim.y + threadIdx.y;      if (gidx < x_size && gidy < y_size) {         d_out[gidx * y_size + gidy] = d_in[gidy * x_size + gidx];     } }  int main() {          int x_size = 4;      int y_size = 3;            float* h_in = (float*)malloc(x_size * y_size * sizeof(float));     float* h_out = (float*)malloc(y_size * x_size * sizeof(float));           for (int i = 0; i < x_size * y_size; ++i) {         h_in[i] = i + 1.0;      }           float* d_in;     float* d_out;     cudaMalloc((void**)&d_in, x_size * y_size * sizeof(float));     cudaMalloc((void**)&d_out, y_size * x_size * sizeof(float));           cudaMemcpy(d_in, h_in, x_size * y_size * sizeof(float), cudaMemcpyHostToDevice);           dim3 block_size(2, 2);      dim3 grid_size((x_size + block_size.x - 1) / block_size.x, (y_size + block_size.y - 1) / block_size.y);           naiveParTrans<<<grid_size, block_size>>>(d_in, d_out, x_size, y_size);           cudaMemcpy(h_out, d_out, y_size * x_size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Input Matrix:\\n\");     for (int i = 0; i < x_size * y_size; ++i) {         printf(\"%.2f \", h_in[i]);         if ((i + 1) % y_size == 0) {             printf(\"\\n\");         }     }      printf(\"\\nTransposed Matrix:\\n\");     for (int i = 0; i < y_size * x_size; ++i) {         printf(\"%.2f \", h_out[i]);         if ((i + 1) % x_size == 0) {             printf(\"\\n\");         }     }           free(h_in);     free(h_out);     cudaFree(d_in);     cudaFree(d_out);      return 0; }   "
    },
    {
        "id": "266",
        "c_code": "#include <stdio.h>   void l2_cpu(int n, float *pred, float *truth, float *delta, float *error)\uff1b  int main() {          int n = 3;     float pred[] = {1.0, 2.0, 3.0};     float truth[] = {2.0, 3.0, 5.0};     float delta[3];     float error[3];           l2_cpu(n, pred, truth, delta, error);           printf(\"Delta array: \");     for (int i = 0; i < n; ++i) {         printf(\"%f \", delta[i]);     }     printf(\"\\nError array: \");     for (int i = 0; i < n; ++i) {         printf(\"%f \", error[i]);     }      return 0; }  void l2_cpu(int n, float *pred, float *truth, float *delta, float *error) {     int i;     for (i = 0; i < n; ++i) {         float diff = truth[i] - pred[i];         error[i] = diff * diff;         delta[i] = diff;     } }   ",
        "cuda_code": "#include <stdio.h> #include <math.h>   __global__ void l2_kernel(int n, float* pred, float* truth, float* delta, float* error) {     int i = (blockIdx.x + blockIdx.y * gridDim.x) * blockDim.x + threadIdx.x;      if (i < n) {         float diff = truth[i] - pred[i];         error[i] = diff * diff;         delta[i] = diff;     } }  int main() {          int n = 100;            float* h_pred = (float*)malloc(n * sizeof(float));     float* h_truth = (float*)malloc(n * sizeof(float));     float* h_delta = (float*)malloc(n * sizeof(float));     float* h_error = (float*)malloc(n * sizeof(float));           for (int i = 0; i < n; ++i) {         h_pred[i] = i;          h_truth[i] = 2 * i;      }           float* d_pred;     float* d_truth;     float* d_delta;     float* d_error;     cudaMalloc((void**)&d_pred, n * sizeof(float));     cudaMalloc((void**)&d_truth, n * sizeof(float));     cudaMalloc((void**)&d_delta, n * sizeof(float));     cudaMalloc((void**)&d_error, n * sizeof(float));           cudaMemcpy(d_pred, h_pred, n * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_truth, h_truth, n * sizeof(float), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((n + block_size - 1) / block_size, 1);           l2_kernel<<<grid_size, block_size>>>(n, d_pred, d_truth, d_delta, d_error);           cudaMemcpy(h_delta, d_delta, n * sizeof(float), cudaMemcpyDeviceToHost);     cudaMemcpy(h_error, d_error, n * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Pred Array:\\n\");     for (int i = 0; i < n; ++i) {         printf(\"%.2f \", h_pred[i]);     }      printf(\"\\nTruth Array:\\n\");     for (int i = 0; i < n; ++i) {         printf(\"%.2f \", h_truth[i]);     }      printf(\"\\nDelta Array:\\n\");     for (int i = 0; i < n; ++i) {         printf(\"%.2f \", h_delta[i]);     }      printf(\"\\nError Array:\\n\");     for (int i = 0; i < n; ++i) {         printf(\"%.2f \", h_error[i]);     }     printf(\"\\n\");           free(h_pred);     free(h_truth);     free(h_delta);     free(h_error);     cudaFree(d_pred);     cudaFree(d_truth);     cudaFree(d_delta);     cudaFree(d_error);      return 0; }   "
    },
    {
        "id": "267",
        "c_code": "#include <stdio.h>   void flipKernel(float *array1, int width);  int main() {          int width = 4;     float array1[] = {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0};           flipKernel(array1, width);           printf(\"Flipped Array:\\n\");     for (int i = 0; i < width * width; i++) {         printf(\"%.1f \", array1[i]);     }     printf(\"\\n\");      return 0; }   void flipKernel(float *array1, int width) {     for (int current_index = 0; current_index < width * width / 2; current_index++) {         int replace = (width - 1 - current_index / width) * width + current_index % width;         float temp = array1[current_index];         array1[current_index] = array1[replace];         array1[replace] = temp;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void flipKernel(float* array1, int width) {     int current_index = blockIdx.x * blockDim.x + threadIdx.x;     int replace = (width - 1 - current_index / width) * width + current_index % width;      if (current_index < width * width / 2) {         float temp = array1[current_index];         array1[current_index] = array1[replace];         array1[replace] = temp;     } }  int main() {          int width = 4;            float* h_array1 = (float*)malloc(width * width * sizeof(float));           for (int i = 0; i < width * width; ++i) {         h_array1[i] = i + 1.0;      }           float* d_array1;     cudaMalloc((void**)&d_array1, width * width * sizeof(float));           cudaMemcpy(d_array1, h_array1, width * width * sizeof(float), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((width * width + block_size - 1) / block_size, 1);           flipKernel<<<grid_size, block_size>>>(d_array1, width);           cudaMemcpy(h_array1, d_array1, width * width * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Original Array:\\n\");     for (int i = 0; i < width * width; ++i) {         printf(\"%.2f \", h_array1[i]);         if ((i + 1) % width == 0) {             printf(\"\\n\");         }     }           free(h_array1);     cudaFree(d_array1);      return 0; }   "
    },
    {
        "id": "268",
        "c_code": "#include <stdio.h>   void get_conf_inds(const float *mlvl_conf, const float conf_thr, int *conf_inds, int dims);  int main() {          int dims = 5;     float mlvl_conf[] = {0.8, 0.6, 0.9, 0.4, 0.7};     float conf_thr = 0.7;     int conf_inds[dims];           get_conf_inds(mlvl_conf, conf_thr, conf_inds, dims);           printf(\"Confidence Indices:\\n\");     for (int i = 0; i < dims; i++) {         printf(\"%d \", conf_inds[i]);     }     printf(\"\\n\");      return 0; }   void get_conf_inds(const float *mlvl_conf, const float conf_thr, int *conf_inds, int dims) {     for (int tid = 0; tid < dims; tid++) {         if (mlvl_conf[tid] >= conf_thr) {             conf_inds[tid] = 1;         } else {             conf_inds[tid] = -1;         }     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void get_conf_inds(const float* mlvl_conf, const float conf_thr, int* conf_inds, int dims) {     int tid = blockIdx.x * blockDim.x + threadIdx.x;      if (tid < dims) {         conf_inds[tid] = (mlvl_conf[tid] >= conf_thr) ? 1 : -1;     } }  int main() {          int dims = 10;            float* h_mlvl_conf = (float*)malloc(dims * sizeof(float));     int* h_conf_inds = (int*)malloc(dims * sizeof(int));           for (int i = 0; i < dims; ++i) {         h_mlvl_conf[i] = i * 0.1;      }           float* d_mlvl_conf;     int* d_conf_inds;     cudaMalloc((void**)&d_mlvl_conf, dims * sizeof(float));     cudaMalloc((void**)&d_conf_inds, dims * sizeof(int));           cudaMemcpy(d_mlvl_conf, h_mlvl_conf, dims * sizeof(float), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((dims + block_size - 1) / block_size, 1);           float conf_thr = 0.5;            get_conf_inds<<<grid_size, block_size>>>(d_mlvl_conf, conf_thr, d_conf_inds, dims);           cudaMemcpy(h_conf_inds, d_conf_inds, dims * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"mlvl_conf Array:\\n\");     for (int i = 0; i < dims; ++i) {         printf(\"%.2f \", h_mlvl_conf[i]);     }      printf(\"\\nconf_inds Array:\\n\");     for (int i = 0; i < dims; ++i) {         printf(\"%d \", h_conf_inds[i]);     }     printf(\"\\n\");           free(h_mlvl_conf);     free(h_conf_inds);     cudaFree(d_mlvl_conf);     cudaFree(d_conf_inds);      return 0; }   "
    },
    {
        "id": "269",
        "c_code": "#include <stdio.h>   void cpuSearchPosShmem1(int key, int *gpu_key_arr, int *gpu_pos, int size);  int main() {          int size = 5;     int key = 7;     int gpu_key_arr[] = {3, 6, 8, 10, 12};     int gpu_pos;           cpuSearchPosShmem1(key, gpu_key_arr, &gpu_pos, size);           printf(\"Position of key %d: %d\\n\", key, gpu_pos);      return 0; }   void cpuSearchPosShmem1(int key, int *gpu_key_arr, int *gpu_pos, int size) {     for (int globalTx = 0; globalTx < size - 1; globalTx++) {         if (key >= gpu_key_arr[globalTx] && key < gpu_key_arr[globalTx + 1]) {             *gpu_pos = globalTx;             return;          }     }           if (key >= gpu_key_arr[size - 1]) {         *gpu_pos = size - 1;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void gpuSearchPosShmem1(int key, int* gpu_key_arr, int* gpu_pos, int size) {     int globalTx = blockIdx.x * blockDim.x + threadIdx.x;      if (globalTx < size - 1) {         if (key >= gpu_key_arr[globalTx] && key < gpu_key_arr[globalTx + 1]) {             *gpu_pos = globalTx;         }     } }  int main() {          int size = 10;            int* h_gpu_key_arr = (int*)malloc(size * sizeof(int));     int* h_gpu_pos = (int*)malloc(sizeof(int));           for (int i = 0; i < size; ++i) {         h_gpu_key_arr[i] = i * 10;      }           int* d_gpu_key_arr;     int* d_gpu_pos;     cudaMalloc((void**)&d_gpu_key_arr, size * sizeof(int));     cudaMalloc((void**)&d_gpu_pos, sizeof(int));           cudaMemcpy(d_gpu_key_arr, h_gpu_key_arr, size * sizeof(int), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((size + block_size - 1) / block_size, 1);           int key = 15;            gpuSearchPosShmem1<<<grid_size, block_size>>>(key, d_gpu_key_arr, d_gpu_pos, size);           cudaMemcpy(h_gpu_pos, d_gpu_pos, sizeof(int), cudaMemcpyDeviceToHost);           printf(\"gpu_key_arr Array:\\n\");     for (int i = 0; i < size; ++i) {         printf(\"%d \", h_gpu_key_arr[i]);     }      printf(\"\\nSearch Key: %d\\n\", key);     printf(\"Position: %d\\n\", *h_gpu_pos);           free(h_gpu_key_arr);     free(h_gpu_pos);     cudaFree(d_gpu_key_arr);     cudaFree(d_gpu_pos);      return 0; }   "
    },
    {
        "id": "270",
        "c_code": "#include <stdio.h>   void histogram(int *hist_out, unsigned char *img_in, int img_size, int nbr_bin);  int main() {          int nbr_bin = 256;       int img_size = 10;     unsigned char img_in[] = {1, 2, 3, 4, 5, 1, 2, 3, 4, 5};     int hist_out[nbr_bin];           histogram(hist_out, img_in, img_size, nbr_bin);           printf(\"Histogram:\\n\");     for (int i = 0; i < nbr_bin; i++) {         printf(\"%d \", hist_out[i]);     }     printf(\"\\n\");      return 0; }   void histogram(int *hist_out, unsigned char *img_in, int img_size, int nbr_bin) {          for (int i = 0; i < nbr_bin; i++) {         hist_out[i] = 0;     }           for (int i = 0; i < img_size; i++) {         hist_out[img_in[i]]++;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void HistogramKernel(int* hist, unsigned char* img_in, int img_size) {     int i = blockIdx.x * blockDim.x + threadIdx.x;      if (i < img_size) {         atomicAdd(&(hist[img_in[i]]), 1);     }     __syncthreads(); }  int main() {          int img_size = 100;            unsigned char* h_img_in = (unsigned char*)malloc(img_size * sizeof(unsigned char));     int* h_hist = (int*)malloc(256 * sizeof(int));           for (int i = 0; i < img_size; ++i) {         h_img_in[i] = i % 256;      }           unsigned char* d_img_in;     int* d_hist;     cudaMalloc((void**)&d_img_in, img_size * sizeof(unsigned char));     cudaMalloc((void**)&d_hist, 256 * sizeof(int));           cudaMemcpy(d_img_in, h_img_in, img_size * sizeof(unsigned char), cudaMemcpyHostToDevice);     cudaMemset(d_hist, 0, 256 * sizeof(int));           int block_size = 256;     dim3 grid_size((img_size + block_size - 1) / block_size, 1);           HistogramKernel<<<grid_size, block_size>>>(d_hist, d_img_in, img_size);           cudaMemcpy(h_hist, d_hist, 256 * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Image Array:\\n\");     for (int i = 0; i < img_size; ++i) {         printf(\"%d \", h_img_in[i]);     }      printf(\"\\nHistogram:\\n\");     for (int i = 0; i < 256; ++i) {         printf(\"Value %d: %d\\n\", i, h_hist[i]);     }           free(h_img_in);     free(h_hist);     cudaFree(d_img_in);     cudaFree(d_hist);      return 0; }   "
    },
    {
        "id": "271",
        "c_code": "#include <stdio.h>   void mean(float *A, float *means, int size_row, int size_col);  int main() {          int size_row = 3;     int size_col = 4;     float A[] = {1.0, 2.0, 3.0, 4.0,                  5.0, 6.0, 7.0, 8.0,                  9.0, 10.0, 11.0, 12.0};     float means[size_col];           mean(A, means, size_row, size_col);           printf(\"Means:\\n\");     for (int i = 0; i < size_col; i++) {         printf(\"%.2f \", means[i]);     }     printf(\"\\n\");      return 0; }   void mean(float *A, float *means, int size_row, int size_col) {     for (int idx = 0; idx < size_col; idx++) {         for (int i = 0; i < size_row; i++) {             means[idx] += A[idx * size_row + i];         }         means[idx] = means[idx] / size_row;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void mean(float* A, float* means, int size_row, int size_col) {     int idx = blockIdx.x * blockDim.x + threadIdx.x;      if (idx < size_col) {         float sum = 0.0f;         for (int i = 0; i < size_row; ++i) {             sum += A[idx * size_row + i];         }         means[idx] = sum / size_row;     } }  int main() {          int size_row = 3;      int size_col = 4;            float* h_A = (float*)malloc(size_row * size_col * sizeof(float));     float* h_means = (float*)malloc(size_col * sizeof(float));           for (int i = 0; i < size_row * size_col; ++i) {         h_A[i] = i + 1;      }           float* d_A;     float* d_means;     cudaMalloc((void**)&d_A, size_row * size_col * sizeof(float));     cudaMalloc((void**)&d_means, size_col * sizeof(float));           cudaMemcpy(d_A, h_A, size_row * size_col * sizeof(float), cudaMemcpyHostToDevice);           int block_size = 256;     dim3 grid_size((size_col + block_size - 1) / block_size, 1);           mean<<<grid_size, block_size>>>(d_A, d_means, size_row, size_col);           cudaMemcpy(h_means, d_means, size_col * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Array A:\\n\");     for (int i = 0; i < size_row * size_col; ++i) {         printf(\"%.2f \", h_A[i]);     }      printf(\"\\nColumn Means:\\n\");     for (int i = 0; i < size_col; ++i) {         printf(\"Column %d: %.2f\\n\", i, h_means[i]);     }           free(h_A);     free(h_means);     cudaFree(d_A);     cudaFree(d_means);      return 0; }   "
    },
    {
        "id": "272",
        "c_code": "#include <stdio.h>   float reduceCPU(float *data, int size);  int main() {          int size = 5;     float data[] = {1.0, 2.0, 3.0, 4.0, 5.0};           float result = reduceCPU(data, size);           printf(\"Sum: %.2f\\n\", result);      return 0; }   float reduceCPU(float *data, int size) {     float sum = data[0];     float c = (float)0.0;      for (int i = 1; i < size; i++) {         float y = data[i] - c;         float t = sum + y;         c = (t - sum) - y;         sum = t;     }      return sum; }   ",
        "cuda_code": "#include <stdio.h>   __device__ float sumreduce(float in) {     extern __shared__ float sdata[];     unsigned int tid = threadIdx.x;      sdata[tid] = in;     __syncthreads();      for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {         if (tid < s) {             sdata[tid] += sdata[tid + s];         }         __syncthreads();     }      return sdata[0]; }   __global__ void reduceKernel(float* input, float* output, int size) {     extern __shared__ float sdata[];      unsigned int tid = threadIdx.x;     unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;           sdata[tid] = (i < size) ? input[i] : 0;     __syncthreads();           for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {         if (tid < s) {             sdata[tid] += sdata[tid + s];         }         __syncthreads();     }           if (tid == 0) {         output[blockIdx.x] = sdata[0];     } }  int main() {          int size = 1024;             float* h_input = (float*)malloc(size * sizeof(float));     float* h_output = (float*)malloc(size * sizeof(float));           for (int i = 0; i < size; ++i) {         h_input[i] = 1.0f;       }           float* d_input;     float* d_output;     cudaMalloc((void**)&d_input, size * sizeof(float));     cudaMalloc((void**)&d_output, size * sizeof(float));           cudaMemcpy(d_input, h_input, size * sizeof(float), cudaMemcpyHostToDevice);           int block_size = 256;     int grid_size = (size + block_size - 1) / block_size;           reduceKernel<<<grid_size, block_size, block_size * sizeof(float)>>>(d_input, d_output, size);           cudaMemcpy(h_output, d_output, grid_size * sizeof(float), cudaMemcpyDeviceToHost);           float final_result = 0.0f;     for (int i = 0; i < grid_size; ++i) {         final_result += h_output[i];     }           printf(\"Final Result: %.2f\\n\", final_result);           free(h_input);     free(h_output);     cudaFree(d_input);     cudaFree(d_output);      return 0; }   "
    },
    {
        "id": "273",
        "c_code": "#include <stdio.h> #include <math.h>   void pythagoras(unsigned char *a, unsigned char *b, unsigned char *c, int size);  int main() {          int size = 5;     unsigned char a[] = {3, 4, 5, 6, 7};     unsigned char b[] = {4, 5, 6, 7, 8};     unsigned char c[size];           pythagoras(a, b, c, size);           printf(\"Resultant C:\\n\");     for (int i = 0; i < size; i++) {         printf(\"%d \", c[i]);     }     printf(\"\\n\");      return 0; }   void pythagoras(unsigned char *a, unsigned char *b, unsigned char *c, int size) {     for (int idx = 0; idx < size; idx++) {         float af = (float)(a[idx]);         float bf = (float)(b[idx]);         c[idx] = (unsigned char)sqrtf(af * af + bf * bf);     } }   ",
        "cuda_code": "#include <stdio.h> #include <math.h>   __global__ void pythagoras(unsigned char* a, unsigned char* b, unsigned char* c, int size) {     int idx = blockIdx.x * blockDim.x + threadIdx.x;      if (idx < size) {         float af = static_cast<float>(a[idx]);         float bf = static_cast<float>(b[idx]);         c[idx] = static_cast<unsigned char>(sqrtf(af * af + bf * bf));     } }  int main() {          int size = 1024;             unsigned char* h_a = (unsigned char*)malloc(size * sizeof(unsigned char));     unsigned char* h_b = (unsigned char*)malloc(size * sizeof(unsigned char));     unsigned char* h_c = (unsigned char*)malloc(size * sizeof(unsigned char));           for (int i = 0; i < size; ++i) {         h_a[i] = 100;           h_b[i] = 150;       }           unsigned char* d_a;     unsigned char* d_b;     unsigned char* d_c;     cudaMalloc((void**)&d_a, size * sizeof(unsigned char));     cudaMalloc((void**)&d_b, size * sizeof(unsigned char));     cudaMalloc((void**)&d_c, size * sizeof(unsigned char));           cudaMemcpy(d_a, h_a, size * sizeof(unsigned char), cudaMemcpyHostToDevice);     cudaMemcpy(d_b, h_b, size * sizeof(unsigned char), cudaMemcpyHostToDevice);           int block_size = 256;     int grid_size = (size + block_size - 1) / block_size;           pythagoras<<<grid_size, block_size>>>(d_a, d_b, d_c, size);           cudaMemcpy(h_c, d_c, size * sizeof(unsigned char), cudaMemcpyDeviceToHost);           printf(\"Results:\\n\");     for (int i = 0; i < size; ++i) {         printf(\"%u \", h_c[i]);     }     printf(\"\\n\");           free(h_a);     free(h_b);     free(h_c);     cudaFree(d_a);     cudaFree(d_b);     cudaFree(d_c);      return 0; }   "
    },
    {
        "id": "274",
        "c_code": "#include <stdio.h>   void avgpool_cpu(int n, float *input_im, float *output_im);  int main() {          int n = 3;     float input_im[] = {1.0, 2.0, 3.0,  9.0, 10.0, 11.0,  17.0, 18.0, 19.0};     float output_im[n];           avgpool_cpu(n, input_im, output_im);           printf(\"Output Image:\\n\");     for (int i = 0; i < n; i++) {         printf(\"%.2f \", output_im[i]);     }     printf(\"\\n\");      return 0; }   void avgpool_cpu(int n, float *input_im, float *output_im) {     for (int class_index = 0; class_index < n; class_index++) {         float *tmp_input = input_im + 169 * class_index;         float tmp = 0.0f;          for (int i = 0; i < 169; i++) {             tmp += tmp_input[i];         }          output_im[class_index] = tmp / 169.0;     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void avgpool(int n, float* input_im, float* output_im) {     int class_index = blockIdx.x * blockDim.x + threadIdx.x;      if (class_index < n) {         input_im += 169 * class_index;         float tmp = 0.0f;          for (int i = 0; i < 169; i++) {             tmp += input_im[i];         }          output_im[class_index] = tmp / 169.0;     } }  int main() {          int num_classes = 10;             float* h_input_im = (float*)malloc(169 * num_classes * sizeof(float));     float* h_output_im = (float*)malloc(num_classes * sizeof(float));           for (int i = 0; i < 169 * num_classes; ++i) {         h_input_im[i] = static_cast<float>(i);       }           float* d_input_im;     float* d_output_im;     cudaMalloc((void**)&d_input_im, 169 * num_classes * sizeof(float));     cudaMalloc((void**)&d_output_im, num_classes * sizeof(float));           cudaMemcpy(d_input_im, h_input_im, 169 * num_classes * sizeof(float), cudaMemcpyHostToDevice);           int block_size = 256;     int grid_size = (num_classes + block_size - 1) / block_size;           avgpool<<<grid_size, block_size>>>(num_classes, d_input_im, d_output_im);           cudaMemcpy(h_output_im, d_output_im, num_classes * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Results:\\n\");     for (int i = 0; i < num_classes; ++i) {         printf(\"%f \", h_output_im[i]);     }     printf(\"\\n\");           free(h_input_im);     free(h_output_im);     cudaFree(d_input_im);     cudaFree(d_output_im);      return 0; }   "
    },
    {
        "id": "275",
        "c_code": "#include <stdio.h>   void weighted_sum_cpu(float *a, float *b, float *s, int n, float *c);  int main() {          int n = 5;     float a[] = {1.0, 2.0, 3.0, 4.0, 5.0};     float b[] = {6.0, 7.0, 8.0, 9.0, 10.0};     float s[] = {0.2, 0.4, 0.6, 0.8, 1.0};     float c[n];           weighted_sum_cpu(a, b, s, n, c);           printf(\"Weighted Sum:\\n\");     for (int i = 0; i < n; i++) {         printf(\"%.2f \", c[i]);     }     printf(\"\\n\");      return 0; }   void weighted_sum_cpu(float *a, float *b, float *s, int n, float *c) {     for (int i = 0; i < n; ++i) {         c[i] = s[i] * a[i] + (1 - s[i]) * (b ? b[i] : 0);     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void weighted_sum_kernel(int n, float* a, float* b, float* s, float* c) {     int i = (blockIdx.x + blockIdx.y * gridDim.x) * blockDim.x + threadIdx.x;      if (i < n) {         c[i] = s[i] * a[i] + (1 - s[i]) * (b ? b[i] : 0);     } }  int main() {          int size = 1024;             float* h_a = (float*)malloc(size * sizeof(float));     float* h_b = (float*)malloc(size * sizeof(float));     float* h_s = (float*)malloc(size * sizeof(float));     float* h_c = (float*)malloc(size * sizeof(float));           for (int i = 0; i < size; ++i) {         h_a[i] = 1.0f;           h_b[i] = 2.0f;           h_s[i] = 0.5f;       }           float* d_a;     float* d_b;     float* d_s;     float* d_c;     cudaMalloc((void**)&d_a, size * sizeof(float));     cudaMalloc((void**)&d_b, size * sizeof(float));     cudaMalloc((void**)&d_s, size * sizeof(float));     cudaMalloc((void**)&d_c, size * sizeof(float));           cudaMemcpy(d_a, h_a, size * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_b, h_b, size * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_s, h_s, size * sizeof(float), cudaMemcpyHostToDevice);           int block_size = 256;     int grid_size = (size + block_size - 1) / block_size;           weighted_sum_kernel<<<grid_size, block_size>>>(size, d_a, d_b, d_s, d_c);           cudaMemcpy(h_c, d_c, size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Results:\\n\");     for (int i = 0; i < size; ++i) {         printf(\"%f \", h_c[i]);     }     printf(\"\\n\");           free(h_a);     free(h_b);     free(h_s);     free(h_c);     cudaFree(d_a);     cudaFree(d_b);     cudaFree(d_s);     cudaFree(d_c);      return 0; }   "
    },
    {
        "id": "276",
        "c_code": "#include <stdio.h> #include <math.h>   void kernel(float *x, int n);  int main() {          int n = 5;     float x[n];           kernel(x, n);           printf(\"Resultant x:\\n\");     for (int i = 0; i < n; i++) {         printf(\"%.4f \", x[i]);     }     printf(\"\\n\");      return 0; }   void kernel(float *x, int n) {     for (int i = 0; i < n; i++) {         double sum = 0;         for (int j = 0; j < 1000; j++) {             sum += sqrt(pow(3.14159, i)) / (float)j;         }         x[i] = sum;     } }   ",
        "cuda_code": "#include <stdio.h> #include <math.h>   __global__ void kernel(float* x, int n) {     int tid = threadIdx.x + blockIdx.x * blockDim.x;      for (int i = tid; i < n; i += blockDim.x * gridDim.x) {         double sum = 0;          for (int j = 0; j < 1000; j++) {             sum += sqrt(pow(3.14159, i)) / float(j);         }          x[i] = sum;     } }  int main() {          int size = 1024;             float* h_x = (float*)malloc(size * sizeof(float));           float* d_x;     cudaMalloc((void**)&d_x, size * sizeof(float));           int block_size = 256;     int grid_size = (size + block_size - 1) / block_size;           kernel<<<grid_size, block_size>>>(d_x, size);           cudaMemcpy(h_x, d_x, size * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Results:\\n\");     for (int i = 0; i < size; ++i) {         printf(\"%f \", h_x[i]);     }     printf(\"\\n\");           free(h_x);     cudaFree(d_x);      return 0; }   "
    },
    {
        "id": "277",
        "c_code": "#include <stdio.h>   void MMDSelfComputeWithSum(float *x_average, int size_x, float *distance_matrix);  int main() {          int size_x = 4;     float x_average[] = {1.5, 2.0, 3.0, 4.5};     float distance_matrix[size_x * size_x];           MMDSelfComputeWithSum(x_average, size_x, distance_matrix);           printf(\"Distance Matrix:\\n\");     for (int i = 0; i < size_x; i++) {         for (int j = 0; j < size_x; j++) {             printf(\"%.2f \", distance_matrix[i * size_x + j]);         }         printf(\"\\n\");     }      return 0; }   void MMDSelfComputeWithSum(float *x_average, int size_x, float *distance_matrix) {     for (int i = 0; i < size_x; i++) {         for (int j = i; j < size_x; j++) {             distance_matrix[i * size_x + j] = x_average[i] * x_average[j];             distance_matrix[j * size_x + i] = distance_matrix[i * size_x + j];          }     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void MMDSelfComputeWithSum(float* x_average, int size_x, float* distance_matrix) {     int block_id = blockIdx.x;     int thread_id = threadIdx.x;      for (int i = block_id; i < size_x; i += gridDim.x) {         for (int j = thread_id + i; j < size_x; j += blockDim.x) {             distance_matrix[i * size_x + j] = x_average[i] * x_average[j];         }     } }  int main() {          int size_x = 1024;             float* h_x_average = (float*)malloc(size_x * sizeof(float));     float* h_distance_matrix = (float*)malloc(size_x * size_x * sizeof(float));           for (int i = 0; i < size_x; ++i) {         h_x_average[i] = float(i);       }           float* d_x_average;     float* d_distance_matrix;     cudaMalloc((void**)&d_x_average, size_x * sizeof(float));     cudaMalloc((void**)&d_distance_matrix, size_x * size_x * sizeof(float));           cudaMemcpy(d_x_average, h_x_average, size_x * sizeof(float), cudaMemcpyHostToDevice);           int block_size = 256;     int grid_size = (size_x + block_size - 1) / block_size;           MMDSelfComputeWithSum<<<grid_size, block_size>>>(d_x_average, size_x, d_distance_matrix);           cudaMemcpy(h_distance_matrix, d_distance_matrix, size_x * size_x * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Results:\\n\");     for (int i = 0; i < size_x; ++i) {         for (int j = 0; j < size_x; ++j) {             printf(\"%f \", h_distance_matrix[i * size_x + j]);         }         printf(\"\\n\");     }           free(h_x_average);     free(h_distance_matrix);     cudaFree(d_x_average);     cudaFree(d_distance_matrix);      return 0; }   "
    },
    {
        "id": "278",
        "c_code": "#include <stdio.h>   void matVecRowSub_cpu(const double *mat, const double *vec, double *buf, int m, int n);  int main() {          int m = 3;     int n = 4;     double mat[] = {1.0, 2.0, 3.0, 4.0,                     5.0, 6.0, 7.0, 8.0,                     9.0, 10.0, 11.0, 12.0};     double vec[] = {2.0, 4.0, 6.0, 8.0};     double buf[m * n];           matVecRowSub_cpu(mat, vec, buf, m, n);           printf(\"Resultant Buffer:\\n\");     for (int i = 0; i < m; i++) {         for (int j = 0; j < n; j++) {             printf(\"%.2f \", buf[i * n + j]);         }         printf(\"\\n\");     }      return 0; }   void matVecRowSub_cpu(const double *mat, const double *vec, double *buf, int m, int n) {     for (int index = 0; index < m * n; index++) {         int i = index / n;         int j = index % n;         buf[i * n + j] = mat[i * n + j] - vec[j];     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void matVecRowSubKernel(const double* mat, const double* vec, double* buf, int m, int n) {     int index = blockIdx.x * blockDim.x + threadIdx.x;      if (index < m * n) {         int i = index / n;         int j = index % n;         buf[i * n + j] = mat[i * n + j] - vec[j];     } }  int main() {          int m = 4;             int n = 3;             double* h_mat = (double*)malloc(m * n * sizeof(double));     double* h_vec = (double*)malloc(n * sizeof(double));     double* h_buf = (double*)malloc(m * n * sizeof(double));           for (int i = 0; i < m * n; ++i) {         h_mat[i] = i;       }      for (int i = 0; i < n; ++i) {         h_vec[i] = i;       }           double* d_mat;     double* d_vec;     double* d_buf;     cudaMalloc((void**)&d_mat, m * n * sizeof(double));     cudaMalloc((void**)&d_vec, n * sizeof(double));     cudaMalloc((void**)&d_buf, m * n * sizeof(double));           cudaMemcpy(d_mat, h_mat, m * n * sizeof(double), cudaMemcpyHostToDevice);     cudaMemcpy(d_vec, h_vec, n * sizeof(double), cudaMemcpyHostToDevice);           int block_size = 256;     int grid_size = (m * n + block_size - 1) / block_size;           matVecRowSubKernel<<<grid_size, block_size>>>(d_mat, d_vec, d_buf, m, n);           cudaMemcpy(h_buf, d_buf, m * n * sizeof(double), cudaMemcpyDeviceToHost);           printf(\"Results:\\n\");     for (int i = 0; i < m; ++i) {         for (int j = 0; j < n; ++j) {             printf(\"%f \", h_buf[i * n + j]);         }         printf(\"\\n\");     }           free(h_mat);     free(h_vec);     free(h_buf);     cudaFree(d_mat);     cudaFree(d_vec);     cudaFree(d_buf);      return 0; }   "
    },
    {
        "id": "279",
        "c_code": "#include <stdio.h>   void transKernel(float *array1, float *array2, int width);  int main() {          int width = 3;     float array1[] = {1.0, 2.0, 3.0,                       4.0, 5.0, 6.0,                       7.0, 8.0, 9.0};     float array2[width * width];           transKernel(array1, array2, width);           printf(\"Transposed Array:\\n\");     for (int x = 0; x < width; x++) {         for (int y = 0; y < width; y++) {             printf(\"%.2f \", array2[x * width + y]);         }         printf(\"\\n\");     }      return 0; }   void transKernel(float *array1, float *array2, int width) {     for (int x = 0; x < width; x++) {         for (int y = 0; y < width; y++) {             int current_index = x * width + y;             int replace = y * width + x;             array2[replace] = array1[current_index];         }     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void transKernel(float* array1, float* array2, int width) {     int current_index =         (blockIdx.y * blockDim.y + threadIdx.y) * width +         (blockIdx.x * blockDim.x + threadIdx.x);     int replace =         (blockIdx.x * blockDim.x + threadIdx.x) * width +         blockIdx.y * blockDim.y + threadIdx.y;      if (current_index < width * width) {         array2[replace] = array1[current_index];     } }  int main() {          int width = 4;             float* h_array1 = (float*)malloc(width * width * sizeof(float));     float* h_array2 = (float*)malloc(width * width * sizeof(float));           for (int i = 0; i < width * width; ++i) {         h_array1[i] = i;       }           float* d_array1;     float* d_array2;     cudaMalloc((void**)&d_array1, width * width * sizeof(float));     cudaMalloc((void**)&d_array2, width * width * sizeof(float));           cudaMemcpy(d_array1, h_array1, width * width * sizeof(float), cudaMemcpyHostToDevice);           dim3 block_size(2, 2);       dim3 grid_size((width + block_size.x - 1) / block_size.x, (width + block_size.y - 1) / block_size.y);           transKernel<<<grid_size, block_size>>>(d_array1, d_array2, width);           cudaMemcpy(h_array2, d_array2, width * width * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Results:\\n\");     for (int i = 0; i < width; ++i) {         for (int j = 0; j < width; ++j) {             printf(\"%f \", h_array2[i * width + j]);         }         printf(\"\\n\");     }           free(h_array1);     free(h_array2);     cudaFree(d_array1);     cudaFree(d_array2);      return 0; }   "
    },
    {
        "id": "280",
        "c_code": "#include <stdio.h>   void rowSumSquare_cpu(const double *mat, double *buf, int m, int n);  int main() {          int m = 3;     int n = 4;     double mat[] = {1.0, 2.0, 3.0, 4.0,                     5.0, 6.0, 7.0, 8.0,                     9.0, 10.0, 11.0, 12.0};     double buf[m];           rowSumSquare_cpu(mat, buf, m, n);           printf(\"Row Sum Squares:\\n\");     for (int i = 0; i < m; i++) {         printf(\"%.2f \", buf[i]);     }     printf(\"\\n\");      return 0; }   void rowSumSquare_cpu(const double *mat, double *buf, int m, int n) {     for (int i = 0; i < m; i++) {         double sum = 0.0;       ",
        "cuda_code": "#include <stdio.h>   __global__ void rowSumSquareKernel(const double* mat, double* buf, int m, int n) {     int i = blockIdx.x * blockDim.x + threadIdx.x;      if (i < m) {         double sum = 0.0;          for (int j = 0; j < n; j++) {             double a = mat[i * n + j];             sum += a * a;         }          buf[i] = sum;     } }  int main() {          int m = 3;       int n = 4;             double* h_mat = (double*)malloc(m * n * sizeof(double));     double* h_buf = (double*)malloc(m * sizeof(double));           for (int i = 0; i < m * n; ++i) {         h_mat[i] = i;       }           double* d_mat;     double* d_buf;     cudaMalloc((void**)&d_mat, m * n * sizeof(double));     cudaMalloc((void**)&d_buf, m * sizeof(double));           cudaMemcpy(d_mat, h_mat, m * n * sizeof(double), cudaMemcpyHostToDevice);           dim3 block_size(2, 1);       dim3 grid_size((m + block_size.x - 1) / block_size.x, 1);           rowSumSquareKernel<<<grid_size, block_size>>>(d_mat, d_buf, m, n);           cudaMemcpy(h_buf, d_buf, m * sizeof(double), cudaMemcpyDeviceToHost);           printf(\"Results:\\n\");     for (int i = 0; i < m; ++i) {         printf(\"%f \", h_buf[i]);     }     printf(\"\\n\");           free(h_mat);     free(h_buf);     cudaFree(d_mat);     cudaFree(d_buf);      return 0; }   "
    },
    {
        "id": "281",
        "c_code": "#include <stdio.h>   void matrixTranspose_cpu(int *in_mat, int *out_mat, int dim_rows, int dim_cols);  int main() {          int dim_rows = 3;     int dim_cols = 4;     int in_mat[] = {1, 2, 3, 4,                     5, 6, 7, 8,                     9, 10, 11, 12};     int out_mat[dim_cols * dim_rows];           matrixTranspose_cpu(in_mat, out_mat, dim_rows, dim_cols);           printf(\"Transposed Matrix:\\n\");     for (int i = 0; i < dim_cols; i++) {         for (int j = 0; j < dim_rows; j++) {             printf(\"%d \", out_mat[i * dim_rows + j]);         }         printf(\"\\n\");     }      return 0; }   void matrixTranspose_cpu(int *in_mat, int *out_mat, int dim_rows, int dim_cols) {     for (int i = 0; i < dim_rows; ++i) {         for (int j = 0; j < dim_cols; ++j) {             unsigned int new_pos = j * dim_rows + i;             out_mat[new_pos] = in_mat[i * dim_cols + j];         }     } }   ",
        "cuda_code": "#include <stdio.h>   __global__ void matrixTranspose(int* in_mat, int* out_mat, int dim_rows, int dim_cols) {     int row = threadIdx.y + blockIdx.y * blockDim.y;     int col = threadIdx.x + blockIdx.x * blockDim.x;      if (row < dim_rows && col < dim_cols) {         unsigned int new_pos = col * dim_rows + row;         out_mat[new_pos] = in_mat[row * dim_cols + col];     } }  int main() {          int dim_rows = 3;       int dim_cols = 4;             int* h_in_mat = (int*)malloc(dim_rows * dim_cols * sizeof(int));     int* h_out_mat = (int*)malloc(dim_cols * dim_rows * sizeof(int));           for (int i = 0; i < dim_rows * dim_cols; ++i) {         h_in_mat[i] = i;       }           int* d_in_mat;     int* d_out_mat;     cudaMalloc((void**)&d_in_mat, dim_rows * dim_cols * sizeof(int));     cudaMalloc((void**)&d_out_mat, dim_cols * dim_rows * sizeof(int));           cudaMemcpy(d_in_mat, h_in_mat, dim_rows * dim_cols * sizeof(int), cudaMemcpyHostToDevice);           dim3 block_size(2, 2);       dim3 grid_size((dim_cols + block_size.x - 1) / block_size.x, (dim_rows + block_size.y - 1) / block_size.y);           matrixTranspose<<<grid_size, block_size>>>(d_in_mat, d_out_mat, dim_rows, dim_cols);           cudaMemcpy(h_out_mat, d_out_mat, dim_cols * dim_rows * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Original Matrix:\\n\");     for (int i = 0; i < dim_rows; ++i) {         for (int j = 0; j < dim_cols; ++j) {             printf(\"%d \", h_in_mat[i * dim_cols + j]);         }         printf(\"\\n\");     }      printf(\"\\nTransposed Matrix:\\n\");     for (int i = 0; i < dim_cols; ++i) {         for (int j = 0; j < dim_rows; ++j) {             printf(\"%d \", h_out_mat[i * dim_rows + j]);         }         printf(\"\\n\");     }           free(h_in_mat);     free(h_out_mat);     cudaFree(d_in_mat);     cudaFree(d_out_mat);      return 0; }   "
    },
    {
        "id": "282",
        "c_code": "#include <stdio.h>   void analysis(int D[], int L[], int R[], int N);  int main() {          int N = 5;     int D[] = {0, 1, 2, 3, 4};     int L[] = {0, 1, 2, 3, 4};     int R[] = {0, 1, 2, 3, 4};           analysis(D, L, R, N);           printf(\"Results:\\n\");     printf(\"D: \");     for (int i = 0; i < N; i++) {         printf(\"%d \", D[i]);     }     printf(\"\\n\");     printf(\"L: \");     for (int i = 0; i < N; i++) {         printf(\"%d \", L[i]);     }     printf(\"\\n\");     printf(\"R: \");     for (int i = 0; i < N; i++) {         printf(\"%d \", R[i]);     }     printf(\"\\n\");      return 0; }   void analysis(int D[], int L[], int R[], int N) {     int id;     for (id = 0; id < N; id++) {         int label = L[id];         int ref;         if (label == id) {             do {                 label = R[ref = label];             } while (ref ^ label);             R[id] = label;         }     } }   ",
        "cuda_code": "#include <stdio.h>   #include <cuda_runtime.h>   __global__ void analysis(int D[], int L[], int R[], int N) {     int id = blockIdx.x * blockDim.x + blockIdx.y * blockDim.x * gridDim.x + threadIdx.x;      if (id >= N)         return;      int label = L[id];     int ref;      if (label == id) {         do {             label = R[ref = label];         } while (ref ^ label);          R[id] = label;     } }  int main() {          int N = 100;             int* h_D = (int*)malloc(N * sizeof(int));     int* h_L = (int*)malloc(N * sizeof(int));     int* h_R = (int*)malloc(N * sizeof(int));           for (int i = 0; i < N; ++i) {         h_D[i] = i;         h_L[i] = i;         h_R[i] = i;     }           int* d_D;     int* d_L;     int* d_R;      cudaMalloc((void**)&d_D, N * sizeof(int));     cudaMalloc((void**)&d_L, N * sizeof(int));     cudaMalloc((void**)&d_R, N * sizeof(int));           cudaMemcpy(d_D, h_D, N * sizeof(int), cudaMemcpyHostToDevice);     cudaMemcpy(d_L, h_L, N * sizeof(int), cudaMemcpyHostToDevice);     cudaMemcpy(d_R, h_R, N * sizeof(int), cudaMemcpyHostToDevice);           dim3 block_size(256);       dim3 grid_size((N + block_size.x - 1) / block_size.x);           analysis<<<grid_size, block_size>>>(d_D, d_L, d_R, N);           cudaMemcpy(h_R, d_R, N * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Result array (R):\\n\");     for (int i = 0; i < N; ++i) {         printf(\"%d \", h_R[i]);     }     printf(\"\\n\");           free(h_D);     free(h_L);     free(h_R);     cudaFree(d_D);     cudaFree(d_L);     cudaFree(d_R);      return 0; }   "
    },
    {
        "id": "283",
        "c_code": "#include <stdio.h>   void Transpose2d(float *array_transpose, float *array, const int r, const int c);  int main() {          int r = 3;     int c = 4;     float array[] = {1.0, 2.0, 3.0, 4.0,                      5.0, 6.0, 7.0, 8.0,                      9.0, 10.0, 11.0, 12.0};     float array_transpose[c * r];           Transpose2d(array_transpose, array, r, c);           printf(\"Original Array:\\n\");     for (int i = 0; i < r; i++) {         for (int j = 0; j < c; j++) {             printf(\"%.2f \", array[i * c + j]);         }         printf(\"\\n\");     }      printf(\"\\nTransposed Array:\\n\");     for (int i = 0; i < c; i++) {         for (int j = 0; j < r; j++) {             printf(\"%.2f \", array_transpose[i * r + j]);         }         printf(\"\\n\");     }      return 0; }   void Transpose2d(float *array_transpose, float *array, const int r, const int c) {     int i, j;     for (i = 0; i < r; i++) {         for (j = 0; j < c; j++) {             array_transpose[j * r + i] = array[i * c + j];         }     } }   ",
        "cuda_code": "#include <stdio.h>   #include <cuda_runtime.h>   __global__ void Kernel_Transpose2d(float* dev_transposeArray, float* dev_array, const int r, const int c) {     unsigned int i = blockDim.x * blockIdx.x + threadIdx.x;     unsigned int j = blockDim.y * blockIdx.y + threadIdx.y;      if (i >= r || j >= c)         return;      int idx_transposeArray, idx_array;     idx_array = i * c + j;     idx_transposeArray = j * r + i;     dev_transposeArray[idx_transposeArray] = dev_array[idx_array]; }  int main() {          const int r = 4;      const int c = 3;            float* h_dev_transposeArray = (float*)malloc(r * c * sizeof(float));     float* h_dev_array = (float*)malloc(r * c * sizeof(float));           for (int i = 0; i < r * c; ++i) {         h_dev_array[i] = i;     }           float* d_dev_transposeArray;     float* d_dev_array;      cudaMalloc((void**)&d_dev_transposeArray, r * c * sizeof(float));     cudaMalloc((void**)&d_dev_array, r * c * sizeof(float));           cudaMemcpy(d_dev_array, h_dev_array, r * c * sizeof(float), cudaMemcpyHostToDevice);           dim3 block_size(16, 16);      dim3 grid_size((r + block_size.x - 1) / block_size.x, (c + block_size.y - 1) / block_size.y);           Kernel_Transpose2d<<<grid_size, block_size>>>(d_dev_transposeArray, d_dev_array, r, c);           cudaMemcpy(h_dev_transposeArray, d_dev_transposeArray, r * c * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Original Array:\\n\");     for (int i = 0; i < r; ++i) {         for (int j = 0; j < c; ++j) {             printf(\"%.2f\\t\", h_dev_array[i * c + j]);         }         printf(\"\\n\");     }      printf(\"\\nTransposed Array:\\n\");     for (int i = 0; i < c; ++i) {         for (int j = 0; j < r; ++j) {             printf(\"%.2f\\t\", h_dev_transposeArray[i * r + j]);         }         printf(\"\\n\");     }           free(h_dev_transposeArray);     free(h_dev_array);     cudaFree(d_dev_transposeArray);     cudaFree(d_dev_array);      return 0; }   "
    },
    {
        "id": "284",
        "c_code": "#include <stdio.h>   void getMeanImage_cpu(const double *images, double *meanImage, int imageNum, int pixelNum);  int main() {          int imageNum = 3;     int pixelNum = 4;     double images[] = {1.0, 2.0, 3.0, 4.0,                        5.0, 6.0, 7.0, 8.0,                        9.0, 10.0, 11.0, 12.0};     double meanImage[pixelNum];           getMeanImage_cpu(images, meanImage, imageNum, pixelNum);           printf(\"Mean Image:\\n\");     for (int col = 0; col < pixelNum; col++) {         printf(\"%.2f \", meanImage[col]);     }     printf(\"\\n\");      return 0; }   void getMeanImage_cpu(const double *images, double *meanImage, int imageNum, int pixelNum) {     for (int col = 0; col < pixelNum; col++) {         meanImage[col] = 0.0;         for (int row = 0; row < imageNum; ++row) {             meanImage[col] += images[row * pixelNum + col];         }         meanImage[col] /= imageNum;     } }   ",
        "cuda_code": "#include <stdio.h>   #include <cuda_runtime.h>   __global__ void getMeanImage(const double* images, double* meanImage, int imageNum, int pixelNum) {     int col = blockIdx.x * blockDim.x + threadIdx.x;      if (col >= pixelNum) {         return;     }      meanImage[col] = 0.0;      for (int row = 0; row < imageNum; ++row) {         meanImage[col] += images[row * pixelNum + col];     }      meanImage[col] /= imageNum; }  int main() {          const int imageNum = 100;      const int pixelNum = 64;             double* h_images = (double*)malloc(imageNum * pixelNum * sizeof(double));     double* h_meanImage = (double*)malloc(pixelNum * sizeof(double));           for (int i = 0; i < imageNum * pixelNum; ++i) {         h_images[i] = i;      }           double* d_images;     double* d_meanImage;      cudaMalloc((void**)&d_images, imageNum * pixelNum * sizeof(double));     cudaMalloc((void**)&d_meanImage, pixelNum * sizeof(double));           cudaMemcpy(d_images, h_images, imageNum * pixelNum * sizeof(double), cudaMemcpyHostToDevice);           int threadsPerBlock = 256;      int blocksPerGrid = (pixelNum + threadsPerBlock - 1) / threadsPerBlock;           getMeanImage<<<blocksPerGrid, threadsPerBlock>>>(d_images, d_meanImage, imageNum, pixelNum);           cudaMemcpy(h_meanImage, d_meanImage, pixelNum * sizeof(double), cudaMemcpyDeviceToHost);           printf(\"Mean Image:\\n\");     for (int i = 0; i < pixelNum; ++i) {         printf(\"%.2f\\t\", h_meanImage[i]);     }     printf(\"\\n\");           free(h_images);     free(h_meanImage);     cudaFree(d_images);     cudaFree(d_meanImage);      return 0; }   "
    },
    {
        "id": "285",
        "c_code": "#include <stdio.h>   void Avg(float *array_avg, float *array, const int r, const int c);  int main() {          int r = 3;     int c = 4;     float array[] = {1.0, 2.0, 3.0, 4.0,                      5.0, 6.0, 7.0, 8.0,                      9.0, 10.0, 11.0, 12.0};     float array_avg[r];           Avg(array_avg, array, r, c);           printf(\"Average Array:\\n\");     for (int i = 0; i < r; i++) {         printf(\"%.2f \", array_avg[i]);     }     printf(\"\\n\");      return 0; }   void Avg(float *array_avg, float *array, const int r, const int c) {     float sum;     for (int i = 0; i < r; i++) {         sum = 0.0;         for (int j = 0; j < c; j++) {             sum += array[i * c + j];         }         array_avg[i] = sum / c;     } }   ",
        "cuda_code": "#include <stdio.h>   #include <cuda_runtime.h>   __global__ void Kernel_Avg(float* dev_arrayMax, float* dev_array, const int r, const int c) {     unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;     int N = r;     float sum;     int i;      while (tid < N) {         i = tid;         sum = 0.0;          for (int j = 0; j < c; j++) {             sum += dev_array[i * c + j];         }          dev_arrayMax[i] = sum / c;         tid += gridDim.x * blockDim.x;     } }  int main() {          const int r = 100;      const int c = 64;             float* h_array = (float*)malloc(r * c * sizeof(float));     float* h_arrayMax = (float*)malloc(r * sizeof(float));           for (int i = 0; i < r * c; ++i) {         h_array[i] = i;      }           float* d_array;     float* d_arrayMax;      cudaMalloc((void**)&d_array, r * c * sizeof(float));     cudaMalloc((void**)&d_arrayMax, r * sizeof(float));           cudaMemcpy(d_array, h_array, r * c * sizeof(float), cudaMemcpyHostToDevice);           int threadsPerBlock = 256;      int blocksPerGrid = (r + threadsPerBlock - 1) / threadsPerBlock;           Kernel_Avg<<<blocksPerGrid, threadsPerBlock>>>(d_arrayMax, d_array, r, c);           cudaMemcpy(h_arrayMax, d_arrayMax, r * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Average for each row:\\n\");     for (int i = 0; i < r; ++i) {         printf(\"%.2f\\t\", h_arrayMax[i]);     }     printf(\"\\n\");           free(h_array);     free(h_arrayMax);     cudaFree(d_array);     cudaFree(d_arrayMax);      return 0; }   "
    },
    {
        "id": "286",
        "c_code": "#include <stdio.h>   void smallCorrelation_cpu(float *L, float *innerSums, int innerSumsLength);  int main() {          int innerSumsLength = 5;     float innerSums[] = {1.0, 2.0, 3.0, 4.0, 5.0};     float L[innerSumsLength];           smallCorrelation_cpu(L, innerSums, innerSumsLength);           printf(\"Correlation Array (L):\\n\");     for (int u = 0; u < innerSumsLength; u++) {         printf(\"%.2f \", L[u]);     }     printf(\"\\n\");      return 0; }   void smallCorrelation_cpu(float *L, float *innerSums, int innerSumsLength) {     for (int u = 0; u < innerSumsLength; u++) {         int realIdx = 2 * u;         int imagIdx = realIdx + 1;         L[u] = (innerSums[realIdx] * innerSums[realIdx]) + (innerSums[imagIdx] * innerSums[imagIdx]);     } }   ",
        "cuda_code": "#include <stdio.h>   #include <cuda_runtime.h>   __global__ void smallCorrelation(float* L, float* innerSums, int innerSumsLength) {     int u = (blockIdx.x * blockDim.x) + threadIdx.x;     if (u >= innerSumsLength)         return;      int realIdx = 2 * u;     int imagIdx = realIdx + 1;     L[u] = (innerSums[realIdx] * innerSums[realIdx]) + (innerSums[imagIdx] * innerSums[imagIdx]); }  int main() {          const int innerSumsLength = 100;            float* h_L = (float*)malloc(innerSumsLength * sizeof(float));     float* h_innerSums = (float*)malloc(2 * innerSumsLength * sizeof(float));            for (int i = 0; i < 2 * innerSumsLength; ++i) {         h_innerSums[i] = i;      }           float* d_L;     float* d_innerSums;      cudaMalloc((void**)&d_L, innerSumsLength * sizeof(float));     cudaMalloc((void**)&d_innerSums, 2 * innerSumsLength * sizeof(float));           cudaMemcpy(d_innerSums, h_innerSums, 2 * innerSumsLength * sizeof(float), cudaMemcpyHostToDevice);           int threadsPerBlock = 256;      int blocksPerGrid = (innerSumsLength + threadsPerBlock - 1) / threadsPerBlock;           smallCorrelation<<<blocksPerGrid, threadsPerBlock>>>(d_L, d_innerSums, innerSumsLength);           cudaMemcpy(h_L, d_L, innerSumsLength * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Resultant array:\\n\");     for (int i = 0; i < innerSumsLength; ++i) {         printf(\"%.2f\\t\", h_L[i]);     }     printf(\"\\n\");           free(h_L);     free(h_innerSums);     cudaFree(d_L);     cudaFree(d_innerSums);      return 0; }   "
    },
    {
        "id": "287",
        "c_code": "#include <stdio.h>   void cpuDecodeBitstream(unsigned short *encoded, unsigned short *decoded, int size);  int main() {          int size = 5;     unsigned short encoded[] = {1, 0, 0, 1, 1, 1, 1, 0, 1, 0};     unsigned short decoded[size];           cpuDecodeBitstream(encoded, decoded, size);           printf(\"Decoded Bitstream:\\n\");     for (int i = 0; i < size; i++) {         printf(\"%hu \", decoded[i]);     }     printf(\"\\n\");      return 0; }   void cpuDecodeBitstream(unsigned short *encoded, unsigned short *decoded, int size) {     for (int i = 0; i < size; i++) {         int bit_index = (i * 2) + 2;         unsigned short curr_bit = encoded[bit_index];         decoded[bit_index] = !encoded[bit_index - 1] ^ curr_bit;         decoded[bit_index + 1] = curr_bit ^ encoded[bit_index + 1];     } }   ",
        "cuda_code": "#include <stdio.h>   #include <cuda_runtime.h>   __global__ void cudaDecodeBitstream(unsigned short* encoded, unsigned short* decoded, int size) {     int bit_index = (((blockIdx.x * blockDim.x) + threadIdx.x) * 2) + 2;     if (bit_index >= size)         return;      unsigned short curr_bit = encoded[bit_index];     decoded[bit_index] = !encoded[bit_index - 1] ^ curr_bit;     decoded[bit_index + 1] = curr_bit ^ encoded[bit_index + 1]; }  int main() {          const int size = 100;            unsigned short* h_encoded = (unsigned short*)malloc(size * sizeof(unsigned short));     unsigned short* h_decoded = (unsigned short*)malloc(size * sizeof(unsigned short));           for (int i = 0; i < size; ++i) {         h_encoded[i] = i;      }           unsigned short* d_encoded;     unsigned short* d_decoded;      cudaMalloc((void**)&d_encoded, size * sizeof(unsigned short));     cudaMalloc((void**)&d_decoded, size * sizeof(unsigned short));           cudaMemcpy(d_encoded, h_encoded, size * sizeof(unsigned short), cudaMemcpyHostToDevice);           int threadsPerBlock = 256;      int blocksPerGrid = (size + threadsPerBlock - 1) / threadsPerBlock;           cudaDecodeBitstream<<<blocksPerGrid, threadsPerBlock>>>(d_encoded, d_decoded, size);           cudaMemcpy(h_decoded, d_decoded, size * sizeof(unsigned short), cudaMemcpyDeviceToHost);           printf(\"Decoded array:\\n\");     for (int i = 0; i < size; ++i) {         printf(\"%hu\\t\", h_decoded[i]);     }     printf(\"\\n\");           free(h_encoded);     free(h_decoded);     cudaFree(d_encoded);     cudaFree(d_decoded);      return 0; }   "
    },
    {
        "id": "288",
        "c_code": "#include <stdio.h>   void vectorMatrixMult(long int totalPixels, float *matrix, float *vector, float *out);  int main() {          long int totalPixels = 3;     float matrix[] = {1.0, 2.0, 3.0,                       4.0, 5.0, 6.0,                       7.0, 8.0, 9.0};     float vector[] = {2.0, 1.0, 3.0};     float out[totalPixels];           vectorMatrixMult(totalPixels, matrix, vector, out);           printf(\"Resultant Vector:\\n\");     for (long int i = 0; i < totalPixels; i++) {         printf(\"%.2f \", out[i]);     }     printf(\"\\n\");      return 0; }   void vectorMatrixMult(long int totalPixels, float *matrix, float *vector, float *out) {     for (long int i = 0; i < totalPixels; i++) {         float sum = 0.0;         for (long int j = 0; j < totalPixels; j++) {             sum += matrix[i * totalPixels + j] * vector[j];         }         out[i] = sum;     } }   ",
        "cuda_code": "#include <stdio.h>   #include <cuda_runtime.h>   __global__ void vectorMatrixMult(long int totalPixels, float* matrix, float* vector, float* out) {     int index = blockIdx.x * blockDim.x + threadIdx.x;     int stride = blockDim.x * gridDim.x;      for (long int i = index; i < totalPixels; i += stride) {         float sum = 0.0;          for (long int j = 0; j < totalPixels; j++) {             sum += matrix[i * totalPixels + j] * vector[j];         }          out[i] = sum;     } }  int main() {          const long int totalPixels = 100;            float* h_matrix = (float*)malloc(totalPixels * totalPixels * sizeof(float));     float* h_vector = (float*)malloc(totalPixels * sizeof(float));     float* h_out = (float*)malloc(totalPixels * sizeof(float));           for (long int i = 0; i < totalPixels * totalPixels; ++i) {         h_matrix[i] = i;      }      for (long int i = 0; i < totalPixels; ++i) {         h_vector[i] = i;      }           float* d_matrix;     float* d_vector;     float* d_out;      cudaMalloc((void**)&d_matrix, totalPixels * totalPixels * sizeof(float));     cudaMalloc((void**)&d_vector, totalPixels * sizeof(float));     cudaMalloc((void**)&d_out, totalPixels * sizeof(float));           cudaMemcpy(d_matrix, h_matrix, totalPixels * totalPixels * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_vector, h_vector, totalPixels * sizeof(float), cudaMemcpyHostToDevice);           int threadsPerBlock = 256;      int blocksPerGrid = (totalPixels + threadsPerBlock - 1) / threadsPerBlock;           vectorMatrixMult<<<blocksPerGrid, threadsPerBlock>>>(totalPixels, d_matrix, d_vector, d_out);           cudaMemcpy(h_out, d_out, totalPixels * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result array:\\n\");     for (long int i = 0; i < totalPixels; ++i) {         printf(\"%f\\t\", h_out[i]);     }     printf(\"\\n\");           free(h_matrix);     free(h_vector);     free(h_out);     cudaFree(d_matrix);     cudaFree(d_vector);     cudaFree(d_out);      return 0; }   "
    },
    {
        "id": "289",
        "c_code": "#include <stdio.h>   void roundOff(float *mat, int N, int M);  int main() {          int N = 3;     int M = 4;     float mat[] = {1.2, -2.5, 3.8,                    -4.1, 5.6, -6.9,                    7.4, -8.2, 9.0,                    -10.3, 11.7, -12.5};           roundOff(mat, N, M);           printf(\"Rounded Matrix:\\n\");     for (int i = 0; i < M; i++) {         for (int j = 0; j < N; j++) {             printf(\"%.0f \", mat[i * N + j]);         }         printf(\"\\n\");     }      return 0; }   void roundOff(float *mat, int N, int M) {     for (int i = 0; i < M; i++) {         for (int j = 0; j < N; j++) {             if (mat[i * N + j] >= 0)                 mat[i * N + j] = (int)(mat[i * N + j] + 0.5);             else                 mat[i * N + j] = (int)(mat[i * N + j] - 0.5);         }     } }   ",
        "cuda_code": "#include <stdio.h>   #include <cuda_runtime.h>   __global__ void roundOff(float* mat, int N) {     int i = threadIdx.x;     int j = blockIdx.x;      if (mat[i * N + j] >= 0)         mat[i * N + j] = (int)(mat[i * N + j] + 0.5);     else         mat[i * N + j] = (int)(mat[i * N + j] - 0.5); }  int main() {          const int N = 100;            float* h_mat = (float*)malloc(N * N * sizeof(float));           for (int i = 0; i < N * N; ++i) {         h_mat[i] = i;      }           float* d_mat;     cudaMalloc((void**)&d_mat, N * N * sizeof(float));           cudaMemcpy(d_mat, h_mat, N * N * sizeof(float), cudaMemcpyHostToDevice);           int threadsPerBlock = N;     int blocksPerGrid = 1;           roundOff<<<blocksPerGrid, threadsPerBlock>>>(d_mat, N);           cudaMemcpy(h_mat, d_mat, N * N * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result matrix:\\n\");     for (int i = 0; i < N; ++i) {         for (int j = 0; j < N; ++j) {             printf(\"%.1f\\t\", h_mat[i * N + j]);         }         printf(\"\\n\");     }           free(h_mat);     cudaFree(d_mat);      return 0; }   "
    },
    {
        "id": "290",
        "c_code": "#include <stdio.h>   void update_clusters_cpu(int n, int k, double *Cx, double *Cy, double *Cx_sum, double *Cy_sum, int *Csize);  int main() {          int n = 5;     int k = 3;     double Cx[k] = {1.0, 2.0, 3.0};     double Cy[k] = {4.0, 5.0, 6.0};     double Cx_sum[k] = {10.0, 20.0, 30.0};     double Cy_sum[k] = {40.0, 50.0, 60.0};     int Csize[k] = {2, 3, 0};            update_clusters_cpu(n, k, Cx, Cy, Cx_sum, Cy_sum, Csize);           printf(\"Updated Clusters:\\n\");     for (int index = 0; index < k; index++) {         printf(\"Cluster %d: Cx=%.2f, Cy=%.2f\\n\", index, Cx[index], Cy[index]);     }      return 0; }   void update_clusters_cpu(int n, int k, double *Cx, double *Cy, double *Cx_sum, double *Cy_sum, int *Csize) {     for (int index = 0; index < k; index++) {         if (Csize[index]) {             Cx[index] = Cx_sum[index] / Csize[index];             Cy[index] = Cy_sum[index] / Csize[index];         }     } }   ",
        "cuda_code": "#include <stdio.h>   #include <cuda_runtime.h>   __global__ void update_clusters(int n, int k, double* Cx, double* Cy, double* Cx_sum, double* Cy_sum, int* Csize) {     int index = blockIdx.x * blockDim.x + threadIdx.x;      if (index < k && Csize[index]) {         Cx[index] = Cx_sum[index] / Csize[index];         Cy[index] = Cy_sum[index] / Csize[index];     } }  int main() {          const int k = 10;      const int n = 1000;            double* h_Cx = (double*)malloc(k * sizeof(double));     double* h_Cy = (double*)malloc(k * sizeof(double));     double* h_Cx_sum = (double*)malloc(k * sizeof(double));     double* h_Cy_sum = (double*)malloc(k * sizeof(double));     int* h_Csize = (int*)malloc(k * sizeof(int));           for (int i = 0; i < k; ++i) {         h_Cx[i] = i;          h_Cy[i] = i;         h_Cx_sum[i] = i;         h_Cy_sum[i] = i;         h_Csize[i] = i + 1;     }           double* d_Cx, * d_Cy, * d_Cx_sum, * d_Cy_sum;     int* d_Csize;     cudaMalloc((void**)&d_Cx, k * sizeof(double));     cudaMalloc((void**)&d_Cy, k * sizeof(double));     cudaMalloc((void**)&d_Cx_sum, k * sizeof(double));     cudaMalloc((void**)&d_Cy_sum, k * sizeof(double));     cudaMalloc((void**)&d_Csize, k * sizeof(int));           cudaMemcpy(d_Cx, h_Cx, k * sizeof(double), cudaMemcpyHostToDevice);     cudaMemcpy(d_Cy, h_Cy, k * sizeof(double), cudaMemcpyHostToDevice);     cudaMemcpy(d_Cx_sum, h_Cx_sum, k * sizeof(double), cudaMemcpyHostToDevice);     cudaMemcpy(d_Cy_sum, h_Cy_sum, k * sizeof(double), cudaMemcpyHostToDevice);     cudaMemcpy(d_Csize, h_Csize, k * sizeof(int), cudaMemcpyHostToDevice);           int threadsPerBlock = 256;     int blocksPerGrid = (k + threadsPerBlock - 1) / threadsPerBlock;           update_clusters<<<blocksPerGrid, threadsPerBlock>>>(n, k, d_Cx, d_Cy, d_Cx_sum, d_Cy_sum, d_Csize);           cudaMemcpy(h_Cx, d_Cx, k * sizeof(double), cudaMemcpyDeviceToHost);     cudaMemcpy(h_Cy, d_Cy, k * sizeof(double), cudaMemcpyDeviceToHost);           printf(\"Updated cluster centers:\\n\");     for (int i = 0; i < k; ++i) {         printf(\"Cluster %d: Cx=%.2f, Cy=%.2f\\n\", i, h_Cx[i], h_Cy[i]);     }           free(h_Cx);     free(h_Cy);     free(h_Cx_sum);     free(h_Cy_sum);     free(h_Csize);     cudaFree(d_Cx);     cudaFree(d_Cy);     cudaFree(d_Cx_sum);     cudaFree(d_Cy_sum);     cudaFree(d_Csize);      return 0; }   "
    },
    {
        "id": "291",
        "c_code": "#include <stdio.h>   void Gather_cpu(const int *input, float *output, int input_size, const float *data, int count, int dim, int data_offset);  int main() {          int input_size = 3;     int input[] = {2, 0, 1};      float output[input_size * dim];     int count = 2;     int dim = 3;     int data_offset = 1;     float data[] = {1.1, 1.2, 1.3,                     2.1, 2.2, 2.3,                     3.1, 3.2, 3.3,                     4.1, 4.2, 4.3};           Gather_cpu(input, output, input_size, data, count, dim, data_offset);           printf(\"Gathered Data:\\n\");     for (int i = 0; i < input_size; i++) {         for (int j = 0; j < dim; j++) {             printf(\"%.2f \", output[i * dim + j]);         }         printf(\"\\n\");     }      return 0; }   void Gather_cpu(const int *input, float *output, int input_size, const float *data, int count, int dim, int data_offset) {     int index;     for (index = 0; index < input_size * dim; index++) {         const int input_id = input[index / dim];         const int pos = index % dim;         if (input_id < count + data_offset && input_id >= data_offset) {             output[index] = data[input_id * dim + pos];         }     } }   ",
        "cuda_code": "#include <stdio.h>   #include <cuda_runtime.h>   __global__ void GatherKernel(const int* input, float* output, int input_size, const float* data, int count, int dim, int data_offset) {     const int thread_index = blockIdx.x * blockDim.x + threadIdx.x;      if (thread_index < input_size * dim) {         const int input_id = input[thread_index / dim];         const int pos = thread_index % dim;          if (input_id < count + data_offset && input_id >= data_offset) {             output[thread_index] = data[input_id * dim + pos];         }     } }  int main() {          const int input_size = 1000;      const int count = 100;      const int dim = 3;      const int data_offset = 50;            int* h_input = (int*)malloc(input_size * sizeof(int));     float* h_output = (float*)malloc(input_size * dim * sizeof(float));     float* h_data = (float*)malloc((count + data_offset) * dim * sizeof(float));           for (int i = 0; i < input_size; ++i) {         h_input[i] = i % (count + data_offset);      }      for (int i = 0; i < (count + data_offset) * dim; ++i) {         h_data[i] = i;      }           int* d_input;     float* d_output, * d_data;     cudaMalloc((void**)&d_input, input_size * sizeof(int));     cudaMalloc((void**)&d_output, input_size * dim * sizeof(float));     cudaMalloc((void**)&d_data, (count + data_offset) * dim * sizeof(float));           cudaMemcpy(d_input, h_input, input_size * sizeof(int), cudaMemcpyHostToDevice);     cudaMemcpy(d_data, h_data, (count + data_offset) * dim * sizeof(float), cudaMemcpyHostToDevice);           int threadsPerBlock = 256;     int blocksPerGrid = (input_size * dim + threadsPerBlock - 1) / threadsPerBlock;           GatherKernel<<<blocksPerGrid, threadsPerBlock>>>(d_input, d_output, input_size, d_data, count, dim, data_offset);           cudaMemcpy(h_output, d_output, input_size * dim * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Gathered data:\\n\");     for (int i = 0; i < input_size; ++i) {         printf(\"Input ID %d: \", h_input[i]);         for (int j = 0; j < dim; ++j) {             printf(\"%.2f \", h_output[i * dim + j]);         }         printf(\"\\n\");     }           free(h_input);     free(h_output);     free(h_data);     cudaFree(d_input);     cudaFree(d_output);     cudaFree(d_data);      return 0; }   "
    },
    {
        "id": "292",
        "c_code": "#include <stdio.h>   void histogrammPrimitive(unsigned int *histogramVector, unsigned char *grayImage, int rows, int columns);  int main() {          int rows = 3;     int columns = 3;     unsigned char grayImage[] = {1, 2, 3,                                  1, 2, 3,                                  1, 2, 3};     unsigned int histogramVector[256] = {0};           histogramPrimitive(histogramVector, grayImage, rows, columns);           printf(\"Histogram Vector:\\n\");     for (int i = 0; i < 256; i++) {         printf(\"%u \", histogramVector[i]);     }     printf(\"\\n\");      return 0; }   void histogramPrimitive(unsigned int *histogramVector, unsigned char *grayImage, int rows, int columns) {     int column;     int row;     for (column = 0; column < columns; column++) {         for (row = 0; row < rows; row++) {             int offset = (column) + (columns * row);             unsigned char grayValue = grayImage[offset];             histogramVector[grayValue]++;         }     } }   ",
        "cuda_code": "#include <stdio.h>   #include <cuda_runtime.h>   __global__ void histogrammPrimitive(unsigned int* histogrammVector, unsigned char* grayImage, int rows, int columns) {     int column = blockIdx.x * blockDim.x + threadIdx.x;     int row = blockIdx.y * blockDim.y + threadIdx.y;     int offset = (column) + (columns * row);      if ((column < columns) && (row < rows)) {         unsigned char grayValue = grayImage[offset];         atomicAdd(&(histogrammVector[grayValue]), 1);     } }  int main() {          const int rows = 512;      const int columns = 512;      const int histogramSize = 256;            unsigned char* h_grayImage = (unsigned char*)malloc(rows * columns * sizeof(unsigned char));     unsigned int* h_histogramVector = (unsigned int*)malloc(histogramSize * sizeof(unsigned int));           for (int i = 0; i < rows * columns; ++i) {         h_grayImage[i] = i % 256;      }           unsigned char* d_grayImage;     unsigned int* d_histogramVector;      cudaMalloc((void**)&d_grayImage, rows * columns * sizeof(unsigned char));     cudaMalloc((void**)&d_histogramVector, histogramSize * sizeof(unsigned int));           cudaMemcpy(d_grayImage, h_grayImage, rows * columns * sizeof(unsigned char), cudaMemcpyHostToDevice);     cudaMemset(d_histogramVector, 0, histogramSize * sizeof(unsigned int));           dim3 threadsPerBlock(16, 16);      dim3 blocksPerGrid((columns + threadsPerBlock.x - 1) / threadsPerBlock.x, (rows + threadsPerBlock.y - 1) / threadsPerBlock.y);           histogrammPrimitive<<<blocksPerGrid, threadsPerBlock>>>(d_histogramVector, d_grayImage, rows, columns);           cudaMemcpy(h_histogramVector, d_histogramVector, histogramSize * sizeof(unsigned int), cudaMemcpyDeviceToHost);           printf(\"Histogram:\\n\");     for (int i = 0; i < histogramSize; ++i) {         printf(\"Value %d: %u\\n\", i, h_histogramVector[i]);     }           free(h_grayImage);     free(h_histogramVector);     cudaFree(d_grayImage);     cudaFree(d_histogramVector);      return 0; }   "
    },
    {
        "id": "293",
        "c_code": "#include <stdio.h>   void sumAndScale_cpu(float *noiseVariance, float *diffMag2, int n);  int main() {          int n = 3;     float diffMag2[] = {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0};     float noiseVariance[n];           sumAndScale_cpu(noiseVariance, diffMag2, n);           printf(\"Noise Variance:\\n\");     for (int i = 0; i < n; i++) {         printf(\"%.8f \", noiseVariance[i]);     }     printf(\"\\n\");      return 0; }   void sumAndScale_cpu(float *noiseVariance, float *diffMag2, int n) {     for (int i = 0; i < n; i++) {         int batchJump = i * 347;         float temp = 0;         for (int sumIndex = 0; sumIndex < 347; sumIndex++) {             temp += diffMag2[batchJump + sumIndex];         }         temp = 0.00161812 * temp;         noiseVariance[i] = temp;     } }   ",
        "cuda_code": "#include <stdio.h>   #include <cuda_runtime.h>   __global__ void sumAndScale(float* noiseVariance, float* diffMag2, int n) {     int i = blockIdx.x * blockDim.x + threadIdx.x;     if (i >= n)         return;      int batchJump = i * 347;     float temp = 0;      for (int sumIndex = 0; sumIndex < 347; sumIndex++)         temp += diffMag2[batchJump + sumIndex];      temp = 0.00161812 * temp;     noiseVariance[i] = temp; }  int main() {          const int n = 1000;            float* h_diffMag2 = (float*)malloc(n * 347 * sizeof(float));     float* h_noiseVariance = (float*)malloc(n * sizeof(float));           for (int i = 0; i < n * 347; ++i) {         h_diffMag2[i] = static_cast<float>(i % 100);      }           float* d_diffMag2;     float* d_noiseVariance;      cudaMalloc((void**)&d_diffMag2, n * 347 * sizeof(float));     cudaMalloc((void**)&d_noiseVariance, n * sizeof(float));           cudaMemcpy(d_diffMag2, h_diffMag2, n * 347 * sizeof(float), cudaMemcpyHostToDevice);     cudaMemset(d_noiseVariance, 0, n * sizeof(float));           dim3 threadsPerBlock(256);     dim3 blocksPerGrid((n + threadsPerBlock.x - 1) / threadsPerBlock.x);           sumAndScale<<<blocksPerGrid, threadsPerBlock>>>(d_noiseVariance, d_diffMag2, n);           cudaMemcpy(h_noiseVariance, d_noiseVariance, n * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Noise Variance:\\n\");     for (int i = 0; i < n; ++i) {         printf(\"Element %d: %f\\n\", i, h_noiseVariance[i]);     }           free(h_diffMag2);     free(h_noiseVariance);     cudaFree(d_diffMag2);     cudaFree(d_noiseVariance);      return 0; }   "
    },
    {
        "id": "294",
        "c_code": "#include <stdio.h>   void convertInstanceToLabel_Kernel_cpu(unsigned short *d_outputLabel, const unsigned char *d_inputInstance, const unsigned short *d_instanceToLabel, unsigned int width, unsigned int height);  int main() {          unsigned int width = 3;     unsigned int height = 3;     unsigned char d_inputInstance[] = {1, 2, 3,                                         1, 2, 3,                                         1, 2, 3};     unsigned short d_instanceToLabel[] = {10, 20, 30};     unsigned short d_outputLabel[width * height];           convertInstanceToLabel_Kernel_cpu(d_outputLabel, d_inputInstance, d_instanceToLabel, width, height);           printf(\"Converted Labels:\\n\");     for (int y = 0; y < height; y++) {         for (int x = 0; x < width; x++) {             printf(\"%hu \", d_outputLabel[y * width + x]);         }         printf(\"\\n\");     }      return 0; }   void convertInstanceToLabel_Kernel_cpu(unsigned short *d_outputLabel, const unsigned char *d_inputInstance, const unsigned short *d_instanceToLabel, unsigned int width, unsigned int height) {     for (int x = 0; x < width; x++) {         for (int y = 0; y < height; y++) {             d_outputLabel[y * width + x] = d_instanceToLabel[d_inputInstance[y * width + x]];         }     } }   ",
        "cuda_code": "#include <stdio.h>   #include <cuda_runtime.h>   __global__ void convertInstanceToLabel_Kernel(unsigned short* d_outputLabel, const unsigned char* d_inputInstance,                                               const unsigned short* d_instanceToLabel, unsigned int width,                                               unsigned int height) {     const unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;     const unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;      if (x < width && y < height) {         d_outputLabel[y * width + x] = d_instanceToLabel[d_inputInstance[y * width + x]];     } }  int main() {          const unsigned int width = 512;       const unsigned int height = 512;            unsigned char* h_inputInstance = (unsigned char*)malloc(width * height * sizeof(unsigned char));     unsigned short* h_instanceToLabel = (unsigned short*)malloc(256 * sizeof(unsigned short));      unsigned short* h_outputLabel = (unsigned short*)malloc(width * height * sizeof(unsigned short));           for (unsigned int i = 0; i < width * height; ++i) {         h_inputInstance[i] = static_cast<unsigned char>(i % 256);      }      for (int i = 0; i < 256; ++i) {         h_instanceToLabel[i] = static_cast<unsigned short>(i);      }           unsigned char* d_inputInstance;     unsigned short* d_instanceToLabel;     unsigned short* d_outputLabel;      cudaMalloc((void**)&d_inputInstance, width * height * sizeof(unsigned char));     cudaMalloc((void**)&d_instanceToLabel, 256 * sizeof(unsigned short));     cudaMalloc((void**)&d_outputLabel, width * height * sizeof(unsigned short));           cudaMemcpy(d_inputInstance, h_inputInstance, width * height * sizeof(unsigned char), cudaMemcpyHostToDevice);     cudaMemcpy(d_instanceToLabel, h_instanceToLabel, 256 * sizeof(unsigned short), cudaMemcpyHostToDevice);     cudaMemset(d_outputLabel, 0, width * height * sizeof(unsigned short));           dim3 threadsPerBlock(16, 16);     dim3 blocksPerGrid((width + threadsPerBlock.x - 1) / threadsPerBlock.x,                        (height + threadsPerBlock.y - 1) / threadsPerBlock.y);           convertInstanceToLabel_Kernel<<<blocksPerGrid, threadsPerBlock>>>(d_outputLabel, d_inputInstance, d_instanceToLabel,                                                                       width, height);           cudaMemcpy(h_outputLabel, d_outputLabel, width * height * sizeof(unsigned short), cudaMemcpyDeviceToHost);           printf(\"Output Label:\\n\");     for (unsigned int i = 0; i < width * height; ++i) {         printf(\"Element %u: %u\\n\", i, h_outputLabel[i]);     }           free(h_inputInstance);     free(h_instanceToLabel);     free(h_outputLabel);     cudaFree(d_inputInstance);     cudaFree(d_instanceToLabel);     cudaFree(d_outputLabel);      return 0; }   "
    },
    {
        "id": "295",
        "c_code": "#include <stdio.h>   void downsampleCpu(float *I, float *Q, unsigned int numDownsampledSamples, float *downsampled_I, float *downsampled_Q, unsigned int factor);  int main() {          unsigned int numDownsampledSamples = 3;     unsigned int factor = 2;     float I[] = {1.0, 2.0, 3.0, 4.0, 5.0, 6.0};     float Q[] = {0.1, 0.2, 0.3, 0.4, 0.5, 0.6};     float downsampled_I[numDownsampledSamples];     float downsampled_Q[numDownsampledSamples];           downsampleCpu(I, Q, numDownsampledSamples, downsampled_I, downsampled_Q, factor);           printf(\"Downsampled Data:\\n\");     for (int i = 0; i < numDownsampledSamples; i++) {         printf(\"I=%.2f, Q=%.2f\\n\", downsampled_I[i], downsampled_Q[i]);     }      return 0; }   void downsampleCpu(float *I, float *Q, unsigned int numDownsampledSamples, float *downsampled_I, float *downsampled_Q, unsigned int factor) {     for (int sampleIndex = 0; sampleIndex < numDownsampledSamples; sampleIndex++) {         unsigned int absoluteIndex = sampleIndex * factor;         downsampled_I[sampleIndex] = I[absoluteIndex];         downsampled_Q[sampleIndex] = Q[absoluteIndex];     } }   ",
        "cuda_code": "#include <stdio.h>   #include <cuda_runtime.h>   __global__ void downsampleCuda(float* I, float* Q, unsigned int numDownsampledSamples,                                 float* downsampled_I, float* downsampled_Q, unsigned int factor) {     int sampleIndex = (blockIdx.x * blockDim.x) + threadIdx.x;      if (sampleIndex < numDownsampledSamples) {         unsigned int absoluteIndex = sampleIndex * factor;         downsampled_I[sampleIndex] = I[absoluteIndex];         downsampled_Q[sampleIndex] = Q[absoluteIndex];     } }  int main() {          const unsigned int numDownsampledSamples = 1024;      const unsigned int factor = 2;                              float* h_I = (float*)malloc(numDownsampledSamples * factor * sizeof(float));     float* h_Q = (float*)malloc(numDownsampledSamples * factor * sizeof(float));     float* h_downsampled_I = (float*)malloc(numDownsampledSamples * sizeof(float));     float* h_downsampled_Q = (float*)malloc(numDownsampledSamples * sizeof(float));           for (unsigned int i = 0; i < numDownsampledSamples * factor; ++i) {         h_I[i] = static_cast<float>(i);          h_Q[i] = static_cast<float>(i);      }           float *d_I, *d_Q, *d_downsampled_I, *d_downsampled_Q;      cudaMalloc((void**)&d_I, numDownsampledSamples * factor * sizeof(float));     cudaMalloc((void**)&d_Q, numDownsampledSamples * factor * sizeof(float));     cudaMalloc((void**)&d_downsampled_I, numDownsampledSamples * sizeof(float));     cudaMalloc((void**)&d_downsampled_Q, numDownsampledSamples * sizeof(float));           cudaMemcpy(d_I, h_I, numDownsampledSamples * factor * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_Q, h_Q, numDownsampledSamples * factor * sizeof(float), cudaMemcpyHostToDevice);           dim3 threadsPerBlock(256);     dim3 blocksPerGrid((numDownsampledSamples + threadsPerBlock.x - 1) / threadsPerBlock.x);           downsampleCuda<<<blocksPerGrid, threadsPerBlock>>>(d_I, d_Q, numDownsampledSamples,                                                         d_downsampled_I, d_downsampled_Q, factor);           cudaMemcpy(h_downsampled_I, d_downsampled_I, numDownsampledSamples * sizeof(float),                cudaMemcpyDeviceToHost);     cudaMemcpy(h_downsampled_Q, d_downsampled_Q, numDownsampledSamples * sizeof(float),                cudaMemcpyDeviceToHost);           printf(\"Downsampled I and Q:\\n\");     for (unsigned int i = 0; i < numDownsampledSamples; ++i) {         printf(\"Element %u: I=%f, Q=%f\\n\", i, h_downsampled_I[i], h_downsampled_Q[i]);     }           free(h_I);     free(h_Q);     free(h_downsampled_I);     free(h_downsampled_Q);     cudaFree(d_I);     cudaFree(d_Q);     cudaFree(d_downsampled_I);     cudaFree(d_downsampled_Q);      return 0; }   "
    },
    {
        "id": "296",
        "c_code": "#include <stdio.h> #include <math.h>   void logistic_x_ent_cpu(int n, float *pred, float *truth, float *delta, float *error);  int main() {          int n = 3;     float pred[] = {0.2, 0.8, 0.6};     float truth[] = {0.0, 1.0, 1.0};     float delta[n];     float error[n];           logistic_x_ent_cpu(n, pred, truth, delta, error);           printf(\"Error:\\n\");     for (int i = 0; i < n; i++) {         printf(\"%.4f \", error[i]);     }     printf(\"\\n\");      printf(\"Delta:\\n\");     for (int i = 0; i < n; i++) {         printf(\"%.4f \", delta[i]);     }     printf(\"\\n\");      return 0; }   void logistic_x_ent_cpu(int n, float *pred, float *truth, float *delta, float *error) {     for (int i = 0; i < n; ++i) {         float t = truth[i];         float p = pred[i];         error[i] = -t * log(p) - (1 - t) * log(1 - p);         delta[i] = t - p;     } }   ",
        "cuda_code": "#include <stdio.h> #include <cmath>   #include <cuda_runtime.h>   __global__ void logistic_x_ent_kernel(int n, float* pred, float* truth, float* delta, float* error) {     int i = (blockIdx.x + blockIdx.y * gridDim.x) * blockDim.x + threadIdx.x;      if (i < n) {         float t = truth[i];         float p = pred[i];         error[i] = -t * log(p + 0.0000001f) - (1 - t) * log(1 - p + 0.0000001f);         delta[i] = t - p;     } }  int main() {          const int n = 1024;            float* h_pred = (float*)malloc(n * sizeof(float));     float* h_truth = (float*)malloc(n * sizeof(float));     float* h_delta = (float*)malloc(n * sizeof(float));     float* h_error = (float*)malloc(n * sizeof(float));           for (int i = 0; i < n; ++i) {         h_pred[i] = 0.7f;           h_truth[i] = 0.5f;      }           float *d_pred, *d_truth, *d_delta, *d_error;      cudaMalloc((void**)&d_pred, n * sizeof(float));     cudaMalloc((void**)&d_truth, n * sizeof(float));     cudaMalloc((void**)&d_delta, n * sizeof(float));     cudaMalloc((void**)&d_error, n * sizeof(float));           cudaMemcpy(d_pred, h_pred, n * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_truth, h_truth, n * sizeof(float), cudaMemcpyHostToDevice);           dim3 threadsPerBlock(256);     dim3 blocksPerGrid((n + threadsPerBlock.x - 1) / threadsPerBlock.x);           logistic_x_ent_kernel<<<blocksPerGrid, threadsPerBlock>>>(n, d_pred, d_truth, d_delta, d_error);           cudaMemcpy(h_delta, d_delta, n * sizeof(float), cudaMemcpyDeviceToHost);     cudaMemcpy(h_error, d_error, n * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Delta and Error:\\n\");     for (int i = 0; i < n; ++i) {         printf(\"Element %d: Delta=%f, Error=%f\\n\", i, h_delta[i], h_error[i]);     }           free(h_pred);     free(h_truth);     free(h_delta);     free(h_error);     cudaFree(d_pred);     cudaFree(d_truth);     cudaFree(d_delta);     cudaFree(d_error);      return 0; }   "
    },
    {
        "id": "297",
        "c_code": "#include <stdio.h>   void Argmax(int *argMax, float **array, const int r, const int c);  int main() {          int r = 3;     int c = 4;     float data[3][4] = {{1.2, 2.3, 0.5, 1.8},                         {0.9, 2.0, 1.5, 3.2},                         {2.1, 1.4, 3.0, 0.8}};     float *array[r];     for (int i = 0; i < r; i++) {         array[i] = data[i];     }     int argMax[r];           Argmax(argMax, array, r, c);           printf(\"Argmax Indices:\\n\");     for (int i = 0; i < r; i++) {         printf(\"%d \", argMax[i]);     }     printf(\"\\n\");      return 0; }   void Argmax(int *argMax, float **array, const int r, const int c) {     int idx;     float temp;     for (int i = 0; i < r; i++) {         idx = 0;         temp = 0.0;         for (int j = 0; j < c; j++) {             if (array[i][j] > temp) {                 temp = array[i][j];                 idx = j;             }         }         argMax[i] = idx;     } }   ",
        "cuda_code": "#include <stdio.h>   #include <cuda_runtime.h>   __global__ void Kernel_Argmax(int* dev_argMax, float* dev_array, const int r, const int c) {     unsigned int i = blockDim.x * blockIdx.x + threadIdx.x;     if (i >= r) return;      int idx;     float temp = 0.0;      for (int j = 0; j < c; j++) {         if (dev_array[i * c + j] > temp) {             temp = dev_array[i * c + j];             idx = j;         }     }      dev_argMax[i] = idx; }  int main() {          const int r = 4;      const int c = 5;            float* h_array = (float*)malloc(r * c * sizeof(float));     int* h_argMax = (int*)malloc(r * sizeof(int));           for (int i = 0; i < r * c; ++i) {         h_array[i] = static_cast<float>(i % 10);      }           float* d_array;     int* d_argMax;      cudaMalloc((void**)&d_array, r * c * sizeof(float));     cudaMalloc((void**)&d_argMax, r * sizeof(int));           cudaMemcpy(d_array, h_array, r * c * sizeof(float), cudaMemcpyHostToDevice);           dim3 threadsPerBlock(256);     dim3 blocksPerGrid((r + threadsPerBlock.x - 1) / threadsPerBlock.x);           Kernel_Argmax<<<blocksPerGrid, threadsPerBlock>>>(d_argMax, d_array, r, c);           cudaMemcpy(h_argMax, d_argMax, r * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Argmax Result:\\n\");     for (int i = 0; i < r; ++i) {         printf(\"Row %d: Argmax=%d\\n\", i, h_argMax[i]);     }           free(h_array);     free(h_argMax);     cudaFree(d_array);     cudaFree(d_argMax);      return 0; }   "
    },
    {
        "id": "298",
        "c_code": "#include <stdio.h> #include <math.h>   float CEE(float *x, int *t, int r, int c);  int main() {          int r = 2;     int c = 3;     float x[] = {0.2, 0.8, 0.6, 0.4, 0.5, 0.9};     int t[] = {0, 1, 1, 0, 1, 0};           float result = CEE(x, t, r, c);           printf(\"Cross-Entropy Error: %.4f\\n\", result);      return 0; }   float CEE(float *x, int *t, int r, int c) {     float temp = 0;     for (int i = 0; i < r; i++) {         for (int j = 0; j < c; j++) {             if (t[i * c + j] == 1) {                 temp += log(x[i * c + j] + 1e-7);                 continue;             }         }     }     temp /= -r;     return temp; }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h> #include <cuda_runtime.h> #include <math.h>   __global__ void kernel_CEE(float *x, int *t, float *loss, int r, int c) {     int i = blockDim.x * blockIdx.x + threadIdx.x;     int N = r;     float temp;      while (i < N) {         for (int j = 0; j < c; j++) {             if (t[i * c + j] == 1) {                 temp = logf(x[i * c + j] + 1e-7);                 atomicAdd(loss, temp);                 continue;             }         }          i += gridDim.x * blockDim.x;     } }  int main() {          const int r = 100;       const int c = 10;              float *h_x = (float *)malloc(r * c * sizeof(float));     int *h_t = (int *)malloc(r * c * sizeof(int));     float *h_loss = (float *)malloc(sizeof(float));           for (int i = 0; i < r * c; i++) {         h_x[i] = (float)rand() / RAND_MAX;         h_t[i] = rand() % 2;       }           float *d_x;     int *d_t;     float *d_loss;     cudaMalloc((void **)&d_x, r * c * sizeof(float));     cudaMalloc((void **)&d_t, r * c * sizeof(int));     cudaMalloc((void **)&d_loss, sizeof(float));           cudaMemcpy(d_x, h_x, r * c * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_t, h_t, r * c * sizeof(int), cudaMemcpyHostToDevice);     cudaMemset(d_loss, 0, sizeof(float));             dim3 blockSize(256);       dim3 gridSize((r + blockSize.x - 1) / blockSize.x);           kernel_CEE<<<gridSize, blockSize>>>(d_x, d_t, d_loss, r, c);           cudaMemcpy(h_loss, d_loss, sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Cross-Entropy Loss: %f\\n\", *h_loss);           free(h_x);     free(h_t);     free(h_loss);     cudaFree(d_x);     cudaFree(d_t);     cudaFree(d_loss);      return 0; } "
    },
    {
        "id": "299",
        "c_code": "#include <stdio.h>   void check_results_kernel(unsigned int *g_results0, unsigned int *g_results1, int n);  int main() {          int n = 5;     unsigned int g_results0[] = {1, 2, 3, 4, 5};     unsigned int g_results1[] = {1, 2, 8, 4, 5};             check_results_kernel(g_results0, g_results1, n);      return 0; }   void check_results_kernel(unsigned int *g_results0, unsigned int *g_results1, int n) {     unsigned int gidx;     unsigned int result0;     unsigned int result1;     for (gidx = 0; gidx < n; gidx++) {         result0 = g_results0[gidx];         result1 = g_results1[gidx];         if (result0 != result1) {             printf(\"%u != %u for %u\\n\", result0, result1, gidx);         }     } }   ",
        "cuda_code": "#include <stdio.h>  __global__ void check_results_kernel(uint* g_results0, uint* g_results1, int n) {     uint idx = threadIdx.x;     uint gidx = blockDim.x * blockIdx.x + idx;     uint result0, result1;      if (gidx < n) {         result0 = g_results0[gidx];         result1 = g_results1[gidx];          if (result0 != result1) {             printf(\"%d != %d for thread %d\\n\", result0, result1, gidx);         }     } }  int main() {          const int n = 1000;             uint* h_results0 = (uint*)malloc(n * sizeof(uint));     uint* h_results1 = (uint*)malloc(n * sizeof(uint));           for (int i = 0; i < n; ++i) {         h_results0[i] = i;         h_results1[i] = i * 2;       }           uint* d_results0;     uint* d_results1;      cudaMalloc((void**)&d_results0, n * sizeof(uint));     cudaMalloc((void**)&d_results1, n * sizeof(uint));           cudaMemcpy(d_results0, h_results0, n * sizeof(uint), cudaMemcpyHostToDevice);     cudaMemcpy(d_results1, h_results1, n * sizeof(uint), cudaMemcpyHostToDevice);           dim3 threadsPerBlock(256);     dim3 blocksPerGrid((n + threadsPerBlock.x - 1) / threadsPerBlock.x   "
    },
    {
        "id": "300",
        "c_code": "#include <stdio.h>   void matrixMulOnHost(float *M, float *N, float *P, int width);  int main() {          int width = 3;     float M[] = {1.0, 2.0, 3.0,                  4.0, 5.0, 6.0,                  7.0, 8.0, 9.0};     float N[] = {9.0, 8.0, 7.0,                  6.0, 5.0, 4.0,                  3.0, 2.0, 1.0};     float P[width * width];           matrixMulOnHost(M, N, P, width);           printf(\"Result Matrix:\\n\");     for (int i = 0; i < width; i++) {         for (int j = 0; j < width; j++) {             printf(\"%.2f \", P[i * width + j]);         }         printf(\"\\n\");     }      return 0; }   void matrixMulOnHost(float *M, float *N, float *P, int width) {     for (int i = 0; i < width; ++i) {         for (int j = 0; j < width; ++j) {             double sum = 0;             for (int k = 0; k < width; ++k) {                 double a = M[i * width + k];                 double b = N[k * width + j];                 sum += a * b;             }             P[i * width + j] = sum;         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h> #include <cuda_runtime.h>   __global__ void MatrixMulKernel(float *Md, float *Nd, float *Pd, int width) {     int tx = threadIdx.x;     int ty = threadIdx.y;     float pvalue = 0;      for (int k = 0; k < width; ++k) {         float Mdelement = Md[ty * width + k];         float Ndelement = Nd[ty * width + k];         pvalue += Mdelement * Ndelement;     }      Pd[ty * width + tx] = pvalue; }  int main() {          const int width = 4;             float *h_Md = (float *)malloc(width * width * sizeof(float));     float *h_Nd = (float *)malloc(width * width * sizeof(float));     float *h_Pd = (float *)malloc(width * width * sizeof(float));           for (int i = 0; i < width * width; i++) {         h_Md[i] = (float)rand() / RAND_MAX;         h_Nd[i] = (float)rand() / RAND_MAX;     }           float *d_Md, *d_Nd, *d_Pd;     cudaMalloc((void **)&d_Md, width * width * sizeof(float));     cudaMalloc((void **)&d_Nd, width * width * sizeof(float));     cudaMalloc((void **)&d_Pd, width * width * sizeof(float));           cudaMemcpy(d_Md, h_Md, width * width * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_Nd, h_Nd, width * width * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(width, width);     dim3 gridSize(1, 1);           MatrixMulKernel<<<gridSize, blockSize>>>(d_Md, d_Nd, d_Pd, width);           cudaMemcpy(h_Pd, d_Pd, width * width * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result Matrix:\\n\");     for (int i = 0; i < width; i++) {         for (int j = 0; j < width; j++) {             printf(\"%f \", h_Pd[i * width + j]);         }         printf(\"\\n\");     }           free(h_Md);     free(h_Nd);     free(h_Pd);     cudaFree(d_Md);     cudaFree(d_Nd);     cudaFree(d_Pd);      return 0; }   "
    },
    {
        "id": "301",
        "c_code": "#include <stdio.h> #include <math.h>   void normalize_cpu(float *x, float *mean, float *variance, int batch, int filters, int spatial);  int main() {          int batch = 2;     int filters = 3;     int spatial = 4;     float x[] = {1.0, 2.0, 3.0, 4.0,                  5.0, 6.0, 7.0, 8.0,                  9.0, 10.0, 11.0, 12.0,                  13.0, 14.0, 15.0, 16.0,                  17.0, 18.0, 19.0, 20.0,                  21.0, 22.0, 23.0, 24.0};     float mean[] = {2.0, 4.0, 6.0};     float variance[] = {1.0, 2.0, 3.0};           normalize_cpu(x, mean, variance, batch, filters, spatial);           printf(\"Normalized Data:\\n\");     for (int b = 0; b < batch; ++b) {         for (int f = 0; f < filters; ++f) {             for (int i = 0; i < spatial; ++i) {                 int index = b * filters * spatial + f * spatial + i;                 printf(\"%.4f \", x[index]);             }             printf(\"\\n\");         }     }      return 0; }   void normalize_cpu(float *x, float *mean, float *variance, int batch, int filters, int spatial) {     for (int b = 0; b < batch; ++b) {         for (int f = 0; f < filters; ++f) {             for (int i = 0; i < spatial; ++i) {                 int index = b * filters * spatial + f * spatial + i;                 x[index] = (x[index] - mean[f]) / (sqrt(variance[f]) + 0.000001f);             }         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h> #include <cuda_runtime.h>   __global__ void normalize_kernel(int N, float *x, float *mean, float *variance, int batch, int filters, int spatial) {     int index = blockIdx.x * blockDim.x + threadIdx.x;     if (index < N) {         int f = (index / spatial) % filters;         x[index] = (x[index] - mean[f]) / sqrtf(variance[f] + 0.00001f);     } }  int main() {          const int N = 1024;       const int batch = 2;      const int filters = 3;      const int spatial = 4;            float *h_x = (float *)malloc(N * sizeof(float));     float *h_mean = (float *)malloc(filters * sizeof(float));     float *h_variance = (float *)malloc(filters * sizeof(float));           for (int i = 0; i < N; i++) {         h_x[i] = (float)rand() / RAND_MAX;     }      for (int i = 0; i < filters; i++) {         h_mean[i] = (float)rand() / RAND_MAX;         h_variance[i] = (float)rand() / RAND_MAX;     }           float *d_x, *d_mean, *d_variance;     cudaMalloc((void **)&d_x, N * sizeof(float));     cudaMalloc((void **)&d_mean, filters * sizeof(float));     cudaMalloc((void **)&d_variance, filters * sizeof(float));           cudaMemcpy(d_x, h_x, N * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_mean, h_mean, filters * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_variance, h_variance, filters * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(256);     dim3 gridSize((N + blockSize.x - 1) / blockSize.x);           normalize_kernel<<<gridSize, blockSize>>>(N, d_x, d_mean, d_variance, batch, filters, spatial);           cudaMemcpy(h_x, d_x, N * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Normalized data:\\n\");     for (int i = 0; i < N; i++) {         printf(\"%f \", h_x[i]);     }     printf(\"\\n\");           free(h_x);     free(h_mean);     free(h_variance);     cudaFree(d_x);     cudaFree(d_mean);     cudaFree(d_variance);      return 0; } "
    },
    {
        "id": "302",
        "c_code": "#include <stdio.h>   void kernel(int *a, int *b, int *c);  int main() {          int size = 1024 * 1024;     int a[size], b[size], c[size];           for (int i = 0; i < size; i++) {         a[i] = i;         b[i] = size - i;     }           kernel(a, b, c);           printf(\"Result Array:\\n\");     for (int i = 0; i < size; i++) {         printf(\"%d \", c[i]);     }      return 0; }   void kernel(int *a, int *b, int *c) {     for (int idx = 0; idx < 1024 * 1024; idx++) {         int idx1 = (idx + 1) % 256;         int idx2 = (idx + 2) % 256;         float as = (a[idx] + a[idx1] + a[idx2]) / 3.0f;         float bs = (b[idx] + b[idx1] + b[idx2]) / 3.0f;         c[idx] = (as + bs) / 2;     } }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h> #include <cuda_runtime.h>   __global__ void kernel(int *a, int *b, int *c) {     int idx = threadIdx.x + blockIdx.x * blockDim.x;      if (idx < (1024 * 1024)) {         int idx1 = (idx + 1) % 256;         int idx2 = (idx + 2) % 256;          float as = (a[idx] + a[idx1] + a[idx2]) / 3.0f;         float bs = (b[idx] + b[idx1] + b[idx2]) / 3.0f;          c[idx] = (as + bs) / 2;     } }  int main() {          const int size = 1024 * 1024;           int *h_a = (int *)malloc(size * sizeof(int));     int *h_b = (int *)malloc(size * sizeof(int));     int *h_c = (int *)malloc(size * sizeof(int));           for (int i = 0; i < size; i++) {         h_a[i] = rand() % 256;         h_b[i] = rand() % 256;     }           int *d_a, *d_b, *d_c;     cudaMalloc((void **)&d_a, size * sizeof(int));     cudaMalloc((void **)&d_b, size * sizeof(int));     cudaMalloc((void **)&d_c, size * sizeof(int));           cudaMemcpy(d_a, h_a, size * sizeof(int), cudaMemcpyHostToDevice);     cudaMemcpy(d_b, h_b, size * sizeof(int), cudaMemcpyHostToDevice);           dim3 blockSize(256);       dim3 gridSize((size + blockSize.x - 1) / blockSize.x);           kernel<<<gridSize, blockSize>>>(d_a, d_b, d_c);           cudaMemcpy(h_c, d_c, size * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Results printed here:\\n\");           free(h_a);     free(h_b);     free(h_c);     cudaFree(d_a);     cudaFree(d_b);     cudaFree(d_c);      return 0; }   "
    },
    {
        "id": "303",
        "c_code": "#include <stdio.h>   void kernel(int *a, int *b, int *c, int size);  int main() {          int size = 1024 * 1024;     int a[size], b[size], c[size];           for (int i = 0; i < size; i++) {         a[i] = i;         b[i] = size - i;     }           kernel(a, b, c, size);           printf(\"Result Array:\\n\");     for (int i = 0; i < size; i++) {         printf(\"%d \", c[i]);     }      return 0; }   void kernel(int *a, int *b, int *c, int size) {     for (int idx = 0; idx < size; idx++) {         int idx1 = (idx + 1) % size;         int idx2 = (idx + 2) % size;         float as = (a[idx] + a[idx1] + a[idx2]) / 3.0f;         float bs = (b[idx] + b[idx1] + b[idx2]) / 3.0f;         c[idx] = (as + bs) / 2;     } }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h> #include <cuda_runtime.h>   __global__ void kernel(int *a, int *b, int *c) {     int idx = threadIdx.x + blockIdx.x * blockDim.x;      if (idx < 1024 * 1024) {         int idx1 = (idx + 1) % 256;         int idx2 = (idx + 2) % 256;         float as = (a[idx] + a[idx1] + a[idx2]) / 3.0f;         float bs = (b[idx] + b[idx1] + b[idx2]) / 3.0f;         c[idx] = (as + bs) / 2;     } }  int main() {          const int size = 1024 * 1024;           int *h_a = (int *)malloc(size * sizeof(int));     int *h_b = (int *)malloc(size * sizeof(int));     int *h_c = (int *)malloc(size * sizeof(int));           for (int i = 0; i < size; i++) {         h_a[i] = rand() % 256;         h_b[i] = rand() % 256;     }           int *d_a, *d_b, *d_c;     cudaMalloc((void **)&d_a, size * sizeof(int));     cudaMalloc((void **)&d_b, size * sizeof(int));     cudaMalloc((void **)&d_c, size * sizeof(int));           cudaMemcpy(d_a, h_a, size * sizeof(int), cudaMemcpyHostToDevice);     cudaMemcpy(d_b, h_b, size * sizeof(int), cudaMemcpyHostToDevice);           dim3 blockSize(256);       dim3 gridSize((size + blockSize.x - 1) / blockSize.x);           kernel<<<gridSize, blockSize>>>(d_a, d_b, d_c);           cudaMemcpy(h_c, d_c, size * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Results printed here:\\n\");           free(h_a);     free(h_b);     free(h_c);     cudaFree(d_a);     cudaFree(d_b);     cudaFree(d_c);      return 0; }   "
    },
    {
        "id": "304",
        "c_code": "#include <stdio.h>   void inter_cpu(int NX, float *X, int NY, float *Y, int B, float *OUT);  int main() {          int NX = 3;     int NY = 2;     int B = 4;     float X[] = {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0};     float Y[] = {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8};     float OUT[NX + NY];           inter_cpu(NX, X, NY, Y, B, OUT);           printf(\"Interleaved Array:\\n\");     for (int i = 0; i < NX + NY; i++) {         printf(\"%.2f \", OUT[i]);     }      return 0; }   void inter_cpu(int NX, float *X, int NY, float *Y, int B, float *OUT) {     int i, j;     int index = 0;     for (j = 0; j < B; ++j) {         for (i = 0; i < NX; ++i) {             OUT[index++] = X[j * NX + i];         }         for (i = 0; i < NY; ++i) {             OUT[index++] = Y[j * NY + i];         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h> #include <cuda_runtime.h>   __global__ void inter_kernel(int NX, float *X, int NY, float *Y, int B, float *OUT) {     int i = (blockIdx.x + blockIdx.y * gridDim.x) * blockDim.x + threadIdx.x;      if (i < (NX + NY) * B) {         int b = i / (NX + NY);         int j = i % (NX + NY);          if (j < NX) {             OUT[i] = X[b * NX + j];         } else {             OUT[i] = Y[b * NY + j - NX];         }     } }  int main() {          const int NX = 128;     const int NY = 64;     const int B = 256;           float *h_X = (float *)malloc(NX * B * sizeof(float));     float *h_Y = (float *)malloc(NY * B * sizeof(float));     float *h_OUT = (float *)malloc((NX + NY) * B * sizeof(float));           for (int i = 0; i < NX * B; ++i) {         h_X[i] = static_cast<float>(i);     }      for (int i = 0; i < NY * B; ++i) {         h_Y[i] = static_cast<float>(i + NX * B);     }           float *d_X, *d_Y, *d_OUT;     cudaMalloc((void **)&d_X, NX * B * sizeof(float));     cudaMalloc((void **)&d_Y, NY * B * sizeof(float));     cudaMalloc((void **)&d_OUT, (NX + NY) * B * sizeof(float));           cudaMemcpy(d_X, h_X, NX * B * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_Y, h_Y, NY * B * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(256);       dim3 gridSize((B * (NX + NY) + blockSize.x - 1) / blockSize.x);           inter_kernel<<<gridSize, blockSize>>>(NX, d_X, NY, d_Y, B, d_OUT);           cudaMemcpy(h_OUT, d_OUT, (NX + NY) * B * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Results printed here:\\n\");           free(h_X);     free(h_Y);     free(h_OUT);     cudaFree(d_X);     cudaFree(d_Y);     cudaFree(d_OUT);      return 0; }   "
    },
    {
        "id": "305",
        "c_code": "#include <stdio.h>   void manage_adj_matrix(float **graph, int n);  int main() {          int n = 3;     float **graph = (float **)malloc(n * sizeof(float *));     for (int i = 0; i < n; ++i) {         graph[i] = (float *)malloc(n * sizeof(float));         for (int j = 0; j < n; ++j) {             graph[i][j] = i + j + 1.0;          }     }           manage_adj_matrix(graph, n);           printf(\"Modified Adjacency Matrix:\\n\");     for (int i = 0; i < n; ++i) {         for (int j = 0; j < n; ++j) {             printf(\"%.2f \", graph[i][j]);         }         printf(\"\\n\");     }           for (int i = 0; i < n; ++i) {         free(graph[i]);     }     free(graph);      return 0; }   void manage_adj_matrix(float **graph, int n) {     for (int j = 0; j < n; ++j) {         float sum = 0.0;         for (int i = 0; i < n; ++i) {             sum += graph[i][j];         }         for (int i = 0; i < n; ++i) {             if (sum != 0.0) {                 graph[i][j] /= sum;             } else {                 graph[i][j] = 1.0 / (float)n;             }         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h> #include <cuda_runtime.h>  __global__ void manage_adj_matrix(float *gpu_graph, int n) {     int id = blockIdx.x * blockDim.x + threadIdx.x;     if (id < n) {         float sum = 0.0;          for (int i = 0; i < n; ++i) {             sum += gpu_graph[i * n + id];         }          for (int i = 0; i < n; ++i) {             if (sum != 0.0) {                 gpu_graph[i * n + id] /= sum;             } else {                 gpu_graph[i * n + id] = (1.0 / (float)n);             }         }     } }  int main() {          const int n = 256;           float *h_gpu_graph = (float *)malloc(n * n * sizeof(float));           for (int i = 0; i < n * n; ++i) {         h_gpu_graph[i] = static_cast<float>(i % 10);     }           float *d_gpu_graph;     cudaMalloc((void **)&d_gpu_graph, n * n * sizeof(float));           cudaMemcpy(d_gpu_graph, h_gpu_graph, n * n * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(256);       dim3 gridSize((n + blockSize.x - 1) / blockSize.x);           manage_adj_matrix<<<gridSize, blockSize>>>(d_gpu_graph, n);           cudaMemcpy(h_gpu_graph, d_gpu_graph, n * n * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Results printed here:\\n\");           free(h_gpu_graph);     cudaFree(d_gpu_graph);      return 0; }   "
    },
    {
        "id": "306",
        "c_code": "#include <stdio.h>   void fa_cpu(const float *q, const float *h, int nq, float *a, float *fa);  int main() {          int nq = 4;     float q[] = {1.0, 2.0, 3.0, 4.0};     float h[] = {0.5, 1.0, 1.5, 2.0};     float a[nq - 1], fa[nq - 1];           fa_cpu(q, h, nq, a, fa);           printf(\"Acceleration (a) Array:\\n\");     for (int i = 0; i < nq - 1; ++i) {         printf(\"%.2f \", a[i]);     }      printf(\"\\n\\n\");      printf(\"Result (fa) Array:\\n\");     for (int i = 0; i < nq - 1; ++i) {         printf(\"%.2f \", fa[i]);     }      return 0; }   void fa_cpu(const float *q, const float *h, int nq, float *a, float *fa) {     for (int iq = 0; iq < (nq - 1); iq++) {         float dq = q[iq + 1] - q[iq];         a[iq] = (h[iq + 1] * q[iq + 1] - h[iq] * q[iq]) / dq;         fa[iq] = q[iq] * (a[iq] - h[iq]) + 1.0;     } }   ",
        "cuda_code": " #include <stdio.h> #include <stdlib.h> #include <cuda_runtime.h>  __global__ void faKernel(const float *__restrict__ q, const float *__restrict__ h, int nq, float *__restrict__ a, float *__restrict__ fa) {     int iq = blockIdx.x * blockDim.x + threadIdx.x;     if (iq < (nq - 1)) {         float dq = q[1] - q[0];         a[iq] = (h[iq + 1] * q[iq + 1] - h[iq] * q[iq]) / dq;         fa[iq] = q[iq] * (a[iq] - h[iq]) + 1.0f;     } }  int main() {          const int nq = 256;           float *h_q = (float *)malloc(nq * sizeof(float));     float *h_h = (float *)malloc(nq * sizeof(float));     float *h_a = (float *)malloc((nq - 1) * sizeof(float));     float *h_fa = (float *)malloc((nq - 1) * sizeof(float));           for (int i = 0; i < nq; ++i) {         h_q[i] = static_cast<float>(i);         h_h[i] = static_cast<float>(i * 2);     }           float *d_q, *d_h, *d_a, *d_fa;     cudaMalloc((void **)&d_q, nq * sizeof(float));     cudaMalloc((void **)&d_h, nq * sizeof(float));     cudaMalloc((void **)&d_a, (nq - 1) * sizeof(float));     cudaMalloc((void **)&d_fa, (nq - 1) * sizeof(float));           cudaMemcpy(d_q, h_q, nq * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_h, h_h, nq * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(256);       dim3 gridSize((nq + blockSize.x - 1) / blockSize.x);           faKernel<<<gridSize, blockSize>>>(d_q, d_h, nq, d_a, d_fa);           cudaMemcpy(h_a, d_a, (nq - 1) * sizeof(float), cudaMemcpyDeviceToHost);     cudaMemcpy(h_fa, d_fa, (nq - 1) * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Results printed here:\\n\");           free(h_q);     free(h_h);     free(h_a);     free(h_fa);     cudaFree(d_q);     cudaFree(d_h);     cudaFree(d_a);     cudaFree(d_fa);      return 0; }  "
    },
    {
        "id": "307",
        "c_code": "#include <stdio.h> #include <stdlib.h>   void matrixProduct(double *matrix_a, double *matrix_b, double *matrix_c, int width, int height, int from, int my_rank);  int main() {          int width = 3;     int height = 3;     int from = 0;     int my_rank = 0;          double *matrix_a = (double *)malloc(width * height * sizeof(double));     double *matrix_b = (double *)malloc(width * height * sizeof(double));     double *matrix_c = (double *)malloc(width * height * sizeof(double));           for (int i = 0; i < width * height; i++) {         matrix_a[i] = i + 1.0;          matrix_b[i] = i + 1.0;      }           matrixProduct(matrix_a, matrix_b, matrix_c, width, height, from, my_rank);           printf(\"Result Matrix:\\n\");     for (int i = 0; i < width; i++) {         for (int j = 0; j < height; j++) {             printf(\"%.2f \", matrix_c[i * width + j]);         }         printf(\"\\n\");     }           free(matrix_a);     free(matrix_b);     free(matrix_c);      return 0; }   void matrixProduct(double *matrix_a, double *matrix_b, double *matrix_c, int width, int height, int from, int my_rank) {     int row, col;      for (row = 0; row < width; row++) {         for (col = 0; col < height; col++) {             matrix_c[row * width + col] = 0;              for (int k = 0; k < width; k++) {                 matrix_c[row * width + col] += matrix_a[((row + from) * width) + k] * matrix_b[k * width + col];             }         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h> #include <cuda_runtime.h>  __global__ void matrixProduct(double *matrix_a, double *matrix_b, double *matrix_c, int width, int from, int my_rank) {     int row = threadIdx.y + blockDim.y * blockIdx.y;     int col = threadIdx.x + blockDim.x * blockIdx.x;     matrix_c[row * width + col] = 0;     for (int k = 0; k < width; k++) {         matrix_c[row * width + col] += matrix_a[((row + from) * width) + k] * matrix_b[k * width + col];     } }  int main() {          const int width = 256;       const int height = 256;      const int from = 0;          const int my_rank = 0;             double *h_matrix_a = (double *)malloc(width * height * sizeof(double));     double *h_matrix_b = (double *)malloc(width * width * sizeof(double));     double *h_matrix_c = (double *)malloc(width * height * sizeof(double));           for (int i = 0; i < width * height; ++i) {         h_matrix_a[i] = static_cast<double>(i);     }      for (int i = 0; i < width * width; ++i) {         h_matrix_b[i] = static_cast<double>(i);     }           double *d_matrix_a, *d_matrix_b, *d_matrix_c;     cudaMalloc((void **)&d_matrix_a, width * height * sizeof(double));     cudaMalloc((void **)&d_matrix_b, width * width * sizeof(double));     cudaMalloc((void **)&d_matrix_c, width * height * sizeof(double));           cudaMemcpy(d_matrix_a, h_matrix_a, width * height * sizeof(double), cudaMemcpyHostToDevice);     cudaMemcpy(d_matrix_b, h_matrix_b, width * width * sizeof(double), cudaMemcpyHostToDevice);           dim3 blockSize(16, 16);      dim3 gridSize((width + blockSize.x - 1) / blockSize.x, (height + blockSize.y - 1) / blockSize.y);           matrixProduct<<<gridSize, blockSize>>>(d_matrix_a, d_matrix_b, d_matrix_c, width, from, my_rank);           cudaMemcpy(h_matrix_c, d_matrix_c, width * height * sizeof(double), cudaMemcpyDeviceToHost);           printf(\"Results printed here:\\n\");           free(h_matrix_a);     free(h_matrix_b);     free(h_matrix_c);     cudaFree(d_matrix_a);     cudaFree(d_matrix_b);     cudaFree(d_matrix_c);      return 0; }   "
    },
    {
        "id": "308",
        "c_code": "#include <stdio.h> #include <stdlib.h>   void expandBoxes_cpu(const float *input, float *output, int dims, int clsNum);  int main() {          int dims = 12;     int clsNum = 3;     float *input = (float *)malloc(clsNum * 4 * sizeof(float));     float *output = (float *)malloc(dims * 4 * sizeof(float));           for (int i = 0; i < clsNum * 4; i++) {         input[i] = i + 1.0;      }           expandBoxes_cpu(input, output, dims, clsNum);           printf(\"Expanded Boxes Matrix:\\n\");     for (int i = 0; i < dims; i++) {         for (int j = 0; j < 4; j++) {             printf(\"%.2f \", output[i * 4 + j]);         }         printf(\"\\n\");     }           free(input);     free(output);      return 0; }   void expandBoxes_cpu(const float *input, float *output, int dims, int clsNum) {     for (int tid = 0; tid < dims; tid++) {         int k = tid / clsNum;         output[tid * 4 + 0] = input[k * 4 + 0];         output[tid * 4 + 1] = input[k * 4 + 1];         output[tid * 4 + 2] = input[k * 4 + 2];         output[tid * 4 + 3] = input[k * 4 + 3];     } }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h> #include <cuda_runtime.h>  __global__ void expandBoxes(const float *input, float *output, int dims, int clsNum) {     int tid = blockIdx.x * blockDim.x + threadIdx.x;     if (tid >= dims) {         return;     }      int k = tid / clsNum;     output[tid * 4 + 0] = input[k * 4 + 0];     output[tid * 4 + 1] = input[k * 4 + 1];     output[tid * 4 + 2] = input[k * 4 + 2];     output[tid * 4 + 3] = input[k * 4 + 3]; }  int main() {          const int dims = 256;        const int clsNum = 4;              float *h_input = (float *)malloc(dims * clsNum * sizeof(float));     float *h_output = (float *)malloc(dims * 4 * sizeof(float));           for (int i = 0; i < dims * clsNum; ++i) {         h_input[i] = static_cast<float>(i);     }           float *d_input, *d_output;     cudaMalloc((void **)&d_input, dims * clsNum * sizeof(float));     cudaMalloc((void **)&d_output, dims * 4 * sizeof(float));           cudaMemcpy(d_input, h_input, dims * clsNum * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(256);      dim3 gridSize((dims + blockSize.x - 1) / blockSize.x);           expandBoxes<<<gridSize, blockSize>>>(d_input, d_output, dims, clsNum);           cudaMemcpy(h_output, d_output, dims * 4 * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Results printed here:\\n\");           free(h_input);     free(h_output);     cudaFree(d_input);     cudaFree(d_output);      return 0; }   "
    },
    {
        "id": "309",
        "c_code": "#include <stdio.h> #include <stdlib.h>   void equalization(float *cdf, float *mincdf, unsigned char *ucharImage, int imageWidth, int imageHeight, int channels, int pixelSize);  int main() {          int imageWidth = 100;     int imageHeight = 100;     int channels = 3;     int pixelSize = imageWidth * imageHeight * channels;      float *cdf = (float *)malloc(256 * sizeof(float));     float *mincdf = (float *)malloc(1 * sizeof(float));     unsigned char *ucharImage = (unsigned char *)malloc(pixelSize * sizeof(unsigned char));           for (int i = 0; i < 256; i++) {         cdf[i] = i / 255.0;      }      for (int i = 0; i < 1; i++) {         mincdf[i] = 0.1;      }      for (int i = 0; i < pixelSize; i++) {         ucharImage[i] = i % 256;      }           equalization(cdf, mincdf, ucharImage, imageWidth, imageHeight, channels, pixelSize);           printf(\"Equalized Image:\\n\");     for (int i = 0; i < pixelSize; i++) {         printf(\"%u \", ucharImage[i]);         if ((i + 1) % channels == 0) {             printf(\"\\n\");         }     }           free(cdf);     free(mincdf);     free(ucharImage);      return 0; }   void equalization(float *cdf, float *mincdf, unsigned char *ucharImage, int imageWidth, int imageHeight, int channels, int pixelSize) {     int idx;      for (idx = 0; idx < pixelSize; idx++) {         unsigned char val = ucharImage[idx];         float data = 255 * (cdf[val] - mincdf[0]) / (1 - mincdf[0]);          if (data < 0.0f) {             data = 0.0f;         } else if (data > 255.0f) {             data = 255.0f;         }          ucharImage[idx] = (unsigned char)data;     } }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h> #include <cuda_runtime.h>  __global__ void equalization(float *cdf, float *mincdf, unsigned char *ucharImage, int imageWidth, int imageHeight, int channels, int pixelSize) {     int idx = threadIdx.x + blockDim.x * blockIdx.x;     if (idx < pixelSize) {         unsigned char val = ucharImage[idx];         float data = 255 * (cdf[val] - mincdf[0]) / (1 - mincdf[0]);         if (data < 0.0f)             data = 0.0f;         else if (data > 255.0f)             data = 255.0f;         ucharImage[idx] = (unsigned char)data;     } }  int main() {          const int imageWidth = 512;     const int imageHeight = 512;     const int channels = 3;      const int pixelSize = imageWidth * imageHeight * channels;           float *h_cdf = (float *)malloc(256 * sizeof(float));     float *h_mincdf = (float *)malloc(sizeof(float));     unsigned char *h_ucharImage = (unsigned char *)malloc(pixelSize * sizeof(unsigned char));           for (int i = 0; i < 256; ++i) {         h_cdf[i] = static_cast<float>(i);     }      *h_mincdf = 0.5f;       for (int i = 0; i < pixelSize; ++i) {         h_ucharImage[i] = static_cast<unsigned char>(i % 256);     }           float *d_cdf, *d_mincdf;     unsigned char *d_ucharImage;     cudaMalloc((void **)&d_cdf, 256 * sizeof(float));     cudaMalloc((void **)&d_mincdf, sizeof(float));     cudaMalloc((void **)&d_ucharImage, pixelSize * sizeof(unsigned char));           cudaMemcpy(d_cdf, h_cdf, 256 * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_mincdf, h_mincdf, sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_ucharImage, h_ucharImage, pixelSize * sizeof(unsigned char), cudaMemcpyHostToDevice);           dim3 blockSize(256);      dim3 gridSize((pixelSize + blockSize.x - 1) / blockSize.x);           equalization<<<gridSize, blockSize>>>(d_cdf, d_mincdf, d_ucharImage, imageWidth, imageHeight, channels, pixelSize);           cudaMemcpy(h_ucharImage, d_ucharImage, pixelSize * sizeof(unsigned char), cudaMemcpyDeviceToHost);           printf(\"Results printed here:\\n\");           free(h_cdf);     free(h_mincdf);     free(h_ucharImage);     cudaFree(d_cdf);     cudaFree(d_mincdf);     cudaFree(d_ucharImage);      return 0; }   "
    },
    {
        "id": "310",
        "c_code": "#include <stdio.h> #include <stdlib.h> #include <math.h>   void smooth_l1_cpu(int n, float *pred, float *truth, float *delta, float *error);  int main() {          int n = 5;     float *pred = (float *)malloc(n * sizeof(float));     float *truth = (float *)malloc(n * sizeof(float));     float *delta = (float *)malloc(n * sizeof(float));     float *error = (float *)malloc(n * sizeof(float));           for (int i = 0; i < n; i++) {         pred[i] = i + 1.0;          truth[i] = i + 1.5;      }           smooth_l1_cpu(n, pred, truth, delta, error);           printf(\"Smooth L1 Error:\\n\");     for (int i = 0; i < n; i++) {         printf(\"%.2f \", error[i]);     }           free(pred);     free(truth);     free(delta);     free(error);      return 0; }   void smooth_l1_cpu(int n, float *pred, float *truth, float *delta, float *error) {     int i;      for (i = 0; i < n; ++i) {         float diff = truth[i] - pred[i];         float abs_val = fabs(diff);          if (abs_val < 1) {             error[i] = diff * diff;             delta[i] = diff;         } else {             error[i] = 2 * abs_val - 1;             delta[i] = (diff < 0) ? 1 : -1;         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h> #include <cuda_runtime.h>  __global__ void smooth_l1_kernel(int n, float *pred, float *truth, float *delta, float *error) {     int i = (blockIdx.x + blockIdx.y * gridDim.x) * blockDim.x + threadIdx.x;     if (i < n) {         float diff = truth[i] - pred[i];         float abs_val = fabsf(diff);         if (abs_val < 1) {             error[i] = diff * diff;             delta[i] = diff;         } else {             error[i] = 2 * abs_val - 1;             delta[i] = (diff > 0) ? 1 : -1;         }     } }  int main() {          const int n = 1024;           float *h_pred = (float *)malloc(n * sizeof(float));     float *h_truth = (float *)malloc(n * sizeof(float));     float *h_delta = (float *)malloc(n * sizeof(float));     float *h_error = (float *)malloc(n * sizeof(float));           for (int i = 0; i < n; ++i) {         h_pred[i] = static_cast<float>(i);         h_truth[i] = static_cast<float>(i * 2);     }           float *d_pred, *d_truth, *d_delta, *d_error;     cudaMalloc((void **)&d_pred, n * sizeof(float));     cudaMalloc((void **)&d_truth, n * sizeof(float));     cudaMalloc((void **)&d_delta, n * sizeof(float));     cudaMalloc((void **)&d_error, n * sizeof(float));           cudaMemcpy(d_pred, h_pred, n * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_truth, h_truth, n * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(256);      dim3 gridSize((n + blockSize.x - 1) / blockSize.x);           smooth_l1_kernel<<<gridSize, blockSize>>>(n, d_pred, d_truth, d_delta, d_error);           cudaMemcpy(h_delta, d_delta, n * sizeof(float), cudaMemcpyDeviceToHost);     cudaMemcpy(h_error, d_error, n * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Results printed here:\\n\");           free(h_pred);     free(h_truth);     free(h_delta);     free(h_error);     cudaFree(d_pred);     cudaFree(d_truth);     cudaFree(d_delta);     cudaFree(d_error);      return 0; }   "
    },
    {
        "id": "311",
        "c_code": "#include <stdio.h> #include <stdlib.h>   void multiply_matrices(float *A_Matrix, float *B_Matrix, float *ANS_Matrix, int N);  int main() {          int N = 3;      float *A_Matrix = (float *)malloc(N * N * sizeof(float));     float *B_Matrix = (float *)malloc(N * N * sizeof(float));     float *ANS_Matrix = (float *)malloc(N * N * sizeof(float));           for (int i = 0; i < N * N; i++) {         A_Matrix[i] = i + 1.0;          B_Matrix[i] = i + 2.0;      }           multiply_matrices(A_Matrix, B_Matrix, ANS_Matrix, N);           printf(\"Resultant Matrix:\\n\");     for (int i = 0; i < N; i++) {         for (int j = 0; j < N; j++) {             printf(\"%.2f \", ANS_Matrix[i * N + j]);         }         printf(\"\\n\");     }           free(A_Matrix);     free(B_Matrix);     free(ANS_Matrix);      return 0; }   void multiply_matrices(float *A_Matrix, float *B_Matrix, float *ANS_Matrix, int N) {     int i, j, k;     float sum, m, n;      for (i = 0; i < N; i++) {         for (j = 0; j < N; j++) {             sum = 0;             for (k = 0; k < N; k++) {                 m = *(A_Matrix + i * N + k);                 n = *(B_Matrix + k * N + j);                 sum += m * n;             }             *(ANS_Matrix + i * N + j) = sum;         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h> #include <cuda_runtime.h>   #define WIDTH 1024   __global__ void matrixMult(float *A, float *B, float *C, int width) {     int k = 0;     float sum = 0;     int col = blockDim.x * blockIdx.x + threadIdx.x;     int row = blockDim.y * blockIdx.y + threadIdx.y;      if (col < width && row < width) {         for (k = 0; k < width; k++) {             sum += A[row * width + k] * B[k * width + col];         }         C[row * width + col] = sum;     } }  int main() {          float *h_A, *h_B, *h_C;     h_A = (float *)malloc(WIDTH * WIDTH * sizeof(float));     h_B = (float *)malloc(WIDTH * WIDTH * sizeof(float));     h_C = (float *)malloc(WIDTH * WIDTH * sizeof(float));           for (int i = 0; i < WIDTH * WIDTH; ++i) {         h_A[i] = 1.0f;         h_B[i] = 2.0f;     }           float *d_A, *d_B, *d_C;     cudaMalloc((void **)&d_A, WIDTH * WIDTH * sizeof(float));     cudaMalloc((void **)&d_B, WIDTH * WIDTH * sizeof(float));     cudaMalloc((void **)&d_C, WIDTH * WIDTH * sizeof(float));           cudaMemcpy(d_A, h_A, WIDTH * WIDTH * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_B, h_B, WIDTH * WIDTH * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(16, 16);      dim3 gridSize((WIDTH + blockSize.x - 1) / blockSize.x, (WIDTH + blockSize.y - 1) / blockSize.y);           matrixMult<<<gridSize, blockSize>>>(d_A, d_B, d_C, WIDTH);           cudaMemcpy(h_C, d_C, WIDTH * WIDTH * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Results printed here:\\n\");           free(h_A);     free(h_B);     free(h_C);     cudaFree(d_A);     cudaFree(d_B);     cudaFree(d_C);      return 0; }   "
    },
    {
        "id": "312",
        "c_code": "#include <stdio.h> #include <stdlib.h>   void SetToZero_kernel(float *d_vx, float *d_vy, float *d_vz, int w, int h, int l);  int main() {          int w = 3;      int h = 3;      int l = 3;            float *d_vx = (float *)malloc(w * h * l * sizeof(float));     float *d_vy = (float *)malloc(w * h * l * sizeof(float));     float *d_vz = (float *)malloc(w * h * l * sizeof(float));           SetToZero_kernel(d_vx, d_vy, d_vz, w, h, l);                      free(d_vx);     free(d_vy);     free(d_vz);      return 0; }   void SetToZero_kernel(float *d_vx, float *d_vy, float *d_vz, int w, int h, int l) {     unsigned int i, j;      for (i = 0; i < w; i++) {         for (j = 0; j < h; j++) {             unsigned int index = j * w + i;             for (int k = 0; k < l; ++k, index += w * h) {                 d_vx[index] = 0;                 d_vy[index] = 0;                 d_vz[index] = 0;             }         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h>   #define WIDTH 1024 #define HEIGHT 1024 #define DEPTH 1024   __global__ void SetToZero_kernel(float *d_vx, float *d_vy, float *d_vz, int w, int h, int l) {     unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;     unsigned int j = blockIdx.y * blockDim.y + threadIdx.y;     unsigned int index = j * w + i;      if (i < w && j < h) {         for (int k = 0; k < l; ++k, index += w * h) {             d_vx[index] = 0;             d_vy[index] = 0;             d_vz[index] = 0;         }     } }  int main() {          float *d_vx, *d_vy, *d_vz;     cudaMalloc((void **)&d_vx, WIDTH * HEIGHT * DEPTH * sizeof(float));     cudaMalloc((void **)&d_vy, WIDTH * HEIGHT * DEPTH * sizeof(float));     cudaMalloc((void **)&d_vz, WIDTH * HEIGHT * DEPTH * sizeof(float));           dim3 blockSize(16, 16);       dim3 gridSize((WIDTH + blockSize.x - 1) / blockSize.x, (HEIGHT + blockSize.y - 1) / blockSize.y);           SetToZero_kernel<<<gridSize, blockSize>>>(d_vx, d_vy, d_vz, WIDTH, HEIGHT, DEPTH);           cudaDeviceSynchronize();           cudaError_t cudaErr = cudaGetLastError();     if (cudaErr != cudaSuccess) {         fprintf(stderr, \"CUDA error: %s\\n\", cudaGetErrorString(cudaErr));         return 1;     }           cudaFree(d_vx);     cudaFree(d_vy);     cudaFree(d_vz);      return 0; }   "
    },
    {
        "id": "313",
        "c_code": "#include <stdio.h> #include <stdlib.h>   void solveLower_cpu(const double *lower, const double *b, double *buf, int dim, int n);  int main() {          int dim = 3;      int n = 2;              double *lower = (double *)malloc(dim * dim * sizeof(double));     double *b = (double *)malloc(n * dim * sizeof(double));     double *buf = (double *)malloc(n * dim * sizeof(double));           for (int i = 0; i < dim * dim; i++) {         lower[i] = i + 1.0;      }      for (int i = 0; i < n * dim; i++) {         b[i] = i + 1.0;      }           solveLower_cpu(lower, b, buf, dim, n);                      free(lower);     free(b);     free(buf);      return 0; }   void solveLower_cpu(const double *lower, const double *b, double *buf, int dim, int n) {     for (int k = 0; k < n; k++) {         for (int i = 0; i < dim; i++) {             double val = b[k * dim + i];             for (int j = 0; j < i; j++) {                 val -= lower[i * dim + j] * buf[k * dim + j];             }             buf[k * dim + i] = val / lower[i * dim + i];         }     } }   ",
        "cuda_code": " #include <stdio.h> #include <cuda_runtime.h>  #define DIM 128 #define N 1024   __global__ void solveLowerKernel(const double *lower, const double *b, double *buf, int dim, int n) {     int k = blockIdx.x * blockDim.x + threadIdx.x;      if (k < n) {         for (int i = 0; i < dim; i++) {             double val = b[k * dim + i];             for (int j = 0; j < i; j++) {                 val -= lower[i * dim + j] * buf[k * dim + j];             }             buf[k * dim + i] = val / lower[i * dim + i];         }     } }  int main() {          double *d_lower, *d_b, *d_buf;      cudaMalloc((void **)&d_lower, DIM * DIM * sizeof(double));     cudaMalloc((void **)&d_b, N * DIM * sizeof(double));     cudaMalloc((void **)&d_buf, N * DIM * sizeof(double));           dim3 blockSize(256);       dim3 gridSize((N + blockSize.x - 1) / blockSize.x);           solveLowerKernel<<<gridSize, blockSize>>>(d_lower, d_b, d_buf, DIM, N);           cudaDeviceSynchronize();           cudaError_t cudaErr = cudaGetLastError();     if (cudaErr != cudaSuccess) {         fprintf(stderr, \"CUDA error: %s\\n\", cudaGetErrorString(cudaErr));         return 1;     }           cudaFree(d_lower);     cudaFree(d_b);     cudaFree(d_buf);      return 0; }  "
    },
    {
        "id": "314",
        "c_code": "#include <stdio.h> #include <stdlib.h>   void deinter_cpu(int NX, float *X, int NY, float *Y, int B, float *OUT);  int main() {          int NX = 3;      int NY = 2;      int B = 4;             float *X = (float *)malloc(B * NX * sizeof(float));     float *Y = (float *)malloc(B * NY * sizeof(float));     float *OUT = (float *)malloc((NX + NY) * B * sizeof(float));           for (int i = 0; i < B * NX; i++) {         X[i] = i + 1.0;      }      for (int i = 0; i < B * NY; i++) {         Y[i] = i + 1.0;      }           deinter_cpu(NX, X, NY, Y, B, OUT);                      free(X);     free(Y);     free(OUT);      return 0; }   void deinter_cpu(int NX, float *X, int NY, float *Y, int B, float *OUT) {     int i, j;     int index = 0;      for (j = 0; j < B; ++j) {         for (i = 0; i < NX; ++i) {             if (X) X[j * NX + i] += OUT[index];             ++index;         }          for (i = 0; i < NY; ++i) {             if (Y) Y[j * NY + i] += OUT[index];             ++index;         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h>  #define NX 128 #define NY 64 #define B 512   __global__ void deinter_kernel(int NX, float *X, int NY, float *Y, int B, float *OUT) {     int i = (blockIdx.x + blockIdx.y * gridDim.x) * blockDim.x + threadIdx.x;      if (i < (NX + NY) * B) {         int b = i / (NX + NY);         int j = i % (NX + NY);          if (j < NX) {             if (X)                 X[b * NX + j] += OUT[i];         } else {             if (Y)                 Y[b * NY + j - NX] += OUT[i];         }     } }  int main() {          float *d_X, *d_Y, *d_OUT;      cudaMalloc((void **)&d_X, B * NX * sizeof(float));     cudaMalloc((void **)&d_Y, B * NY * sizeof(float));     cudaMalloc((void **)&d_OUT, B * (NX + NY) * sizeof(float));           dim3 blockSize(256);       dim3 gridSize((B * (NX + NY) + blockSize.x - 1) / blockSize.x);           deinter_kernel<<<gridSize, blockSize>>>(NX, d_X, NY, d_Y, B, d_OUT);           cudaDeviceSynchronize();           cudaError_t cudaErr = cudaGetLastError();     if (cudaErr != cudaSuccess) {         fprintf(stderr, \"CUDA error: %s\\n\", cudaGetErrorString(cudaErr));         return 1;     }           cudaFree(d_X);     cudaFree(d_Y);     cudaFree(d_OUT);      return 0; }   "
    },
    {
        "id": "315",
        "c_code": "#include <stdio.h> #include <stdlib.h> #include <math.h>   void binarize_input(float *input, int n, int size, float *binary);  int main() {          int n = 3;         int size = 5;            float *input = (float *)malloc(n * size * sizeof(float));     float *binary = (float *)malloc(n * size * sizeof(float));           for (int i = 0; i < n * size; i++) {         input[i] = i + 1.0;      }           binarize_input(input, n, size, binary);                      free(input);     free(binary);      return 0; }   void binarize_input(float *input, int n, int size, float *binary) {     int i, s;      for (s = 0; s < size; ++s) {         float mean = 0;          for (i = 0; i < n; ++i) {             mean += fabs(input[i * size + s]);         }          mean = mean / n;          for (i = 0; i < n; ++i) {             binary[i * size + s] = (input[i * size + s] > 0) ? mean : -mean;         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h>  #define N 512 #define Size 1024   __global__ void binarize_input_kernel(float *input, int n, int size, float *binary) {     int s = (blockIdx.x + blockIdx.y * gridDim.x) * blockDim.x + threadIdx.x;      if (s >= size)         return;      int i = 0;     float mean = 0;      for (i = 0; i < n; ++i) {         mean += fabs(input[i * size + s]);     }      mean = mean / n;      for (i = 0; i < n; ++i) {         binary[i * size + s] = (input[i * size + s] > 0) ? mean : -mean;     } }  int main() {          float *d_input, *d_binary;      cudaMalloc((void **)&d_input, N * Size * sizeof(float));     cudaMalloc((void **)&d_binary, N * Size * sizeof(float));           dim3 blockSize(256);       dim3 gridSize((N * Size + blockSize.x - 1) / blockSize.x);           binarize_input_kernel<<<gridSize, blockSize>>>(d_input, N, Size, d_binary);           cudaDeviceSynchronize();           cudaError_t cudaErr = cudaGetLastError();     if (cudaErr != cudaSuccess) {         fprintf(stderr, \"CUDA error: %s\\n\", cudaGetErrorString(cudaErr));         return 1;     }           cudaFree(d_input);     cudaFree(d_binary);      return 0; }   "
    },
    {
        "id": "316",
        "c_code": "#include <stdio.h> #include <stdlib.h>   void matrixMultiply_cpu(float *A, float *B, float *C, int numARows, int numAColumns, int numBRows, int numBColumns);  int main() {          int numARows = 3;         int numAColumns = 2;      int numBRows = 2;         int numBColumns = 4;            float *A = (float *)malloc(numARows * numAColumns * sizeof(float));     float *B = (float *)malloc(numBRows * numBColumns * sizeof(float));     float *C = (float *)malloc(numARows * numBColumns * sizeof(float));           for (int i = 0; i < numARows * numAColumns; i++) {         A[i] = i + 1.0;      }      for (int i = 0; i < numBRows * numBColumns; i++) {         B[i] = i + 1.0;      }           matrixMultiply_cpu(A, B, C, numARows, numAColumns, numBRows, numBColumns);                      free(A);     free(B);     free(C);      return 0; }   void matrixMultiply_cpu(float *A, float *B, float *C, int numARows, int numAColumns, int numBRows, int numBColumns) {     int numCRows = numARows;     int numCColumns = numBColumns;      for (int row = 0; row < numCRows; row++) {         for (int col = 0; col < numCColumns; col++) {             float sum = 0;              for (int k = 0; k < numBRows; k++) {                 sum += A[row * numAColumns + k] * B[k * numBColumns + col];             }              C[row * numCColumns + col] = sum;         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h>  #define numARows 1024 #define numAColumns 1024 #define numBRows 1024 #define numBColumns 1024   __global__ void matrixMultiply(float *A, float *B, float *C, int numARows, int numAColumns, int numBRows, int numBColumns) {     int row = blockIdx.y * blockDim.y + threadIdx.y;     int col = blockIdx.x * blockDim.x + threadIdx.x;      int numCRows = numARows;     int numCColumns = numBColumns;      if (row < numCRows && col < numCColumns) {         float sum = 0;          for (int k = 0; k < numBRows; k++) {             sum += A[row * numAColumns + k] * B[k * numBColumns + col];         }          C[row * numCColumns + col] = sum;     } }  int main() {          float *d_A, *d_B, *d_C;      cudaMalloc((void **)&d_A, numARows * numAColumns * sizeof(float));     cudaMalloc((void **)&d_B, numBRows * numBColumns * sizeof(float));     cudaMalloc((void **)&d_C, numARows * numBColumns * sizeof(float));           dim3 blockSize(16, 16);       dim3 gridSize((numBColumns + blockSize.x - 1) / blockSize.x, (numARows + blockSize.y - 1) / blockSize.y);           matrixMultiply<<<gridSize, blockSize>>>(d_A, d_B, d_C, numARows, numAColumns, numBRows, numBColumns);           cudaDeviceSynchronize();           cudaError_t cudaErr = cudaGetLastError();     if (cudaErr != cudaSuccess) {         fprintf(stderr, \"CUDA error: %s\\n\", cudaGetErrorString(cudaErr));         return 1;     }           cudaFree(d_A);     cudaFree(d_B);     cudaFree(d_C);      return 0; }   "
    },
    {
        "id": "317",
        "c_code": "#include <stdio.h> #include <stdlib.h>   void backward_avgpool_layer(int batch, int c, int w, int h, float *delta);  int main() {          int batch = 2;      int c = 3;         int w = 4;         int h = 4;               float *delta = (float *)malloc(batch * c * w * h * sizeof(float));           for (int i = 0; i < batch * c * w * h; i++) {         delta[i] = i + 1.0;      }           backward_avgpool_layer(batch, c, w, h, delta);                      free(delta);      return 0; }   void backward_avgpool_layer(int batch, int c, int w, int h, float *delta) {     int b, i, k;      for (b = 0; b < batch; ++b) {         for (k = 0; k < c; ++k) {             int out_index = k + b * c;              for (i = 0; i < h * w; ++i) {                 int in_index = i + h * w * (k + b * c);                 delta[in_index] += delta[out_index] / (h * w);             }         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h>   __global__ void backward_avgpool_layer_kernel(int n, int w, int h, int c, float *in_delta, float *out_delta) {     int id = (blockIdx.x + blockIdx.y * gridDim.x) * blockDim.x + threadIdx.x;     if (id >= n)         return;      int k = id % c;     id /= c;     int b = id;     int i;     int out_index = (k + c * b);      for (i = 0; i < w * h; ++i) {         int in_index = i + h * w * (k + b * c);         in_delta[in_index] += out_delta[out_index] / (w * h);     } }  int main() {          int n = 1024;       int w = 28;         int h = 28;     int c = 3;           float *d_in_delta, *d_out_delta;      cudaMalloc((void **)&d_in_delta, n * w * h * c * sizeof(float));     cudaMalloc((void **)&d_out_delta, n * c * sizeof(float));           dim3 blockSize(256);       dim3 gridSize((n + blockSize.x - 1) / blockSize.x, 1);           backward_avgpool_layer_kernel<<<gridSize, blockSize>>>(n, w, h, c, d_in_delta, d_out_delta);           cudaDeviceSynchronize();           cudaError_t cudaErr = cudaGetLastError();     if (cudaErr != cudaSuccess) {         fprintf(stderr, \"CUDA error: %s\\n\", cudaGetErrorString(cudaErr));         return 1;     }           cudaFree(d_in_delta);     cudaFree(d_out_delta);      return 0; }   "
    },
    {
        "id": "318",
        "c_code": "#include <stdio.h> #include <stdlib.h>   void mean_cpu(float *x, int batch, int filters, int spatial, float *mean);  int main() {          int batch = 2;           int filters = 3;         int spatial = 4;               float *x = (float *)malloc(batch * filters * spatial * sizeof(float));     float *mean = (float *)malloc(filters * sizeof(float));           for (int i = 0; i < batch * filters * spatial; i++) {         x[i] = i + 1.0;      }           mean_cpu(x, batch, filters, spatial, mean);           printf(\"Mean: \");     for (int i = 0; i < filters; i++) {         printf(\"%f \", mean[i]);     }     printf(\"\\n\");           free(x);     free(mean);      return 0; }   void mean_cpu(float *x, int batch, int filters, int spatial, float *mean) {     float scale = 1.0 / (batch * spatial);     int i, j, k;      for (i = 0; i < filters; ++i) {         mean[i] = 0;          for (j = 0; j < batch; ++j) {             for (k = 0; k < spatial; ++k) {                 int index = j * filters * spatial + i * spatial + k;                 mean[i] += x[index];             }         }          mean[i] *= scale;     } }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h>  __global__ void mean_kernel(float *x, int batch, int filters, int spatial, float *mean) {     float scale = 1.f / (batch * spatial);     int i = (blockIdx.x + blockIdx.y * gridDim.x) * blockDim.x + threadIdx.x;     if (i >= filters)         return;      int j, k;     mean[i] = 0;      for (j = 0; j < batch; ++j) {         for (k = 0; k < spatial; ++k) {             int index = j * filters * spatial + i * spatial + k;             mean[i] += x[index];         }     }      mean[i] *= scale; }  int main() {          int batch = 64;           int filters = 128;        int spatial = 256;              float *d_x, *d_mean;     cudaMalloc((void **)&d_x, batch * filters * spatial * sizeof(float));     cudaMalloc((void **)&d_mean, filters * sizeof(float));           dim3 blockSize(256);       dim3 gridSize((filters + blockSize.x - 1) / blockSize.x, 1);           mean_kernel<<<gridSize, blockSize>>>(d_x, batch, filters, spatial, d_mean);           cudaDeviceSynchronize();           cudaError_t cudaErr = cudaGetLastError();     if (cudaErr != cudaSuccess) {         fprintf(stderr, \"CUDA error: %s\\n\", cudaGetErrorString(cudaErr));         return 1;     }           cudaFree(d_x);     cudaFree(d_mean);      return 0; }   "
    },
    {
        "id": "319",
        "c_code": "#include <stdio.h> #include <stdlib.h>   void boxesScale_cpu(const float *input, float *output, int dims, float scale0, float scale1, float scale2, float scale3);  int main() {          int dims = 2;                   float scale0 = 2.0;             float scale1 = 1.5;             float scale2 = 3.0;             float scale3 = 0.5;                   float *input = (float *)malloc(dims * 4 * sizeof(float));     float *output = (float *)malloc(dims * 4 * sizeof(float));           for (int i = 0; i < dims * 4; i++) {         input[i] = i + 1.0;          }           boxesScale_cpu(input, output, dims, scale0, scale1, scale2, scale3);           printf(\"Output: \");     for (int i = 0; i < dims * 4; i++) {         printf(\"%f \", output[i]);     }     printf(\"\\n\");           free(input);     free(output);      return 0; }   void boxesScale_cpu(const float *input, float *output, int dims, float scale0, float scale1, float scale2, float scale3) {     for (int tid = 0; tid < dims; tid++) {         output[tid * 4] = input[tid * 4] / scale0;         output[tid * 4 + 1] = input[tid * 4 + 1] / scale1;         output[tid * 4 + 2] = input[tid * 4 + 2] / scale2;         output[tid * 4 + 3] = input[tid * 4 + 3] / scale3;     } }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h>  __global__ void boxesScale(const float *input, float *output, int dims, float scale0, float scale1, float scale2, float scale3) {     int tid = blockIdx.x * blockDim.x + threadIdx.x;     if (tid >= dims) {         return;     }      output[tid * 4] = input[tid * 4] / scale0;     output[tid * 4 + 1] = input[tid * 4 + 1] / scale1;     output[tid * 4 + 2] = input[tid * 4 + 2] / scale2;     output[tid * 4 + 3] = input[tid * 4 + 3] / scale3; }  int main() {          int dims = 128;              float scale0 = 2.0f;         float scale1 = 1.5f;         float scale2 = 1.0f;         float scale3 = 0.5f;               float *d_input, *d_output;     cudaMalloc((void **)&d_input, dims * 4 * sizeof(float));     cudaMalloc((void **)&d_output, dims * 4 * sizeof(float));           dim3 blockSize(256);        dim3 gridSize((dims + blockSize.x - 1) / blockSize.x, 1);           boxesScale<<<gridSize, blockSize>>>(d_input, d_output, dims, scale0, scale1, scale2, scale3);           cudaDeviceSynchronize();           cudaError_t cudaErr = cudaGetLastError();     if (cudaErr != cudaSuccess) {         fprintf(stderr, \"CUDA error: %s\\n\", cudaGetErrorString(cudaErr));         return 1;     }           cudaFree(d_input);     cudaFree(d_output);      return 0; }   "
    },
    {
        "id": "320",
        "c_code": "#include <stdio.h>   void mat_mul_seq(int *m_A, int *m_B, int *m_C, int A_rows, int A_cols, int B_rows, int B_cols);  int main() {          int A_rows = 2, A_cols = 3;     int B_rows = 3, B_cols = 4;      int matrix_A[] = {1, 2, 3, 4, 5, 6};     int matrix_B[] = {7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18};           int matrix_C[A_rows * B_cols];           mat_mul_seq(matrix_A, matrix_B, matrix_C, A_rows, A_cols, B_rows, B_cols);           printf(\"Resulting Matrix C:\\n\");     for (int i = 0; i < A_rows; i++) {         for (int j = 0; j < B_cols; j++) {             printf(\"%d \", matrix_C[i * B_cols + j]);         }         printf(\"\\n\");     }      return 0; }   void mat_mul_seq(int *m_A, int *m_B, int *m_C, int A_rows, int A_cols, int B_rows, int B_cols) {     int sum;     for (int i = 0; i < A_rows; i++) {         for (int j = 0; j < B_cols; j++) {             sum = 0;             for (int k = 0; k < A_cols; k++) {                 sum += m_A[i * A_cols + k] * m_B[k * B_cols + j];             }             m_C[i * B_cols + j] = sum;         }     } }  ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h>  __global__ void mat_mul_kernel(int *m_A, int *m_B, int *m_C, int A_rows, int A_cols, int B_rows, int B_cols) {     int sum = 0;     int row = threadIdx.y + blockIdx.y * blockDim.y;     int col = threadIdx.x + blockIdx.x * blockDim.x;      if (row < A_rows && col < B_cols) {         for (int i = 0; i < A_cols; i++) {             sum += m_A[row * A_cols + i] * m_B[i * B_cols + col];         }         m_C[row * B_cols + col] = sum;     } }  int main() {          int A_rows = 4;       int A_cols = 3;       int B_rows = 3;       int B_cols = 5;             int *h_A, *h_B, *h_C;     h_A = (int *)malloc(A_rows * A_cols * sizeof(int));     h_B = (int *)malloc(B_rows * B_cols * sizeof(int));     h_C = (int *)malloc(A_rows * B_cols * sizeof(int));           for (int i = 0; i < A_rows * A_cols; i++) {         h_A[i] = i + 1;     }      for (int i = 0; i < B_rows * B_cols; i++) {         h_B[i] = i + 1;     }           int *d_A, *d_B, *d_C;     cudaMalloc((void **)&d_A, A_rows * A_cols * sizeof(int));     cudaMalloc((void **)&d_B, B_rows * B_cols * sizeof(int));     cudaMalloc((void **)&d_C, A_rows * B_cols * sizeof(int));           cudaMemcpy(d_A, h_A, A_rows * A_cols * sizeof(int), cudaMemcpyHostToDevice);     cudaMemcpy(d_B, h_B, B_rows * B_cols * sizeof(int), cudaMemcpyHostToDevice);           dim3 blockSize(16, 16);       dim3 gridSize((B_cols + blockSize.x - 1) / blockSize.x, (A_rows + blockSize.y - 1) / blockSize.y);           mat_mul_kernel<<<gridSize, blockSize>>>(d_A, d_B, d_C, A_rows, A_cols, B_rows, B_cols);           cudaDeviceSynchronize();           cudaError_t cudaErr = cudaGetLastError();     if (cudaErr != cudaSuccess) {         fprintf(stderr, \"CUDA error: %s\\n\", cudaGetErrorString(cudaErr));         return 1;     }           cudaMemcpy(h_C, d_C, A_rows * B_cols * sizeof(int), cudaMemcpyDeviceToHost);           printf(\"Result Matrix:\\n\");     for (int i = 0; i < A_rows; i++) {         for (int j = 0; j < B_cols; j++) {             printf(\"%d \", h_C[i * B_cols + j]);         }         printf(\"\\n\");     }           free(h_A);     free(h_B);     free(h_C);     cudaFree(d_A);     cudaFree(d_B);     cudaFree(d_C);      return 0; }   "
    },
    {
        "id": "321",
        "c_code": "#include <stdio.h>   void create_p_vect(float *node_info1, float *node_info2, float *p, int n_nodes_1, int n_nodes_2);  int main() {          int n_nodes_1 = 3;     int n_nodes_2 = 4;      float node_info1[] = {0.2, 0.6, 0.8};     float node_info2[] = {0.3, 0.7, 0.4, 0.9};           float p[n_nodes_1 * n_nodes_2];           create_p_vect(node_info1, node_info2, p, n_nodes_1, n_nodes_2);           printf(\"Resulting Array p:\\n\");     for (int i = 0; i < n_nodes_1; i++) {         for (int j = 0; j < n_nodes_2; j++) {             printf(\"%.2f \", p[i * n_nodes_2 + j]);         }         printf(\"\\n\");     }      return 0; }   void create_p_vect(float *node_info1, float *node_info2, float *p, int n_nodes_1, int n_nodes_2) {     int tx, ty;     float cutoff = 0.5;     for (tx = 0; tx < n_nodes_1; tx++) {         for (ty = 0; ty < n_nodes_2; ty++) {             int ind = tx * n_nodes_2 + ty;             if ((node_info1[tx] < cutoff) && (node_info2[ty] < cutoff))                 p[ind] = 0;             else                 p[ind] = node_info1[tx] * node_info2[ty];         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h>  __global__ void create_p_vect(float *node_info1, float *node_info2, float *p, int n_nodes_1, int n_nodes_2) {     int tx = threadIdx.x + blockDim.x * blockIdx.x;     int ty = threadIdx.y + blockDim.y * blockIdx.y;     float cutoff = 0.5;      if ((tx < n_nodes_1) && (ty < n_nodes_2)) {         int ind = tx * n_nodes_2 + ty;          if ((node_info1[tx] < cutoff) && (node_info2[ty] < cutoff))             p[ind] = 0;         else             p[ind] = node_info1[tx] * node_info2[ty];     } }  int main() {          int n_nodes_1 = 4;       int n_nodes_2 = 3;             float *h_node_info1, *h_node_info2, *h_p;     h_node_info1 = (float *)malloc(n_nodes_1 * sizeof(float));     h_node_info2 = (float *)malloc(n_nodes_2 * sizeof(float));     h_p = (float *)malloc(n_nodes_1 * n_nodes_2 * sizeof(float));           for (int i = 0; i < n_nodes_1; i++) {         h_node_info1[i] = static_cast<float>(i + 1) / 10.0f;     }      for (int i = 0; i < n_nodes_2; i++) {         h_node_info2[i] = static_cast<float>(i + 1) / 10.0f;     }           float *d_node_info1, *d_node_info2, *d_p;     cudaMalloc((void **)&d_node_info1, n_nodes_1 * sizeof(float));     cudaMalloc((void **)&d_node_info2, n_nodes_2 * sizeof(float));     cudaMalloc((void **)&d_p, n_nodes_1 * n_nodes_2 * sizeof(float));           cudaMemcpy(d_node_info1, h_node_info1, n_nodes_1 * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_node_info2, h_node_info2, n_nodes_2 * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(16, 16);       dim3 gridSize((n_nodes_1 + blockSize.x - 1) / blockSize.x, (n_nodes_2 + blockSize.y - 1) / blockSize.y);           create_p_vect<<<gridSize, blockSize>>>(d_node_info1, d_node_info2, d_p, n_nodes_1, n_nodes_2);           cudaDeviceSynchronize();           cudaError_t cudaErr = cudaGetLastError();     if (cudaErr != cudaSuccess) {         fprintf(stderr, \"CUDA error: %s\\n\", cudaGetErrorString(cudaErr));         return 1;     }           cudaMemcpy(h_p, d_p, n_nodes_1 * n_nodes_2 * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Result Array:\\n\");     for (int i = 0; i < n_nodes_1; i++) {         for (int j = 0; j < n_nodes_2; j++) {             printf(\"%.2f \", h_p[i * n_nodes_2 + j]);         }         printf(\"\\n\");     }           free(h_node_info1);     free(h_node_info2);     free(h_p);     cudaFree(d_node_info1);     cudaFree(d_node_info2);     cudaFree(d_p);      return 0; }   "
    },
    {
        "id": "322",
        "c_code": "#include <stdio.h> #include <stdlib.h>  void calcbidvalues(int n, int *src2tgt, float *adj, float *prices, unsigned short *complete, float *values, float *bids) {     for (int idx = 0; idx < n * n; idx++) {         int i = idx / n;         int j = idx - i * n;         bids[i * n + j] = -1;         if (src2tgt[i] != -1) {             continue;         }         complete[0] = 0;         values[i * n + j] = -adj[i * n + j] - prices[j];     } }  int main() {     int n = 5;      int *src2tgt = (int *)malloc(n * sizeof(int));     float *adj = (float *)malloc(n * n * sizeof(float));     float *prices = (float *)malloc(n * sizeof(float));     unsigned short *complete = (unsigned short *)malloc(sizeof(unsigned short));     float *values = (float *)malloc(n * n * sizeof(float));     float *bids = (float *)malloc(n * n * sizeof(float));                      calcbidvalues(n, src2tgt, adj, prices, complete, values, bids);                      free(src2tgt);     free(adj);     free(prices);     free(complete);     free(values);     free(bids);      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h>  __global__ void calcbidvalues(int n, int *src2tgt, float *adj, float *prices, bool *complete, float *values, float *bids) {     int INDEX = blockIdx.x * blockDim.x + threadIdx.x;     int stride = blockDim.x * gridDim.x;      for (int idx = INDEX; idx < n * n; idx += stride) {         int i = idx / n;         int j = idx - i * n;         bids[i * n + j] = -1;          if (src2tgt[i] != -1) {             continue;         }          complete[0] = false;         values[i * n + j] = -adj[i * n + j] - prices[j];     } }  int main() {          int n = 4;            int *h_src2tgt;     float *h_adj, *h_prices, *h_values, *h_bids;     bool *h_complete;      h_src2tgt = (int *)malloc(n * sizeof(int));     h_adj = (float *)malloc(n * n * sizeof(float));     h_prices = (float *)malloc(n * sizeof(float));     h_values = (float *)malloc(n * n * sizeof(float));     h_bids = (float *)malloc(n * n * sizeof(float));     h_complete = (bool *)malloc(sizeof(bool));           for (int i = 0; i < n; i++) {         h_src2tgt[i] = -1;          h_prices[i] = 0.5;            for (int j = 0; j < n; j++) {             h_adj[i * n + j] = static_cast<float>(i + j) / 10.0f;          }     }           int *d_src2tgt;     float *d_adj, *d_prices, *d_values, *d_bids;     bool *d_complete;      cudaMalloc((void **)&d_src2tgt, n * sizeof(int));     cudaMalloc((void **)&d_adj, n * n * sizeof(float));     cudaMalloc((void **)&d_prices, n * sizeof(float));     cudaMalloc((void **)&d_values, n * n * sizeof(float));     cudaMalloc((void **)&d_bids, n * n * sizeof(float));     cudaMalloc((void **)&d_complete, sizeof(bool));           cudaMemcpy(d_src2tgt, h_src2tgt, n * sizeof(int), cudaMemcpyHostToDevice);     cudaMemcpy(d_adj, h_adj, n * n * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_prices, h_prices, n * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(16, 16);      dim3 gridSize((n * n + blockSize.x - 1) / blockSize.x, 1);           calcbidvalues<<<gridSize, blockSize>>>(n, d_src2tgt, d_adj, d_prices, d_complete, d_values, d_bids);           cudaDeviceSynchronize();           cudaError_t cudaErr = cudaGetLastError();     if (cudaErr != cudaSuccess) {         fprintf(stderr, \"CUDA error: %s\\n\", cudaGetErrorString(cudaErr));         return 1;     }           cudaMemcpy(h_values, d_values, n * n * sizeof(float), cudaMemcpyDeviceToHost);     cudaMemcpy(h_bids, d_bids, n * n * sizeof(float), cudaMemcpyDeviceToHost);     cudaMemcpy(h_complete, d_complete, sizeof(bool), cudaMemcpyDeviceToHost);           printf(\"Values Array:\\n\");     for (int i = 0; i < n; i++) {         for (int j = 0; j < n; j++) {             printf(\"%.2f \", h_values[i * n + j]);         }         printf(\"\\n\");     }      printf(\"Bids Array:\\n\");     for (int i = 0; i < n; i++) {         for (int j = 0; j < n; j++) {             printf(\"%.2f \", h_bids[i * n + j]);         }         printf(\"\\n\");     }      printf(\"Complete Flag: %s\\n\", *h_complete ? \"true\" : \"false\");           free(h_src2tgt);     free(h_adj);     free(h_prices);     free(h_values);     free(h_bids);     free(h_complete   "
    },
    {
        "id": "323",
        "c_code": "#include <stdio.h> #include <stdlib.h>  void convolutionRowCPU(float *h_Dst, float *h_Src, float *h_Filter, int imageW, int imageH, int filterR) {     int x, y, k;     for (y = 0; y < imageH; y++) {         for (x = 0; x < imageW; x++) {             float sum = 0;             for (k = -filterR; k <= filterR; k++) {                 int d = x + k;                 if (d >= 0 && d < imageW) {                     sum += h_Src[y * imageW + d] * h_Filter[filterR - k];                 }             }             h_Dst[y * imageW + x] = sum;         }     } }  int main() {          int imageW = 5;     int imageH = 5;     int filterR = 1;           float *h_Src = (float *)malloc(imageW * imageH * sizeof(float));     float *h_Filter = (float *)malloc((2 * filterR + 1) * sizeof(float));     float *h_Dst = (float *)malloc(imageW * imageH * sizeof(float));           for (int i = 0; i < imageW * imageH; i++) {         h_Src[i] = rand() % 10;       }      for (int i = 0; i < 2 * filterR + 1; i++) {         h_Filter[i] = 0.1;       }           convolutionRowCPU(h_Dst, h_Src, h_Filter, imageW, imageH, filterR);           printf(\"Result:\\n\");     for (int i = 0; i < imageH; i++) {         for (int j = 0; j < imageW; j++) {             printf(\"%.2f\\t\", h_Dst[i * imageW + j]);         }         printf(\"\\n\");     }           free(h_Src);     free(h_Filter);     free(h_Dst);      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h>  __global__ void kernel_rows(const float *filter, const float *input, float *output, int imageW, int imageH, int filterR) {     int idx_x = threadIdx.x + blockDim.x * blockIdx.x;     int idx_y = threadIdx.y + blockDim.y * blockIdx.y;     int grid_width = gridDim.x * blockDim.x;     int idx = grid_width * idx_y + idx_x;     float sum = 0;      for (int k = -filterR; k <= filterR; k++) {         int d = idx_x + k;         if (d >= 0 && d < imageW) {             sum += input[idx_y * imageW + d] * filter[filterR - k];         }     }      output[idx] = sum; }  int main() {          int imageW = 128;       int imageH = 128;       int filterR = 3;              float *h_filter, *h_input, *h_output;     h_filter = (float *)malloc((2 * filterR + 1) * sizeof(float));     h_input = (float *)malloc(imageW * imageH * sizeof(float));     h_output = (float *)malloc(imageW * imageH * sizeof(float));           for (int i = 0; i < 2 * filterR + 1; i++) {         h_filter[i] = 1.0f / (2 * filterR + 1);      }      for (int i = 0; i < imageW * imageH; i++) {         h_input[i] = static_cast<float>(i % 255);      }           float *d_filter, *d_input, *d_output;     cudaMalloc((void **)&d_filter, (2 * filterR + 1) * sizeof(float));     cudaMalloc((void **)&d_input, imageW * imageH * sizeof(float));     cudaMalloc((void **)&d_output, imageW * imageH * sizeof(float));           cudaMemcpy(d_filter, h_filter, (2 * filterR + 1) * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_input, h_input, imageW * imageH * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(16, 16);      dim3 gridSize((imageW + blockSize.x - 1) / blockSize.x, (imageH + blockSize.y - 1) / blockSize.y);           kernel_rows<<<gridSize, blockSize>>>(d_filter, d_input, d_output, imageW, imageH, filterR);           cudaDeviceSynchronize();           cudaError_t cudaErr = cudaGetLastError();     if (cudaErr != cudaSuccess) {         fprintf(stderr, \"CUDA error: %s\\n\", cudaGetErrorString(cudaErr));         return 1;     }           cudaMemcpy(h_output, d_output, imageW * imageH * sizeof(float), cudaMemcpyDeviceToHost);           printf(\"Output Array:\\n\");     for (int i = 0; i < imageH; i++) {         for (int j = 0; j < imageW; j++) {             printf(\"%.2f \", h_output[i * imageW + j]);         }         printf(\"\\n\");     }           free(h_filter);     free(h_input);     free(h_output);     cudaFree(d_filter);     cudaFree(d_input);     cudaFree(d_output);      return 0; }   "
    },
    {
        "id": "324",
        "c_code": "#include <stdio.h> #include <stdlib.h>  void castImageToGrayScale(unsigned char *ucharImage, unsigned char *grayImage, int imageWidth, int imageHeight, int channels) {     int w, h;     for (w = 0; w < imageWidth; w++) {         for (h = 0; h < imageHeight; h++) {             int idx = imageWidth * h + w;             unsigned char r = ucharImage[idx * channels];             unsigned char g = ucharImage[idx * channels + 1];             unsigned char b = ucharImage[idx * channels + 2];             grayImage[idx] = (unsigned char)(0.21f * r + 0.71f * g + 0.07f * b);         }     } }  int main() {          int imageWidth = 5;     int imageHeight = 5;     int channels = 3;            unsigned char *ucharImage = (unsigned char *)malloc(imageWidth * imageHeight * channels * sizeof(unsigned char));     unsigned char *grayImage = (unsigned char *)malloc(imageWidth * imageHeight * sizeof(unsigned char));           for (int i = 0; i < imageWidth * imageHeight * channels; i++) {         ucharImage[i] = rand() % 256;      }           castImageToGrayScale(ucharImage, grayImage, imageWidth, imageHeight, channels);           printf(\"Original Image:\\n\");     for (int i = 0; i < imageHeight; i++) {         for (int j = 0; j < imageWidth; j++) {             printf(\"(%3u, %3u, %3u)\\t\", ucharImage[(imageWidth * i + j) * channels], ucharImage[(imageWidth * i + j) * channels + 1], ucharImage[(imageWidth * i + j) * channels + 2]);         }         printf(\"\\n\");     }      printf(\"\\nGray Image:\\n\");     for (int i = 0; i < imageHeight; i++) {         for (int j = 0; j < imageWidth; j++) {             printf(\"%3u\\t\", grayImage[imageWidth * i + j]);         }         printf(\"\\n\");     }           free(ucharImage);     free(grayImage);      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h>  __global__ void castImageToGrayScale(unsigned char *ucharImage, unsigned char *grayImage, int imageWidth, int imageHeight, int channels) {     int w = threadIdx.x + blockDim.x * blockIdx.x;     int h = threadIdx.y + blockDim.y * blockIdx.y;     int idx = imageWidth * h + w;      if (w < imageWidth && h < imageHeight) {         unsigned char r = ucharImage[idx * channels];         unsigned char g = ucharImage[idx * channels + 1];         unsigned char b = ucharImage[idx * channels + 2];          grayImage[idx] = (unsigned char)(0.21f * r + 0.71f * g + 0.07f * b);     } }  int main() {          int imageWidth = 512;        int imageHeight = 512;       int channels = 3;                  unsigned char *h_ucharImage, *h_grayImage;     h_ucharImage = (unsigned char *)malloc(imageWidth * imageHeight * channels * sizeof(unsigned char));     h_grayImage = (unsigned char *)malloc(imageWidth * imageHeight * sizeof(unsigned char));           for (int i = 0; i < imageWidth * imageHeight * channels; i++) {         h_ucharImage[i] = (unsigned char)(i % 256);       }           unsigned char *d_ucharImage, *d_grayImage;     cudaMalloc((void **)&d_ucharImage, imageWidth * imageHeight * channels * sizeof(unsigned char));     cudaMalloc((void **)&d_grayImage, imageWidth * imageHeight * sizeof(unsigned char));           cudaMemcpy(d_ucharImage, h_ucharImage, imageWidth * imageHeight * channels * sizeof(unsigned char), cudaMemcpyHostToDevice);           dim3 blockSize(16, 16);       dim3 gridSize((imageWidth + blockSize.x - 1) / blockSize.x, (imageHeight + blockSize.y - 1) / blockSize.y);           castImageToGrayScale<<<gridSize, blockSize>>>(d_ucharImage, d_grayImage, imageWidth, imageHeight, channels);           cudaDeviceSynchronize();           cudaError_t cudaErr = cudaGetLastError();     if (cudaErr != cudaSuccess) {         fprintf(stderr, \"CUDA error: %s\\n\", cudaGetErrorString(cudaErr));         return 1;     }           cudaMemcpy(h_grayImage, d_grayImage, imageWidth * imageHeight * sizeof(unsigned char), cudaMemcpyDeviceToHost);           printf(\"Gray Image:\\n\");     for (int i = 0; i < imageHeight; i++) {         for (int j = 0; j < imageWidth; j++) {             printf(\"%u \", h_grayImage[i * imageWidth + j]);         }         printf(\"\\n\");     }           free(h_ucharImage);     free(h_grayImage);     cudaFree(d_ucharImage);     cudaFree(d_grayImage);      return 0; }   "
    },
    {
        "id": "325",
        "c_code": "#include <stdio.h> #include <stdlib.h>  void CPU_array_rowKernel(double *input, double *output, int length) {     int xCuda, yCuda;     for (xCuda = 0; xCuda < length; xCuda++) {         for (yCuda = 0; yCuda < length; yCuda++) {             int idx = yCuda * length + xCuda;             if (xCuda == 0 || xCuda == length - 1) {                 output[idx] = 0;             } else {                 output[idx] = input[idx];                 output[idx] += xCuda == 0 ? 0 : input[idx - 1];                 output[idx] += xCuda == length - 1 ? 0 : input[idx + 1];             }         }     } }  int main() {          int length = 5;           double *input = (double *)malloc(length * length * sizeof(double));     double *output = (double *)malloc(length * length * sizeof(double));           for (int i = 0; i < length * length; i++) {         input[i] = rand() % 100;      }           CPU_array_rowKernel(input, output, length);           printf(\"Input Array:\\n\");     for (int i = 0; i < length; i++) {         for (int j = 0; j < length; j++) {             printf(\"%8.2f\\t\", input[length * i + j]);         }         printf(\"\\n\");     }      printf(\"\\nOutput Array:\\n\");     for (int i = 0; i < length; i++) {         for (int j = 0; j < length; j++) {             printf(\"%8.2f\\t\", output[length * i + j]);         }         printf(\"\\n\");     }           free(input);     free(output);      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h>   __global__ void GPU_array_rowKernel(double *input, double *output, int length) {     int xCuda = blockDim.x * blockIdx.x + threadIdx.x;     int yCuda = blockDim.y * blockIdx.y + threadIdx.y;     int idx = yCuda * length + xCuda;      if (xCuda >= length || yCuda >= length)         return;      if (xCuda == 0 || xCuda == length - 1) {         output[idx] = 0;         return;     }      output[idx] = input[idx];     output[idx] += xCuda == 0 ? 0 : input[idx - 1];     output[idx] += xCuda == length - 1 ? 0 : input[idx + 1]; }  int main() {          int length = 10;             double *h_input, *h_output;     h_input = (double *)malloc(length * length * sizeof(double));     h_output = (double *)malloc(length * length * sizeof(double));           for (int i = 0; i < length * length; i++) {         h_input[i] = i;       }           double *d_input, *d_output;     cudaMalloc((void **)&d_input, length * length * sizeof(double));     cudaMalloc((void **)&d_output, length * length * sizeof(double));           cudaMemcpy(d_input, h_input, length * length * sizeof(double), cudaMemcpyHostToDevice);           dim3 blockSize(16, 16);       dim3 gridSize((length + blockSize.x - 1) / blockSize.x, (length + blockSize.y - 1) / blockSize.y);           GPU_array_rowKernel<<<gridSize, blockSize>>>(d_input, d_output, length);           cudaDeviceSynchronize();           cudaError_t cudaErr = cudaGetLastError();     if (cudaErr != cudaSuccess) {         fprintf(stderr, \"CUDA error: %s\\n\", cudaGetErrorString(cudaErr));         return 1;     }           cudaMemcpy(h_output, d_output, length * length * sizeof(double), cudaMemcpyDeviceToHost);           printf(\"Output Array:\\n\");     for (int i = 0; i < length; i++) {         for (int j = 0; j < length; j++) {             printf(\"%f \", h_output[i * length + j]);         }         printf(\"\\n\");     }           free(h_input);     free(h_output);     cudaFree(d_input);     cudaFree(d_output);      return 0; }   "
    },
    {
        "id": "326",
        "c_code": "#include <stdio.h> #include <stdlib.h>  void waterElevationToDepth_cpu(const int nx_, const int ny_, float *h_ptr_, int h_pitch_, float *Bm_ptr_, int Bm_pitch_) {     for (int ti = 0; ti < nx_; ti++) {         for (int tj = 0; tj < ny_; tj++) {             float *h_row = (float *)((char *)h_ptr_ + h_pitch_ * tj);             float *Bm_row = (float *)((char *)Bm_ptr_ + Bm_pitch_ * tj);             h_row[ti] -= Bm_row[ti];         }     } }  int main() {          int nx = 5;      int ny = 5;            float *h_ptr = (float *)malloc(nx * ny * sizeof(float));     float *Bm_ptr = (float *)malloc(nx * ny * sizeof(float));           for (int i = 0; i < nx * ny; i++) {         h_ptr[i] = rand() % 100;          Bm_ptr[i] = rand() % 50;      }           waterElevationToDepth_cpu(nx, ny, h_ptr, nx, Bm_ptr, nx);           printf(\"h Array:\\n\");     for (int i = 0; i < nx; i++) {         for (int j = 0; j < ny; j++) {             printf(\"%8.2f\\t\", h_ptr[nx * i + j]);         }         printf(\"\\n\");     }      printf(\"\\nBm Array:\\n\");     for (int i = 0; i < nx; i++) {         for (int j = 0; j < ny; j++) {             printf(\"%8.2f\\t\", Bm_ptr[nx * i + j]);         }         printf(\"\\n\");     }           free(h_ptr);     free(Bm_ptr);      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h>  __global__ void waterElevationToDepth(const int nx_, const int ny_, float *h_ptr_, int h_pitch_, float *Bm_ptr_, int Bm_pitch_) {     int ti = blockIdx.x * blockDim.x + threadIdx.x;     int tj = blockIdx.y * blockDim.y + threadIdx.y;      if (ti < nx_ && tj < ny_) {         float *const h_row = (float *)((char *)h_ptr_ + h_pitch_ * tj);         float *const Bm_row = (float *)((char *)Bm_ptr_ + Bm_pitch_ * tj);         h_row[ti] -= Bm_row[ti];     } }  int main() {          const int nx = 256;       const int ny = 256;             float *h_host, *Bm_host;     size_t h_pitch, Bm_pitch;      cudaMallocPitch((void **)&h_host, &h_pitch, nx * sizeof(float), ny);     cudaMallocPitch((void **)&Bm_host, &Bm_pitch, nx * sizeof(float), ny);                      float *h_device, *Bm_device;     cudaMalloc((void **)&h_device, h_pitch * ny);     cudaMalloc((void **)&Bm_device, Bm_pitch * ny);           cudaMemcpy2D(h_device, h_pitch, h_host, h_pitch, nx * sizeof(float), ny, cudaMemcpyHostToDevice);     cudaMemcpy2D(Bm_device, Bm_pitch, Bm_host, Bm_pitch, nx * sizeof(float), ny, cudaMemcpyHostToDevice);           dim3 blockSize(16, 16);       dim3 gridSize((nx + blockSize.x - 1) / blockSize.x, (ny + blockSize.y - 1) / blockSize.y);           waterElevationToDepth<<<gridSize, blockSize>>>(nx, ny, h_device, h_pitch / sizeof(float), Bm_device, Bm_pitch / sizeof(float));           cudaDeviceSynchronize();           cudaError_t cudaErr = cudaGetLastError();     if (cudaErr != cudaSuccess) {         fprintf(stderr, \"CUDA error: %s\\n\", cudaGetErrorString(cudaErr));         return 1;     }                 cudaFree(h_device);     cudaFree(Bm_device);     cudaFreeHost(h_host);     cudaFreeHost(Bm_host);      return 0; }   "
    },
    {
        "id": "327",
        "c_code": "#include <stdio.h> #include <stdlib.h> #include <math.h>   float max(float a, float b) {     return a > b ? a : b; }  void softmax(float *x, int r, int c) {     float temp1, temp2;     for (int i = 0; i < r; i++) {         temp1 = 0.;         temp2 = 0.;         for (int j = 0; j < c; j++) {             temp1 = max(x[i * c + j], temp1);         }         for (int j = 0; j < c; j++) {             x[i * c + j] = expf(x[i * c + j] - temp1);             temp2 += x[i * c + j];         }         for (int j = 0; j < c; j++) {             x[i * c + j] /= temp2;         }     } }  int main() {          int r = 3;       int c = 4;             float *x = (float *)malloc(r * c * sizeof(float));           for (int i = 0; i < r * c; i++) {         x[i] = rand() % 100;       }           softmax(x, r, c);           printf(\"Softmax Result:\\n\");     for (int i = 0; i < r; i++) {         for (int j = 0; j < c; j++) {             printf(\"%8.4f\\t\", x[i * c + j]);         }         printf(\"\\n\");     }           free(x);      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h> #include <math.h>  __global__ void kernel_softmax(float *x, int r, int c) {     unsigned int i = blockDim.x * blockIdx.x + threadIdx.x;     if (i >= r)         return;      float temp1 = 0., temp2 = 0.;     for (int j = 0; j < c; j++)         temp1 = fmaxf(x[i * c + j], temp1);      for (int j = 0; j < c; j++) {         x[i * c + j] = expf(x[i * c + j] - temp1);         temp2 += x[i * c + j];     }      for (int j = 0; j < c; j++)         x[i * c + j] /= temp2; }  int main() {          const int r = 1000;       const int c = 10;               float *x_host = (float *)malloc(r * c * sizeof(float));                      float *x_device;     cudaMalloc((void **)&x_device, r * c * sizeof(float));           cudaMemcpy(x_device, x_host, r * c * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(256);       dim3 gridSize((r + blockSize.x - 1) / blockSize.x);           kernel_softmax<<<gridSize, blockSize>>>(x_device, r, c);           cudaDeviceSynchronize();           cudaError_t cudaErr = cudaGetLastError();     if (cudaErr != cudaSuccess) {         fprintf(stderr, \"CUDA error: %s\\n\", cudaGetErrorString(cudaErr));         return 1;     }                 cudaFree(x_device);     free(x_host);      return 0; }   "
    },
    {
        "id": "328",
        "c_code": "#include <stdio.h> #include <stdlib.h> #include <math.h>  void cuda_Adam_step_kernel(float *grad, float *data, float *m, float *v,                             short decay, float weight_decay,                             float beta1, float beta2,                             float eps, float step_size, int varsize) {     for (int i = 0; i < varsize; i++) {         float g = grad[i];         if (decay) g += weight_decay * data[i];         m[i] = beta1 * m[i] + (1.0 - beta1) * g;         v[i] = beta2 * v[i] + (1.0 - beta2) * g * g;         data[i] -= step_size * m[i] / (sqrt(v[i]) + eps);     } }  int main() {          int varsize = 10;             float *grad = (float *)malloc(varsize * sizeof(float));     float *data = (float *)malloc(varsize * sizeof(float));     float *m = (float *)malloc(varsize * sizeof(float));     float *v = (float *)malloc(varsize * sizeof(float));           for (int i = 0; i < varsize; i++) {         grad[i] = rand() % 100;           data[i] = rand() % 100;           m[i] = rand() % 100;             v[i] = rand() % 100;         }           short decay = 1;                 float weight_decay = 0.01;        float beta1 = 0.9;     float beta2 = 0.999;     float eps = 1e-8;     float step_size = 0.001;           cuda_Adam_step_kernel(grad, data, m, v, decay, weight_decay, beta1, beta2, eps, step_size, varsize);           printf(\"Updated Data:\\n\");     for (int i = 0; i < varsize; i++) {         printf(\"%8.4f\\t\", data[i]);     }     printf(\"\\n\");           free(grad);     free(data);     free(m);     free(v);      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h> #include <math.h>  __global__ void cuda_Adam_step_kernel(float *grad, float *data, float *m, float *v, bool decay, float weight_decay, float beta1, float beta2, float eps, float step_size, int varsize) {     int i = blockIdx.x * blockDim.x + threadIdx.x;     if (i >= varsize)         return;      float g = grad[i];     if (decay)         g += weight_decay * data[i];      m[i] = beta1 * m[i] + (1.0 - beta1) * g;     v[i] = beta2 * v[i] + (1.0 - beta2) * g * g;     data[i] -= step_size * m[i] / (sqrtf(v[i]) + eps); }  int main() {          const int varsize = 1000;             float *grad_host = (float *)malloc(varsize * sizeof(float));     float *data_host = (float *)malloc(varsize * sizeof(float));     float *m_host = (float *)malloc(varsize * sizeof(float));     float *v_host = (float *)malloc(varsize * sizeof(float));                      float *grad_device, *data_device, *m_device, *v_device;     cudaMalloc((void **)&grad_device, varsize * sizeof(float));     cudaMalloc((void **)&data_device, varsize * sizeof(float));     cudaMalloc((void **)&m_device, varsize * sizeof(float));     cudaMalloc((void **)&v_device, varsize * sizeof(float));           cudaMemcpy(grad_device, grad_host, varsize * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(data_device, data_host, varsize * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(m_device, m_host, varsize * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(v_device, v_host, varsize * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(256);       dim3 gridSize((varsize + blockSize.x - 1) / blockSize.x);           cuda_Adam_step_kernel<<<gridSize, blockSize>>>(grad_device, data_device, m_device, v_device, true, 0.001, 0.9, 0.999, 1e-8, 0.001, varsize);           cudaDeviceSynchronize();           cudaError_t cudaErr = cudaGetLastError();     if (cudaErr != cudaSuccess) {         fprintf(stderr, \"CUDA error: %s\\n\", cudaGetErrorString(cudaErr));         return 1;     }                 cudaFree(grad_device);     cudaFree(data_device);     cudaFree(m_device);     cudaFree(v_device);     free(grad_host);     free(data_host);     free(m_host);     free(v_host);      return 0; }   "
    },
    {
        "id": "329",
        "c_code": "#include <stdio.h> #include <stdlib.h> #include <string.h>  void flatten(float *x, int size, int layers, int batch, int forward) {     float *swap = calloc(size * layers * batch, sizeof(float));     int i, c, b;     for (b = 0; b < batch; ++b) {         for (c = 0; c < layers; ++c) {             for (i = 0; i < size; ++i) {                 int i1 = b * layers * size + c * size + i;                 int i2 = b * layers * size + i * layers + c;                 if (forward)                     swap[i2] = x[i1];                 else                     swap[i1] = x[i2];             }         }     }     memcpy(x, swap, size * layers * batch * sizeof(float));     free(swap); }  int main() {          int size = 2;     int layers = 3;     int batch = 4;           float *array = (float *)malloc(size * layers * batch * sizeof(float));           for (int i = 0; i < size * layers * batch; ++i) {         array[i] = i + 1;     }           printf(\"Original Array:\\n\");     for (int b = 0; b < batch; ++b) {         for (int c = 0; c < layers; ++c) {             for (int i = 0; i < size; ++i) {                 printf(\"%8.2f \", array[b * layers * size + c * size + i]);             }             printf(\"\\n\");         }         printf(\"\\n\");     }           flatten(array, size, layers, batch, 1);           printf(\"\\nFlattened Array (Forward):\\n\");     for (int i = 0; i < size * layers * batch; ++i) {         printf(\"%8.2f \", array[i]);     }     printf(\"\\n\");           flatten(array, size, layers, batch, 0);           printf(\"\\nUnflattened Array (Backward):\\n\");     for (int b = 0; b < batch; ++b) {         for (int c = 0; c < layers; ++c) {             for (int i = 0; i < size; ++i) {                 printf(\"%8.2f \", array[b * layers * size + c * size + i]);             }             printf(\"\\n\");         }         printf(\"\\n\");     }           free(array);      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h>  __global__ void flatten_kernel(int N, float *x, int spatial, int layers, int batch, int forward, float *out) {     int i = (blockIdx.x + blockIdx.y * gridDim.x) * blockDim.x + threadIdx.x;     if (i >= N)         return;      int in_s = i % spatial;     i = i / spatial;     int in_c = i % layers;     i = i / layers;     int b = i;      int i1 = b * layers * spatial + in_c * spatial + in_s;     int i2 = b * layers * spatial + in_s * layers + in_c;      if (forward)         out[i2] = x[i1];     else         out[i1] = x[i2]; }  int main() {          const int N = 1000;       const int spatial = 10;       const int layers = 5;       const int batch = 2;             float *x_host = (float *)malloc(N * sizeof(float));     float *out_host = (float *)malloc(N * sizeof(float));                      float *x_device, *out_device;     cudaMalloc((void **)&x_device, N * sizeof(float));     cudaMalloc((void **)&out_device, N * sizeof(float));           cudaMemcpy(x_device, x_host, N * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(256);       dim3 gridSize((N + blockSize.x - 1) / blockSize.x);           flatten_kernel<<<gridSize, blockSize>>>(N, x_device, spatial, layers, batch, 1, out_device);           cudaDeviceSynchronize();           cudaMemcpy(out_host, out_device, N * sizeof(float), cudaMemcpyDeviceToHost);                 cudaFree(x_device);     cudaFree(out_device);     free(x_host);     free(out_host);      return 0; }   "
    },
    {
        "id": "330",
        "c_code": "#include <stdio.h> #include <math.h>  void Softmax(float *x, const int r, const int c);  int main() {          const int rows = 3;     const int cols = 4;           float array[rows][cols] = {         {1.0, 2.0, 3.0, 4.0},         {5.0, 6.0, 7.0, 8.0},         {9.0, 10.0, 11.0, 12.0}     };           float flatArray[rows * cols];     for (int i = 0; i < rows; ++i) {         for (int j = 0; j < cols; ++j) {             flatArray[i * cols + j] = array[i][j];         }     }           Softmax(flatArray, rows, cols);           printf(\"Softmax Result:\\n\");     for (int i = 0; i < rows; ++i) {         for (int j = 0; j < cols; ++j) {             printf(\"%8.4f \", flatArray[i * cols + j]);         }         printf(\"\\n\");     }      return 0; }  void Softmax(float *x, const int r, const int c) {     float temp1, temp2;     for (int i = 0; i < r; i++) {         temp1 = 0.;         temp2 = 0.;         for (int j = 0; j < c; j++) {             temp1 = fmaxf(x[i * c + j], temp1);         }         for (int j = 0; j < c; j++) {             x[i * c + j] = expf(x[i * c + j] - temp1);             temp2 += x[i * c + j];         }         for (int j = 0; j < c; j++) {             x[i * c + j] /= temp2;         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h>  __global__ void Kernel_Softmax(float *dev_x, const int r, const int c) {     unsigned int i = blockDim.x * blockIdx.x + threadIdx.x;     if (i >= r)         return;      float temp1 = 0., temp2 = 0.;     for (int j = 0; j < c; j++)         temp1 = fmaxf(dev_x[i * c + j], temp1);      for (int j = 0; j < c; j++) {         dev_x[i * c + j] = expf(dev_x[i * c + j] - temp1);         temp2 += dev_x[i * c + j];     }      for (int j = 0; j < c; j++)         dev_x[i * c + j] /= temp2; }  int main() {          const int r = 100;       const int c = 10;              float *dev_x_host = (float *)malloc(r * c * sizeof(float));                      float *dev_x_device;     cudaMalloc((void **)&dev_x_device, r * c * sizeof(float));           cudaMemcpy(dev_x_device, dev_x_host, r * c * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(256);       dim3 gridSize((r + blockSize.x - 1) / blockSize.x);           Kernel_Softmax<<<gridSize, blockSize>>>(dev_x_device, r, c);           cudaDeviceSynchronize();           cudaMemcpy(dev_x_host, dev_x_device, r * c * sizeof(float), cudaMemcpyDeviceToHost);                 cudaFree(dev_x_device);     free(dev_x_host);      return 0; }   "
    },
    {
        "id": "331",
        "c_code": "#include <stdio.h>  void deInterleave_cpu2(float *d_X_out, float *d_Y_out, char *d_XY_in, int pitch_out, int pitch_in, int width, int height);  int main() {          int width = 3;     int height = 2;           char input[2][3 * 2 * sizeof(float)] = {         {1.0, 2.0, 3.0, 4.0, 5.0, 6.0},         {7.0, 8.0, 9.0, 10.0, 11.0, 12.0}     };           float output_X[2][3];     float output_Y[2][3];           deInterleave_cpu2((float *)output_X, (float *)output_Y, (char *)input, sizeof(float) * 3, sizeof(char) * 2 * 3 * sizeof(float), width, height);           printf(\"Output X:\\n\");     for (int y = 0; y < height; ++y) {         for (int x = 0; x < width; ++x) {             printf(\"%8.4f \", output_X[y][x]);         }         printf(\"\\n\");     }      printf(\"\\nOutput Y:\\n\");     for (int y = 0; y < height; ++y) {         for (int x = 0; x < width; ++x) {             printf(\"%8.4f \", output_Y[y][x]);         }         printf(\"\\n\");     }      return 0; }  void deInterleave_cpu2(float *d_X_out, float *d_Y_out, char *d_XY_in, int pitch_out, int pitch_in, int width, int height) {     for (int x = 0; x < width; x++) {         for (int y = 0; y < height; y++) {             float *data = (float *)(d_XY_in + y * pitch_in) + 2 * x;             *((float *)((char *)d_X_out + y * pitch_out) + x) = data[0];             *((float *)((char *)d_Y_out + y * pitch_out) + x) = data[1];         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h>  __global__ void deInterleave_kernel2(float *d_X_out, float *d_Y_out, char *d_XY_in, int pitch_out, int pitch_in, int width, int height) {     unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;     unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;      if ((x < width) & (y < height)) {         float *data = (float *)(d_XY_in + y * pitch_in) + 2 * x;         *((float *)((char *)d_X_out + y * pitch_out) + x) = data[0];         *((float *)((char *)d_Y_out + y * pitch_out) + x) = data[1];     } }  int main() {          const int width = 100;       const int height = 100;            float *d_X_out_host = (float *)malloc(width * height * sizeof(float));     float *d_Y_out_host = (float *)malloc(width * height * sizeof(float));     char *d_XY_in_host = (char *)malloc(2 * width * height * sizeof(float));                       float *d_X_out_device, *d_Y_out_device;     char *d_XY_in_device;      cudaMalloc((void **)&d_X_out_device, width * height * sizeof(float));     cudaMalloc((void **)&d_Y_out_device, width * height * sizeof(float));     cudaMalloc((void **)&d_XY_in_device, 2 * width * height * sizeof(float));           cudaMemcpy(d_X_out_device, d_X_out_host, width * height * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_Y_out_device, d_Y_out_host, width * height * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_XY_in_device, d_XY_in_host, 2 * width * height * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(16, 16);      dim3 gridSize((width + blockSize.x - 1) / blockSize.x, (height + blockSize.y - 1) / blockSize.y);           deInterleave_kernel2<<<gridSize, blockSize>>>(d_X_out_device, d_Y_out_device, d_XY_in_device, 2 * width, 2 * width, width, height);           cudaDeviceSynchronize();           cudaMemcpy(d_X_out_host, d_X_out_device, width * height * sizeof(float), cudaMemcpyDeviceToHost);     cudaMemcpy(d_Y_out_host, d_Y_out_device, width * height * sizeof(float), cudaMemcpyDeviceToHost);                 cudaFree(d_X_out_device);     cudaFree(d_Y_out_device);     cudaFree(d_XY_in_device);     free(d_X_out_host);     free(d_Y_out_host);     free(d_XY_in_host);      return 0; }   "
    },
    {
        "id": "332",
        "c_code": "#include <stdio.h>  void cpu_laplace_filter(float *Img, float *laplace, float _dz, float _dx, int npml, int nnz, int nnx);  int main() {          int npml = 2;     int nnz = 5;     int nnx = 4;           float Img[20] = {         1.0, 2.0, 3.0, 4.0,         5.0, 6.0, 7.0, 8.0,         9.0, 10.0, 11.0, 12.0,         13.0, 14.0, 15.0, 16.0,         17.0, 18.0, 19.0, 20.0     };           float laplace_result[20];           cpu_laplace_filter(Img, laplace_result, 0.1, 0.2, npml, nnz, nnx);           printf(\"Input Img:\\n\");     for (int i = 0; i < nnz; ++i) {         for (int j = 0; j < nnx; ++j) {             printf(\"%8.4f \", Img[i + j * nnz]);         }         printf(\"\\n\");     }      printf(\"\\nOutput Laplace:\\n\");     for (int i = 0; i < nnz; ++i) {         for (int j = 0; j < nnx; ++j) {             printf(\"%8.4f \", laplace_result[i + j * nnz]);         }         printf(\"\\n\");     }      return 0; }  void cpu_laplace_filter(float *Img, float *laplace, float _dz, float _dx, int npml, int nnz, int nnx) {     for (int i1 = npml; i1 < nnz - npml; i1++) {         for (int i2 = npml; i2 < nnx - npml; i2++) {             int id = i1 + i2 * nnz;             float diff1 = 0.0f;             float diff2 = 0.0f;             diff1 = Img[id + 1] - 2.0 * Img[id] + Img[id - 1];             diff2 = Img[id + nnz] - 2.0 * Img[id] + Img[id - nnz];             laplace[id] = _dz * _dz * diff1 + _dx * _dx * diff2;         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h>  __global__ void cuda_laplace_filter(float *Img, float *laplace, float _dz, float _dx, int npml, int nnz, int nnx) {     int i1 = threadIdx.x + blockDim.x * blockIdx.x;     int i2 = threadIdx.y + blockDim.y * blockIdx.y;     int id = i1 + i2 * nnz;     float diff1 = 0.0f;     float diff2 = 0.0f;      if (i1 >= npml && i1 < nnz - npml && i2 >= npml && i2 < nnx - npml) {         diff1 = Img[id + 1] - 2.0 * Img[id] + Img[id - 1];         diff2 = Img[id + nnz] - 2.0 * Img[id] + Img[id - nnz];     }      laplace[id] = _dz * _dz * diff1 + _dx * _dx * diff2; }  int main() {          const int nnz = 100;       const int nnx = 100;       const int npml = 5;              float *Img_host = (float *)malloc(nnz * nnx * sizeof(float));     float *laplace_host = (float *)malloc(nnz * nnx * sizeof(float));                      float *Img_device, *laplace_device;      cudaMalloc((void **)&Img_device, nnz * nnx * sizeof(float));     cudaMalloc((void **)&laplace_device, nnz * nnx * sizeof(float));           cudaMemcpy(Img_device, Img_host, nnz * nnx * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(16, 16);      dim3 gridSize((nnz + blockSize.x - 1) / blockSize.x, (nnx + blockSize.y - 1) / blockSize.y);           cuda_laplace_filter<<<gridSize, blockSize>>>(Img_device, laplace_device, 1.0, 1.0, npml, nnz, nnx);           cudaDeviceSynchronize();           cudaMemcpy(laplace_host, laplace_device, nnz * nnx * sizeof(float), cudaMemcpyDeviceToHost);                 cudaFree(Img_device);     cudaFree(laplace_device);     free(Img_host);     free(laplace_host);      return 0; }   "
    },
    {
        "id": "333",
        "c_code": "#include <stdio.h>  void distanceMatFinal(long int totalPixels, int availablePixels, int outPixelOffset, float *distMat);  int main() {          long int totalPixels = 4;     int availablePixels = 3;     int outPixelOffset = 1;           float distMat[12] = {         1.0, 2.0, 3.0, 4.0,         5.0, 6.0, 7.0, 8.0,         9.0, 10.0, 11.0, 12.0     };           distanceMatFinal(totalPixels, availablePixels, outPixelOffset, distMat);           printf(\"Output distMat:\\n\");     for (long int i = 0; i < availablePixels; ++i) {         for (long int j = 0; j < totalPixels; ++j) {             printf(\"%8.4f \", distMat[i * totalPixels + j]);         }         printf(\"\\n\");     }      return 0; }  void distanceMatFinal(long int totalPixels, int availablePixels, int outPixelOffset, float *distMat) {     for (long int i = 0; i < availablePixels; i++) {         float sum = 0.0;         float max = 0.0;                   for (long int j = 0; j < totalPixels; j++) {             float element = distMat[i * totalPixels + j];             if (element > max) max = element;             sum += element;         }                   sum += max;          for (long int j = 0; j < totalPixels; j++) {             if ((i + outPixelOffset) == j)                 distMat[i * totalPixels + j] = max / sum;             else                 distMat[i * totalPixels + j] /= sum;         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h>  __global__ void distanceMatFinal(long int totalPixels, int availablePixels, int outPixelOffset, float *distMat) {     int index = blockIdx.x * blockDim.x + threadIdx.x;     int stride = blockDim.x * gridDim.x;      for (long int i = index; i < availablePixels; i += stride) {         float sum = 0.0;         float max = 0.0;          for (long int j = 0; j < totalPixels; j++) {             float element = distMat[i * totalPixels + j];             if (element > max) max = element;             sum += element;         }          sum += max;          for (long int j = 0; j < totalPixels; j++) {             if ((i + outPixelOffset) == j)                 distMat[i * totalPixels + j] = max / sum;             else                 distMat[i * totalPixels + j] /= sum;         }     } }  int main() {          const long int totalPixels = 100;        const int availablePixels = 50;          const int outPixelOffset = 10;                 float *distMat_host = (float *)malloc(totalPixels * availablePixels * sizeof(float));                      float *distMat_device;     cudaMalloc((void **)&distMat_device, totalPixels * availablePixels * sizeof(float));           cudaMemcpy(distMat_device, distMat_host, totalPixels * availablePixels * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(256);       dim3 gridSize((availablePixels + blockSize.x - 1) / blockSize.x);           distanceMatFinal<<<gridSize, blockSize>>>(totalPixels, availablePixels, outPixelOffset, distMat_device);           cudaDeviceSynchronize();           cudaMemcpy(distMat_host, distMat_device, totalPixels * availablePixels * sizeof(float), cudaMemcpyDeviceToHost);                 cudaFree(distMat_device);     free(distMat_host);      return 0; }   "
    },
    {
        "id": "334",
        "c_code": "#include <stdio.h>  void permuteData2_cpu(const float *input, float *output, int num, int devideNum, int featureSize, int priorNum, int batchSize);  int main() {          int num = 2;     int devideNum = 3;     int featureSize = 4;     int priorNum = 2;     int batchSize = 2;           float input[48] = {                  1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8,         2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8,         3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8,                  4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8,         5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 5.7, 5.8,         6.1, 6.2, 6.3, 6.4, 6.5, 6.6, 6.7, 6.8,     };           float output[48];           permuteData2_cpu(input, output, num, devideNum, featureSize, priorNum, batchSize);           printf(\"Output data:\\n\");     for (int s = 0; s < batchSize; ++s) {         for (int i = 0; i < priorNum; ++i) {             for (int j = 0; j < devideNum; ++j) {                 for (int tid = 0; tid < num; ++tid) {                     printf(\"%8.4f \", output[s * num * priorNum * devideNum + tid * priorNum * devideNum + i * devideNum + j]);                 }             }         }         printf(\"\\n\");     }      return 0; }  void permuteData2_cpu(const float *input, float *output, int num, int devideNum, int featureSize, int priorNum, int batchSize) {     for (int tid = 0; tid < num; tid++) {         int numPerbatch = num * devideNum * priorNum;         for (int s = 0; s < batchSize; s++) {             for (int i = 0; i < priorNum; i++) {                 for (int j = 0; j < devideNum; j++) {                     output[s * numPerbatch + tid * priorNum * devideNum + i * devideNum + j] =                         input[s * numPerbatch + (i * devideNum * featureSize) + (j * featureSize) + tid];                 }             }         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h>  __global__ void permuteData2(const float *input, float *output, int num, int devideNum, int featureSize, int priorNum, int batchSize) {     int tid = blockIdx.x * blockDim.x + threadIdx.x;      if (tid >= num) {         return;     }      int numPerBatch = num * devideNum * priorNum;      for (int s = 0; s < batchSize; s++) {         for (int i = 0; i < priorNum; i++) {             for (int j = 0; j < devideNum; j++) {                 output[s * numPerBatch + tid * priorNum * devideNum + i * devideNum + j] =                     input[s * numPerBatch + (i * devideNum * featureSize) + (j * featureSize) + tid];             }         }     } }  int main() {          const int num = 100;               const int devideNum = 5;           const int featureSize = 10;        const int priorNum = 3;            const int batchSize = 2;                 float *input_host = (float *)malloc(num * devideNum * priorNum * featureSize * batchSize * sizeof(float));     float *output_host = (float *)malloc(num * devideNum * priorNum * batchSize * sizeof(float));                      float *input_device, *output_device;     cudaMalloc((void **)&input_device, num * devideNum * priorNum * featureSize * batchSize * sizeof(float));     cudaMalloc((void **)&output_device, num * devideNum * priorNum * batchSize * sizeof(float));           cudaMemcpy(input_device, input_host, num * devideNum * priorNum * featureSize * batchSize * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(256);       dim3 gridSize((num + blockSize.x - 1) / blockSize.x);           permuteData2<<<gridSize, blockSize>>>(input_device, output_device, num, devideNum, featureSize, priorNum, batchSize);           cudaDeviceSynchronize();           cudaMemcpy(output_host, output_device, num * devideNum * priorNum * batchSize * sizeof(float), cudaMemcpyDeviceToHost);                 cudaFree(input_device);     cudaFree(output_device);     free(input_host);     free(output_host);      return 0; }   "
    },
    {
        "id": "335",
        "c_code": "#include <stdio.h>  void permuteDataTorch_cpu(const float *input, float *output, int num, int devideNum, int featureSize, int priorNum, int batchSize);  int main() {          int num = 2;     int devideNum = 3;     int featureSize = 4;     int priorNum = 2;     int batchSize = 2;           float input[48] = {                  1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8,         2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8,         3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8,                  4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8,         5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 5.7, 5.8,         6.1, 6.2, 6.3, 6.4, 6.5, 6.6, 6.7, 6.8,     };           float output[48];           permuteDataTorch_cpu(input, output, num, devideNum, featureSize, priorNum, batchSize);           printf(\"Output data:\\n\");     for (int s = 0; s < batchSize; ++s) {         for (int i = 0; i < priorNum; ++i) {             for (int j = 0; j < devideNum; ++j) {                 for (int tid = 0; tid < num; ++tid) {                     printf(\"%8.4f \", output[s * num * priorNum * devideNum + tid * priorNum * devideNum + i * devideNum + j]);                 }             }         }         printf(\"\\n\");     }      return 0; }  void permuteDataTorch_cpu(const float *input, float *output, int num, int devideNum, int featureSize, int priorNum, int batchSize) {     for (int tid = 0; tid < num; tid++) {         int numPerbatch = num * devideNum * priorNum;         for (int s = 0; s < batchSize; s++) {             for (int i = 0; i < priorNum; i++) {                 for (int j = 0; j < devideNum; j++) {                     output[s * numPerbatch + tid * priorNum * devideNum + i * devideNum + j] =                         input[s * numPerbatch + (i * devideNum * featureSize) + (j * featureSize) + tid];                 }             }         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <cuda_runtime.h>  __global__ void permuteDataTorch(const float *input, float *output, int num, int devideNum, int featureSize, int priorNum, int batchSize) {     int tid = blockIdx.x * blockDim.x + threadIdx.x;      if (tid >= num) {         return;     }      int numPerBatch = num * devideNum * priorNum;      for (int s = 0; s < batchSize; s++) {         for (int i = 0; i < priorNum; i++) {             for (int j = 0; j < devideNum; j++) {                 output[s * numPerBatch + tid * priorNum * devideNum + i * devideNum + j] =                     input[s * numPerBatch + (i * devideNum * featureSize) + (j * featureSize) + tid];             }         }     } }  int main() {          const int num = 100;               const int devideNum = 5;           const int featureSize = 10;        const int priorNum = 3;            const int batchSize = 2;                 float *input_host = (float *)malloc(num * devideNum * priorNum * featureSize * batchSize * sizeof(float));     float *output_host = (float *)malloc(num * devideNum * priorNum * batchSize * sizeof(float));                      float *input_device, *output_device;     cudaMalloc((void **)&input_device, num * devideNum * priorNum * featureSize * batchSize * sizeof(float));     cudaMalloc((void **)&output_device, num * devideNum * priorNum * batchSize * sizeof(float));           cudaMemcpy(input_device, input_host, num * devideNum * priorNum * featureSize * batchSize * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(256);       dim3 gridSize((num + blockSize.x - 1) / blockSize.x);           permuteDataTorch<<<gridSize, blockSize>>>(input_device, output_device, num, devideNum, featureSize, priorNum, batchSize);           cudaDeviceSynchronize();           cudaMemcpy(output_host, output_device, num * devideNum * priorNum * batchSize * sizeof(float), cudaMemcpyDeviceToHost);                 cudaFree(input_device);     cudaFree(output_device);     free(input_host);     free(output_host);      return 0; }   "
    },
    {
        "id": "336",
        "c_code": "#include <stdio.h> #include <stdlib.h>  void conv1x1_cpu(int input_channels, int input_size, int n, float *input_im, float *filter_weight, float *filter_bias, float *output_im) {     for (int filter_index = 0; filter_index < n; filter_index++) {         filter_weight += filter_index * input_channels;         float bias = filter_bias[filter_index];         output_im += filter_index * input_size * input_size;          for (int i = 0; i < input_size; i++) {             for (int j = 0; j < input_size; j++) {                 float tmp = bias;                 for (int k = 0; k < input_channels; k++) {                     tmp += input_im[k * input_size * input_size + i * input_size + j] * filter_weight[k];                 }                 output_im[i * input_size + j] = (tmp > 0.0) ? tmp : 0.0;             }         }     } }  int main() {          int input_channels = 3;     int input_size = 4;     int n = 2;           float input_im[input_channels * input_size * input_size];     float filter_weight[input_channels * n];     float filter_bias[n];     float output_im[input_size * input_size * n];           for (int i = 0; i < input_channels * input_size * input_size; i++) {         input_im[i] = i + 1;       }      for (int i = 0; i < input_channels * n; i++) {         filter_weight[i] = 0.5;       }      for (int i = 0; i < n; i++) {         filter_bias[i] = 0.1;       }           conv1x1_cpu(input_channels, input_size, n, input_im, filter_weight, filter_bias, output_im);           printf(\"Output Image:\\n\");     for (int i = 0; i < n; i++) {         printf(\"Channel %d:\\n\", i + 1);         for (int j = 0; j < input_size; j++) {             for (int k = 0; k < input_size; k++) {                 printf(\"%f \", output_im[i * input_size * input_size + j * input_size + k]);             }             printf(\"\\n\");         }         printf(\"\\n\");     }      return 0; }   ",
        "cuda_code": " #include <iostream>   __global__ void conv1x1(int input_channels, int input_size, int n, float *input_im, float *filter_weight, float *filter_bias, float *output_im) {     int filter_index = blockIdx.x * blockDim.x + threadIdx.x;          if (filter_index < n) {         filter_weight += filter_index * input_channels;         float bias = filter_bias[filter_index];         output_im += filter_index * input_size * input_size;          for (int i = 0; i < input_size; i++) {             for (int j = 0; j < input_size; j++) {                 float tmp = bias;                  for (int k = 0; k < input_channels; k++) {                     tmp += input_im[k * input_size * input_size + i * input_size + j] * filter_weight[k];                 }                  output_im[i * input_size + j] = (tmp > 0.0) ? tmp : 0.0;             }         }     } }  int main() {          int input_channels = 3;     int input_size = 5;     int n = 64;       int total_elements = n * input_size * input_size;           float *d_input_im, *d_filter_weight, *d_filter_bias, *d_output_im;     cudaMalloc((void**)&d_input_im, input_channels * input_size * input_size * sizeof(float));     cudaMalloc((void**)&d_filter_weight, n * input_channels * sizeof(float));     cudaMalloc((void**)&d_filter_bias, n * sizeof(float));     cudaMalloc((void**)&d_output_im, total_elements * sizeof(float));           float *h_input_im = new float[input_channels * input_size * input_size];     float *h_filter_weight = new float[n * input_channels];     float *h_filter_bias = new float[n];     float *h_output_im = new float[total_elements];           cudaMemcpy(d_input_im, h_input_im, input_channels * input_size * input_size * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_filter_weight, h_filter_weight, n * input_channels * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_filter_bias, h_filter_bias, n * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(256);     dim3 gridSize((n + blockSize.x - 1) / blockSize.x);           conv1x1<<<gridSize, blockSize>>>(input_channels, input_size, n, d_input_im, d_filter_weight, d_filter_bias, d_output_im);           cudaMemcpy(h_output_im, d_output_im, total_elements * sizeof(float), cudaMemcpyDeviceToHost);           delete[] h_input_im;     delete[] h_filter_weight;     delete[] h_filter_bias;     delete[] h_output_im;      cudaFree(d_input_im);     cudaFree(d_filter_weight);     cudaFree(d_filter_bias);     cudaFree(d_output_im);      return 0; }  "
    },
    {
        "id": "337",
        "c_code": "#include <stdio.h> #include <stdlib.h> #include <math.h>  void Softmax_seg(float *x, const int size_category, const int size_spatial_feature_map) {     int c = size_category;     int size = size_spatial_feature_map;     float temp1, temp2;      for (int i = 0; i < size; i++) {         temp1 = 0.;         temp2 = 0.;          for (int j = 0; j < c; j++) {             temp1 = fmaxf(x[j * size + i], temp1);         }          for (int j = 0; j < c; j++) {             x[j * size + i] = expf(x[j * size + i] - temp1);             temp2 += x[j * size + i];         }          for (int j = 0; j < c; j++) {             x[j * size + i] /= temp2;         }     } }  int main() {          int size_category = 3;     int size_spatial_feature_map = 4;     float input[] = {1.0, 2.0, 3.0, 2.0, 1.0, 3.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0};     float output[size_category * size_spatial_feature_map];      printf(\"Input:\\n\");     for (int i = 0; i < size_category * size_spatial_feature_map; i++) {         printf(\"%.2f \", input[i]);     }     printf(\"\\n\");      Softmax_seg(input, size_category, size_spatial_feature_map);      printf(\"Output:\\n\");     for (int i = 0; i < size_category * size_spatial_feature_map; i++) {         printf(\"%.4f \", input[i]);     }     printf(\"\\n\");      return 0; }   ",
        "cuda_code": "#include <iostream> #include <cmath>   __global__ void Kernel_Softmax_seg(float *dev_x, const int c, const int size) {     unsigned int i = blockDim.x * blockIdx.x + threadIdx.x;     int N = size;      while (i < N) {         float temp = 0.0;                   for (int j = 0; j < c; j++) {             temp = fmaxf(dev_x[j * size + i], temp);         }                   for (int j = 0; j < c; j++) {             dev_x[j * size + i] = expf(dev_x[j * size + i] - temp);         }                   temp = 0.0;         for (int j = 0; j < c; j++) {             temp += dev_x[j * size + i];         }                   for (int j = 0; j < c; j++) {             dev_x[j * size + i] /= temp;         }          i += gridDim.x * blockDim.x;     } }  int main() {          int c = 5;       int size = 1000;       int total_elements = c * size;           float *d_dev_x;     cudaMalloc((void**)&d_dev_x, total_elements * sizeof(float));           float *h_dev_x = new float[total_elements];           cudaMemcpy(d_dev_x, h_dev_x, total_elements * sizeof(float), cudaMemcpyHostToDevice);           dim3 blockSize(256);     dim3 gridSize((size + blockSize.x - 1) / blockSize.x);           Kernel_Softmax_seg<<<gridSize, blockSize>>>(d_dev_x, c, size);           cudaMemcpy(h_dev_x, d_dev_x, total_elements * sizeof(float), cudaMemcpyDeviceToHost);           delete[] h_dev_x;     cudaFree(d_dev_x);      return 0; }   "
    },
    {
        "id": "338",
        "c_code": "#include <stdio.h>  void pad_input(float *f_in, float *f_out, int H, int W, int D, int pad) {     int col, row, dep;     int new_H = H + 2 * pad;     int new_W = W + 2 * pad;      for (col = 0; col < new_H; col++) {         for (row = 0; row < new_W; row++) {             for (dep = 0; dep < D; dep++) {                 int i = dep * new_H * new_W + col * new_W + row;                 int j = dep * H * W + (col - pad) * W + (row - pad);                  if ((col < pad || col > H + pad - 1) || (row < pad || row > W + pad - 1))                     f_out[i] = 0;                 else                     f_out[i] = f_in[j];             }         }     } }  int main() {          int H = 4;     int W = 4;     int D = 2;     int pad = 1;      float input[H * W * D];     float output[(H + 2 * pad) * (W + 2 * pad) * D];           for (int i = 0; i < H * W * D; i++) {         input[i] = i + 1;       }      printf(\"Input:\\n\");     for (int i = 0; i < H * W * D; i++) {         printf(\"%.2f \", input[i]);     }     printf(\"\\n\");           pad_input(input, output, H, W, D, pad);      printf(\"Output:\\n\");     for (int i = 0; i < (H + 2 * pad) * (W + 2 * pad) * D; i++) {         printf(\"%.2f \", output[i]);     }     printf(\"\\n\");      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h>  #define BLOCK_SIZE 16  __global__ void pad_input(float *f_in, float *f_out, int H, int W, int D, int pad) {     int col = blockIdx.x * blockDim.x + threadIdx.x;     int row = blockIdx.y * blockDim.y + threadIdx.y;     int dep = blockIdx.z * blockDim.z + threadIdx.z;      int new_H = H + 2 * pad;     int new_W = W + 2 * pad;      int i = dep * new_H * new_W + col * new_W + row;     int j = dep * H * W + (col - pad) * W + (row - pad);      if (col < new_H && row < new_W && dep < D) {         if ((col < pad || col > H + pad - 1) || (row < pad || row > W + pad - 1)) {             f_out[i] = 0;         } else {             f_out[i] = f_in[j];         }     } }  int main() {     const int H = 128;     const int W = 128;     const int D = 64;     const int pad = 2;      float *h_input, *h_output;     float *d_input, *d_output;      size_t input_size = H * W * D * sizeof(float);     size_t output_size = (H + 2 * pad) * (W + 2 * pad) * D * sizeof(float);           h_input = (float *)malloc(input_size);     h_output = (float *)malloc(output_size);           for (int i = 0; i < H * W * D; ++i) {         h_input[i] = (float)i;     }           cudaMalloc((void **)&d_input, input_size);     cudaMalloc((void **)&d_output, output_size);           cudaMemcpy(d_input, h_input, input_size, cudaMemcpyHostToDevice);           dim3 block_size(BLOCK_SIZE, BLOCK_SIZE, BLOCK_SIZE);     dim3 grid_size((H + block_size.x - 1) / block_size.x, (W + block_size.y - 1) / block_size.y, (D + block_size.z - 1) / block_size.z);           pad_input<<<grid_size, block_size>>>(d_input, d_output, H, W, D, pad);           cudaMemcpy(h_output, d_output, output_size, cudaMemcpyDeviceToHost);           cudaFree(d_input);     cudaFree(d_output);     free(h_input);     free(h_output);      return 0; }   "
    },
    {
        "id": "339",
        "c_code": "#include <stdio.h>  void waterDepthToElevation_cpu(const int nx_, const int ny_, float *w_ptr_, int w_pitch_,                                float *h_ptr_, int h_pitch_, float *Bm_ptr_, int Bm_pitch_) {     for (int ti = 0; ti < nx_; ti++) {         for (int tj = 0; tj < ny_; tj++) {             float *h_row = (float *)((char *)h_ptr_ + h_pitch_ * tj);             float *Bm_row = (float *)((char *)Bm_ptr_ + Bm_pitch_ * tj);             float *w_row = (float *)((char *)w_ptr_ + w_pitch_ * tj);             w_row[ti] = h_row[ti] + Bm_row[ti];         }     } }  int main() {          int nx = 4;     int ny = 4;      float waterDepth[nx * ny];     float elevation[nx * ny];     float bedrockElevation[nx * ny];           for (int i = 0; i < nx * ny; i++) {         waterDepth[i] = i + 1;           bedrockElevation[i] = 2 * i + 1;       }      printf(\"Water Depth:\\n\");     for (int i = 0; i < nx * ny; i++) {         printf(\"%.2f \", waterDepth[i]);     }     printf(\"\\n\");      printf(\"Bedrock Elevation:\\n\");     for (int i = 0; i < nx * ny; i++) {         printf(\"%.2f \", bedrockElevation[i]);     }     printf(\"\\n\");           waterDepthToElevation_cpu(nx, ny, waterDepth, nx, elevation, nx, bedrockElevation, nx);      printf(\"Elevation:\\n\");     for (int i = 0; i < nx * ny; i++) {         printf(\"%.2f \", elevation[i]);     }     printf(\"\\n\");      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h>  #define BLOCK_SIZE 16  __global__ void waterDepthToElevation(const int nx_, const int ny_, float *w_ptr_, int w_pitch_, float *h_ptr_, int h_pitch_, float *Bm_ptr_, int Bm_pitch_) {     const int ti = blockIdx.x * blockDim.x + threadIdx.x;     const int tj = blockIdx.y * blockDim.y + threadIdx.y;      if (ti < nx_ && tj < ny_) {         float *const h_row = (float *)((char *)h_ptr_ + h_pitch_ * tj);         float *const Bm_row = (float *)((char *)Bm_ptr_ + Bm_pitch_ * tj);         float *const w_row = (float *)((char *)w_ptr_ + w_pitch_ * tj);          w_row[ti] = h_row[ti] + Bm_row[ti];     } }  int main() {     const int nx = 128;     const int ny = 64;      float *h_ptr, *Bm_ptr, *w_ptr;     int h_pitch, Bm_pitch, w_pitch;      size_t h_pitch_bytes = nx * sizeof(float);     size_t Bm_pitch_bytes = nx * sizeof(float);     size_t w_pitch_bytes = nx * sizeof(float);           h_ptr = (float *)malloc(ny * h_pitch_bytes);     Bm_ptr = (float *)malloc(ny * Bm_pitch_bytes);     w_ptr = (float *)malloc(ny * w_pitch_bytes);           for (int i = 0; i < nx * ny; ++i) {         h_ptr[i] = static_cast<float>(i);         Bm_ptr[i] = static_cast<float>(i * 2);     }           cudaMallocPitch((void **)&h_ptr, &h_pitch, nx * sizeof(float), ny);     cudaMallocPitch((void **)&Bm_ptr, &Bm_pitch, nx * sizeof(float), ny);     cudaMallocPitch((void **)&w_ptr, &w_pitch, nx * sizeof(float), ny);           cudaMemcpy2D(h_ptr, h_pitch, h_ptr, h_pitch_bytes, nx * sizeof(float), ny, cudaMemcpyHostToDevice);     cudaMemcpy2D(Bm_ptr, Bm_pitch, Bm_ptr, Bm_pitch_bytes, nx * sizeof(float), ny, cudaMemcpyHostToDevice);           dim3 block_size(BLOCK_SIZE, BLOCK_SIZE);     dim3 grid_size((nx + block_size.x - 1) / block_size.x, (ny + block_size.y - 1) / block_size.y);           waterDepthToElevation<<<grid_size, block_size>>>(nx, ny, w_ptr, w_pitch / sizeof(float), h_ptr, h_pitch / sizeof(float), Bm_ptr, Bm_pitch / sizeof(float));           cudaMemcpy2D(w_ptr, w_pitch_bytes, w_ptr, w_pitch, nx * sizeof(float), ny, cudaMemcpyDeviceToHost);           cudaFree(h_ptr);     cudaFree(Bm_ptr);     cudaFree(w_ptr);     free(h_ptr);     free(Bm_ptr);     free(w_ptr);      return 0; }   "
    },
    {
        "id": "340",
        "c_code": "#include <stdio.h>  void invalidateFlow_cpu(float *modFlowX, float *modFlowY, const float *constFlowX,                          const float *constFlowY, int width, int height, float cons_thres) {     for (int x = 0; x < width; x++) {         for (int y = 0; y < height; y++) {             int ind = y * width + x;             float mFX = modFlowX[ind];             float mFY = modFlowY[ind];             float cFX = constFlowX[ind];             float cFY = constFlowY[ind];             float err = (mFX - cFX) * (mFX - cFX) + (mFY - cFY) * (mFY - cFY);             if (err > cons_thres) {                 mFX = 0;                 mFY = 0;             }             modFlowX[ind] = mFX;             modFlowY[ind] = mFY;         }     } }  int main() {          int width = 4;     int height = 4;      float modFlowX[width * height];     float modFlowY[width * height];     float constFlowX[width * height];     float constFlowY[width * height];           for (int i = 0; i < width * height; i++) {         modFlowX[i] = i + 1;           modFlowY[i] = 2 * i + 1;           constFlowX[i] = 3 * i + 1;           constFlowY[i] = 4 * i + 1;       }      printf(\"Modified Flow X:\\n\");     for (int i = 0; i < width * height; i++) {         printf(\"%.2f \", modFlowX[i]);     }     printf(\"\\n\");      printf(\"Modified Flow Y:\\n\");     for (int i = 0; i < width * height; i++) {         printf(\"%.2f \", modFlowY[i]);     }     printf(\"\\n\");      printf(\"Constant Flow X:\\n\");     for (int i = 0; i < width * height; i++) {         printf(\"%.2f \", constFlowX[i]);     }     printf(\"\\n\");      printf(\"Constant Flow Y:\\n\");     for (int i = 0; i < width * height; i++) {         printf(\"%.2f \", constFlowY[i]);     }     printf(\"\\n\");           invalidateFlow_cpu(modFlowX, modFlowY, constFlowX, constFlowY, width, height, 10.0);      printf(\"Invalidated Flow X:\\n\");     for (int i = 0; i < width * height; i++) {         printf(\"%.2f \", modFlowX[i]);     }     printf(\"\\n\");      printf(\"Invalidated Flow Y:\\n\");     for (int i = 0; i < width * height; i++) {         printf(\"%.2f \", modFlowY[i]);     }     printf(\"\\n\");      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h>  #define BLOCK_SIZE 16  __global__ void invalidateFlow_kernel(float *modFlowX, float *modFlowY, const float *constFlowX, const float *constFlowY, int width, int height, float cons_thres) {     const int x = blockIdx.x * blockDim.x + threadIdx.x;     const int y = blockIdx.y * blockDim.y + threadIdx.y;      if (x < width && y < height) {         int ind = y * width + x;         float mFX = modFlowX[ind];         float mFY = modFlowY[ind];         float cFX = constFlowX[ind];         float cFY = constFlowY[ind];          float err = (mFX - cFX) * (mFX - cFX) + (mFY - cFY) * (mFY - cFY);          if (err > cons_thres) {             mFX = 0;             mFY = 0;         }          modFlowX[ind] = mFX;         modFlowY[ind] = mFY;     } }  int main() {     const int width = 128;     const int height = 64;     const float cons_thres = 0.1;       float *modFlowX, *modFlowY, *constFlowX, *constFlowY;      size_t flow_size = width * height * sizeof(float);           modFlowX = (float *)malloc(flow_size);     modFlowY = (float *)malloc(flow_size);     constFlowX = (float *)malloc(flow_size);     constFlowY = (float *)malloc(flow_size);           for (int i = 0; i < width * height; ++i) {         modFlowX[i] = static_cast<float>(i);         modFlowY[i] = static_cast<float>(i * 2);         constFlowX[i] = static_cast<float>(i * 3);         constFlowY[i] = static_cast<float>(i * 4);     }           float *d_modFlowX, *d_modFlowY, *d_constFlowX, *d_constFlowY;     cudaMalloc((void **)&d_modFlowX, flow_size);     cudaMalloc((void **)&d_modFlowY, flow_size);     cudaMalloc((void **)&d_constFlowX, flow_size);     cudaMalloc((void **)&d_constFlowY, flow_size);           cudaMemcpy(d_modFlowX, modFlowX, flow_size, cudaMemcpyHostToDevice);     cudaMemcpy(d_modFlowY, modFlowY, flow_size, cudaMemcpyHostToDevice);     cudaMemcpy(d_constFlowX, constFlowX, flow_size, cudaMemcpyHostToDevice);     cudaMemcpy(d_constFlowY, constFlowY, flow_size, cudaMemcpyHostToDevice);           dim3 block_size(BLOCK_SIZE, BLOCK_SIZE);     dim3 grid_size((width + block_size.x - 1) / block_size.x, (height + block_size.y - 1) / block_size.y);           invalidateFlow_kernel<<<grid_size, block_size>>>(d_modFlowX, d_modFlowY, d_constFlowX, d_constFlowY, width, height, cons_thres);           cudaMemcpy(modFlowX, d_modFlowX, flow_size, cudaMemcpyDeviceToHost);     cudaMemcpy(modFlowY, d_modFlowY, flow_size, cudaMemcpyDeviceToHost);           cudaFree(d_modFlowX);     cudaFree(d_modFlowY);     cudaFree(d_constFlowX);     cudaFree(d_constFlowY);     free(modFlowX);     free(modFlowY);     free(constFlowX);     free(constFlowY);      return 0; }   "
    },
    {
        "id": "341",
        "c_code": "#include <stdio.h>  void cpuRunComplexFilter(float *I, float *Q, int samplesLength, float *hr, float *hi,                           int filterLength, float *filtered_I, float *filtered_Q, int convLength) {     for (int sampleIndex = 0; sampleIndex < convLength; sampleIndex++) {         int index;         float sumI, sumQ;         sumI = 0;         sumQ = 0;         for (int j = sampleIndex - filterLength + 1; j <= sampleIndex; j++) {             index = sampleIndex - j;             if ((j < samplesLength) && (j >= 0)) {                 sumI += (I[j] * hr[index]) - (Q[j] * hi[index]);                 sumQ += (I[j] * hi[index]) + (Q[j] * hr[index]);             }         }         filtered_I[sampleIndex] = sumI;         filtered_Q[sampleIndex] = sumQ;     } }  int main() {          int samplesLength = 5;     int filterLength = 3;     int convLength = samplesLength + filterLength - 1;      float I[] = {1.0, 2.0, 3.0, 4.0, 5.0};     float Q[] = {0.1, 0.2, 0.3, 0.4, 0.5};     float hr[] = {0.5, 0.3, 0.1};     float hi[] = {0.2, 0.4, 0.6};     float filtered_I[convLength];     float filtered_Q[convLength];      cpuRunComplexFilter(I, Q, samplesLength, hr, hi, filterLength, filtered_I, filtered_Q, convLength);      printf(\"Filtered I:\\n\");     for (int i = 0; i < convLength; i++) {         printf(\"%.2f \", filtered_I[i]);     }     printf(\"\\n\");      printf(\"Filtered Q:\\n\");     for (int i = 0; i < convLength; i++) {         printf(\"%.2f \", filtered_Q[i]);     }     printf(\"\\n\");      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h>  #define BLOCK_SIZE 256  __global__ void cudaRunComplexFilter(float *I, float *Q, int samplesLength, float *hr, float *hi, int filterLength, float *filtered_I, float *filtered_Q, int convLength) {     int sampleIndex = (blockIdx.x * blockDim.x) + threadIdx.x;      if (sampleIndex >= convLength)         return;      int index;     float sumI, sumQ;      sumI = 0;     sumQ = 0;      for (int j = sampleIndex - filterLength + 1; j <= sampleIndex; j++) {         index = sampleIndex - j;          if ((j < samplesLength) && (j >= 0)) {             sumI += (I[j] * hr[index]) - (Q[j] * hi[index]);             sumQ += (I[j] * hi[index]) + (Q[j] * hr[index]);         }     }      filtered_I[sampleIndex] = sumI;     filtered_Q[sampleIndex] = sumQ; }  int main() {     const int samplesLength = 1024;     const int filterLength = 64;     const int convLength = samplesLength - filterLength + 1;      float *I, *Q, *hr, *hi, *filtered_I, *filtered_Q;           I = (float *)malloc(samplesLength * sizeof(float));     Q = (float *)malloc(samplesLength * sizeof(float));     hr = (float *)malloc(filterLength * sizeof(float));     hi = (float *)malloc(filterLength * sizeof(float));     filtered_I = (float *)malloc(convLength * sizeof(float));     filtered_Q = (float *)malloc(convLength * sizeof(float));           for (int i = 0; i < samplesLength; ++i) {         I[i] = static_cast<float>(i);         Q[i] = static_cast<float>(i * 2);     }      for (int i = 0; i < filterLength; ++i) {         hr[i] = static_cast<float>(i);         hi[i] = static_cast<float>(i * 2);     }           float *d_I, *d_Q, *d_hr, *d_hi, *d_filtered_I, *d_filtered_Q;     cudaMalloc((void **)&d_I, samplesLength * sizeof(float));     cudaMalloc((void **)&d_Q, samplesLength * sizeof(float));     cudaMalloc((void **)&d_hr, filterLength * sizeof(float));     cudaMalloc((void **)&d_hi, filterLength * sizeof(float));     cudaMalloc((void **)&d_filtered_I, convLength * sizeof(float));     cudaMalloc((void **)&d_filtered_Q, convLength * sizeof(float));           cudaMemcpy(d_I, I, samplesLength * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_Q, Q, samplesLength * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_hr, hr, filterLength * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_hi, hi, filterLength * sizeof(float), cudaMemcpyHostToDevice);           dim3 block_size(BLOCK_SIZE);     dim3 grid_size((convLength + block_size.x - 1) / block_size.x);           cudaRunComplexFilter<<<grid_size, block_size>>>(d_I, d_Q, samplesLength, d_hr, d_hi, filterLength, d_filtered_I, d_filtered_Q, convLength);           cudaMemcpy(filtered_I, d_filtered_I, convLength * sizeof(float), cudaMemcpyDeviceToHost);     cudaMemcpy(filtered_Q, d_filtered_Q, convLength * sizeof(float), cudaMemcpyDeviceToHost);           cudaFree(d_I);     cudaFree(d_Q);     cudaFree(d_hr);     cudaFree(d_hi);     cudaFree(d_filtered_I);     cudaFree(d_filtered_Q);     free(I);     free(Q);     free(hr);     free(hi);     free(filtered_I);     free(filtered_Q);      return 0; }   "
    },
    {
        "id": "342",
        "c_code": "#include <stdio.h>  void opL21_cpu(float *vec, float *vec1, long depth, long rows, long cols) {     for (int x = 0; x < cols; x++) {         for (int y = 0; y < rows; y++) {             for (int z = 0; z < depth; z++) {                 unsigned long long i = z * rows * cols + y * cols + x;                 unsigned long long j = z * rows * cols + x;                 unsigned long size2d = cols;                 unsigned long size3d = depth * rows * cols + rows * cols + cols;                 if (i + cols + 1 >= size3d || j + 1 >= size2d) {                     return;                 }                 vec[i + cols] = 0.25 * (vec1[i + 1] + vec1[i] + vec1[i + cols + 1] + vec1[i + cols]);                 vec[j] = (vec1[j] + vec1[j + 1]) / 4;             }         }     } }  int main() {          long depth = 3;     long rows = 4;     long cols = 5;      float vec1[depth * rows * cols];     float vec[depth * rows * cols];           for (long i = 0; i < depth * rows * cols; i++) {         vec1[i] = i + 1;     }      opL21_cpu(vec, vec1, depth, rows, cols);           printf(\"Output vec:\\n\");     for (long i = 0; i < depth * rows * cols; i++) {         printf(\"%.2f \", vec[i]);     }     printf(\"\\n\");      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h>  #define BLOCK_SIZE_X 16 #define BLOCK_SIZE_Y 16 #define BLOCK_SIZE_Z 4  __global__ void opL21(float *vec, float *vec1, long depth, long rows, long cols) {     unsigned long x = threadIdx.x + blockIdx.x * blockDim.x;     unsigned long y = threadIdx.y + blockIdx.y * blockDim.y;     unsigned long z = threadIdx.z + blockIdx.z * blockDim.z;     unsigned long long i = z * rows * cols + y * cols + x;     unsigned long long j = z * rows * cols + x;     unsigned long size2d = cols;     unsigned long size3d = depth * rows * cols + rows * cols + cols;      if (x >= cols || y >= rows || z >= depth)         return;      if (i + cols + 1 >= size3d)         return;      vec[i + cols] = 0.25 * (vec1[i + 1] + vec1[i] + vec1[i + cols + 1] + vec1[i + cols]);      if (j + 1 >= size2d)         return;      vec[j] = (vec1[j] + vec1[j + 1]) / 4; }  int main() {     const long depth = 64;     const long rows = 128;     const long cols = 64;      float *vec, *vec1;      size_t size3d = depth * rows * cols * sizeof(float);     size_t size2d = rows * cols * sizeof(float);           vec = (float *)malloc(size3d);     vec1 = (float *)malloc(size3d);           for (long i = 0; i < depth * rows * cols; ++i) {         vec[i] = static_cast<float>(i);         vec1[i] = static_cast<float>(i * 2);     }           float *d_vec, *d_vec1;     cudaMalloc((void **)&d_vec, size3d);     cudaMalloc((void **)&d_vec1, size3d);           cudaMemcpy(d_vec, vec, size3d, cudaMemcpyHostToDevice);     cudaMemcpy(d_vec1, vec1, size3d, cudaMemcpyHostToDevice);           dim3 block_size(BLOCK_SIZE_X, BLOCK_SIZE_Y, BLOCK_SIZE_Z);     dim3 grid_size((cols + block_size.x - 1) / block_size.x, (rows + block_size.y - 1) / block_size.y, (depth + block_size.z - 1) / block_size.z);           opL21<<<grid_size, block_size>>>(d_vec, d_vec1, depth, rows, cols);           cudaMemcpy(vec, d_vec, size3d, cudaMemcpyDeviceToHost);           cudaFree(d_vec);     cudaFree(d_vec1);     free(vec);     free(vec1);      return 0; }   "
    },
    {
        "id": "343",
        "c_code": "#include <stdio.h>  void reorg_cpu(float *x, int w, int h, int c, int batch, int stride, int forward, float *out) {     int b, i, j, k;     int out_c = c / (stride * stride);      for (b = 0; b < batch; ++b) {         for (k = 0; k < c; ++k) {             for (j = 0; j < h; ++j) {                 for (i = 0; i < w; ++i) {                     int in_index = i + w * (j + h * (k + c * b));                     int c2 = k % out_c;                     int offset = k / out_c;                     int w2 = i * stride + offset % stride;                     int h2 = j * stride + offset / stride;                     int out_index = w2 + w * stride * (h2 + h * stride * (c2 + out_c * b));                      if (forward)                         out[out_index] = x[in_index];                     else                         out[in_index] = x[out_index];                 }             }         }     } }  int main() {          int w = 4;     int h = 4;     int c = 3;     int batch = 2;     int stride = 2;      float x[w * h * c * batch];     float out[w * h * c * batch];           for (int i = 0; i < w * h * c * batch; i++) {         x[i] = i + 1;     }      reorg_cpu(x, w, h, c, batch, stride, 1, out);             printf(\"Output (Forward Pass):\\n\");     for (int i = 0; i < w * h * c * batch; i++) {         printf(\"%.2f \", out[i]);     }     printf(\"\\n\");      reorg_cpu(x, w, h, c, batch, stride, 0, out);             printf(\"Output (Backward Pass):\\n\");     for (int i = 0; i < w * h * c * batch; i++) {         printf(\"%.2f \", out[i]);     }     printf(\"\\n\");      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h>  #define BLOCK_SIZE 256  __global__ void reorg_kernel(int N, float *x, int w, int h, int c, int batch, int stride, int forward, float *out) {     int i = (blockIdx.x + blockIdx.y * gridDim.x) * blockDim.x + threadIdx.x;      if (i >= N)         return;      int in_index = i;     int in_w = i % w;     i = i / w;     int in_h = i % h;     i = i / h;     int in_c = i % c;     i = i / c;     int b = i % batch;      int out_c = c / (stride * stride);     int c2 = in_c % out_c;     int offset = in_c / out_c;     int w2 = in_w * stride + offset % stride;     int h2 = in_h * stride + offset / stride;      int out_index = w2 + w * stride * (h2 + h * stride * (c2 + out_c * b));      if (forward)         out[out_index] = x[in_index];     else         out[in_index] = x[out_index]; }  int main() {     const int N = 1024;      const int w = 64;        const int h = 64;        const int c = 3;         const int batch = 4;      const int stride = 2;       float *x, *out;      size_t size = N * sizeof(float);           x = (float *)malloc(size);     out = (float *)malloc(size);           for (int i = 0; i < N; ++i) {         x[i] = static_cast<float>(i);     }           float *d_x, *d_out;     cudaMalloc((void **)&d_x, size);     cudaMalloc((void **)&d_out, size);           cudaMemcpy(d_x, x, size, cudaMemcpyHostToDevice);           dim3 block_size(BLOCK_SIZE);     dim3 grid_size((N + block_size.x - 1) / block_size.x);           reorg_kernel<<<grid_size, block_size>>>(N, d_x, w, h, c, batch, stride, 1, d_out);            cudaMemcpy(out, d_out, size, cudaMemcpyDeviceToHost);           cudaFree(d_x);     cudaFree(d_out);     free(x);     free(out);      return 0; }   "
    },
    {
        "id": "344",
        "c_code": "#include <stdio.h>  long int maxValExtractArray(float *normM_aux, long int *b_pos, long int b_pos_size) {     float max_val = -1;     long int pos = -1;      for (long int i = 0; i < b_pos_size; i++) {         if (normM_aux[b_pos[i]] > max_val) {             max_val = normM_aux[b_pos[i]];             pos = i;         }     }      return pos; }  int main() {          long int b_pos_size = 5;     float normM_aux[] = {1.2, 3.5, 2.8, 5.1, 4.2};     long int b_pos[] = {2, 0, 4, 1, 3};      long int result = maxValExtractArray(normM_aux, b_pos, b_pos_size);           printf(\"Position of maximum value: %ld\\n\", result);      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h>  #define BLOCK_SIZE 1024  __global__ void maxValExtract(float *normM_c, float *normM1_c, long int image_size, float *d_projections, int *d_index, float a) {     __shared__ int pos[BLOCK_SIZE * 2];     __shared__ float val[BLOCK_SIZE * 2];      unsigned int tid = threadIdx.x;     unsigned int id = blockIdx.x * 2 * blockDim.x + threadIdx.x;      float faux, faux2;     faux = ((a - normM_c[id]) / a);     faux2 = ((a - normM_c[id + blockDim.x]) / a);      if (id < image_size && faux <= 1.0e-6) {         val[tid] = normM1_c[id];         pos[tid] = id;     } else {         val[tid] = -1;     }      if (id + blockDim.x < image_size && faux2 <= 1.0e-6) {         val[tid + blockDim.x] = normM1_c[id + blockDim.x];         pos[tid + blockDim.x] = id + blockDim.x;     } else {         val[tid + blockDim.x] = -1;     }      __syncthreads();      for (unsigned int s = blockDim.x; s > 0; s >>= 1) {         if (tid < s) {             if (val[tid] <= val[tid + s]) {                 val[tid] = val[tid + s];                 pos[tid] = pos[tid + s];             }         }          __syncthreads();     }      if (tid == 0) {         d_projections[blockIdx.x] = val[0];         d_index[blockIdx.x] = (int)pos[0];     }      __syncthreads(); }  int main() {     const long int image_size = 1024;      const float a = 1.0;       float *normM_c, *normM1_c, *d_projections;     int *d_index;      size_t size = image_size * sizeof(float);           normM_c = (float *)malloc(size * 2);      normM1_c = (float *)malloc(size);     d_projections = (float *)malloc((image_size / BLOCK_SIZE) * sizeof(float));     d_index = (int *)malloc((image_size / BLOCK_SIZE) * sizeof(int));           for (long int i = 0; i < size * 2; ++i) {         normM_c[i] = static_cast<float>(i);     }      for (long int i = 0; i < size; ++i) {         normM1_c[i] = static_cast<float>(i * 2);     }           float *d_normM_c, *d_normM1_c;     int *d_d_index;     cudaMalloc((void **)&d_normM_c, size * 2);     cudaMalloc((void **)&d_normM1_c, size);     cudaMalloc((void **)&d_d_index, (image_size / BLOCK_SIZE) * sizeof(int));           cudaMemcpy(d_normM_c, normM_c, size * 2, cudaMemcpyHostToDevice);     cudaMemcpy(d_normM1_c, normM1_c, size, cudaMemcpyHostToDevice);           dim3 block_size(BLOCK_SIZE);     dim3 grid_size((image_size / BLOCK_SIZE + 1), 1);           maxValExtract<<<grid_size, block_size>>>(d_normM_c, d_normM1_c, image_size, d_projections, d_index, a);           cudaMemcpy(d_projections, d_projections, (image_size / BLOCK_SIZE) * sizeof(float), cudaMemcpyDeviceToHost);     cudaMemcpy(d_index, d_d_index, (image_size / BLOCK_SIZE) * sizeof(int), cudaMemcpyDeviceToHost);           cudaFree(d_normM_c);     cudaFree(d_normM1_c);     cudaFree(d_d_index);     free(normM_c);     free(normM1_c);     free(d_projections);     free(d_index);      return 0; }   "
    },
    {
        "id": "345",
        "c_code": "#include <stdio.h>  void returnResult_cpu(const float *box, const float *score, const int *label,                       float *box_out, float *score_out, int *label_out,                       float score_thr, const int dims) {     for (int tid = 0; tid < dims; tid++) {         if (score[tid] < score_thr) {             score_out[tid] = 0;             box_out[tid * 4 + 0] = -1;             box_out[tid * 4 + 1] = -1;             box_out[tid * 4 + 2] = -1;             box_out[tid * 4 + 3] = -1;             label_out[tid] = -1;         } else {             score_out[tid] = score[tid];             box_out[tid * 4 + 0] = box[tid * 4 + 0];             box_out[tid * 4 + 1] = box[tid * 4 + 1];             box_out[tid * 4 + 2] = box[tid * 4 + 2];             box_out[tid * 4 + 3] = box[tid * 4 + 3];             label_out[tid] = label[tid];         }     } }  int main() {          const int dims = 5;     float box[] = {1.0, 2.0, 3.0, 4.0, 5.0, 2.0, 3.0, 4.0, 5.0, 6.0,                    3.0, 4.0, 5.0, 6.0, 7.0, 4.0, 5.0, 6.0, 7.0, 8.0,                    5.0, 6.0, 7.0, 8.0, 9.0};     float score[] = {0.8, 0.6, 0.3, 0.9, 0.7};     int label[] = {1, 2, 3, 4, 5};     float box_out[dims * 4];     float score_out[dims];     int label_out[dims];      float score_thr = 0.5;      returnResult_cpu(box, score, label, box_out, score_out, label_out, score_thr, dims);           printf(\"Results after filtering:\\n\");     for (int i = 0; i < dims; i++) {         printf(\"Box: %.2f %.2f %.2f %.2f, Score: %.2f, Label: %d\\n\",                box_out[i * 4], box_out[i * 4 + 1], box_out[i * 4 + 2], box_out[i * 4 + 3],                score_out[i], label_out[i]);     }      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h>  #define BLOCK_SIZE 256  __global__ void returnResult(const float *box, const float *score, const int *label, float *box_out, float *score_out, int *label_out, float score_thr, const int dims) {     int tid = blockIdx.x * blockDim.x + threadIdx.x;      if (tid >= dims) {         return;     }      if (score[tid] < score_thr) {         score_out[tid] = 0;         box_out[tid * 4 + 0] = -1;         box_out[tid * 4 + 1] = -1;         box_out[tid * 4 + 2] = -1;         box_out[tid * 4 + 3] = -1;         label_out[tid] = -1;     } else {         score_out[tid] = score[tid];         box_out[tid * 4 + 0] = box[tid * 4 + 0];         box_out[tid * 4 + 1] = box[tid * 4 + 1];         box_out[tid * 4 + 2] = box[tid * 4 + 2];         box_out[tid * 4 + 3] = box[tid * 4 + 3];         label_out[tid] = label[tid];     } }  int main() {     const int dims = 1024;      const float score_thr = 0.5;       float *box, *score_out, *box_out;     int *label, *label_out;      size_t size = dims * sizeof(float);           box = (float *)malloc(size * 4);     score_out = (float *)malloc(size);     box_out = (float *)malloc(size * 4);     label = (int *)malloc(size);     label_out = (int *)malloc(size);           for (int i = 0; i < dims * 4; ++i) {         box[i] = static_cast<float>(i);     }      for (int i = 0; i < dims; ++i) {         score_out[i] = static_cast<float>(i);         label[i] = i;     }           float *d_box, *d_score_out, *d_box_out;     int *d_label, *d_label_out;     cudaMalloc((void **)&d_box, size * 4);     cudaMalloc((void **)&d_score_out, size);     cudaMalloc((void **)&d_box_out, size * 4);     cudaMalloc((void **)&d_label, size);     cudaMalloc((void **)&d_label_out, size);           cudaMemcpy(d_box, box, size * 4, cudaMemcpyHostToDevice);     cudaMemcpy(d_score_out, score_out, size, cudaMemcpyHostToDevice);     cudaMemcpy(d_label, label, size, cudaMemcpyHostToDevice);           dim3 block_size(BLOCK_SIZE);     dim3 grid_size((dims + block_size.x - 1) / block_size.x);           returnResult<<<grid_size, block_size>>>(d_box, d_score_out, d_label, d_box_out, d_score_out, d_label_out, score_thr, dims);           cudaMemcpy(score_out, d_score_out, size, cudaMemcpyDeviceToHost);     cudaMemcpy(box_out, d_box_out, size * 4, cudaMemcpyDeviceToHost);     cudaMemcpy(label_out, d_label_out, size, cudaMemcpyDeviceToHost);           cudaFree(d_box);     cudaFree(d_score_out);     cudaFree(d_box_out);     cudaFree(d_label);     cudaFree(d_label_out);     free(box);     free(score_out);     free(box_out);     free(label);     free(label_out);      return 0; }   "
    },
    {
        "id": "346",
        "c_code": "#include <stdio.h> #include <stdlib.h>  void *RyT(float *R, float *T, float *P, float *Q, int start, int end) {     for (int i = start; i < end; i++) {         Q[0 + i * 3] = R[0 + 0 * 3] * P[0 + i * 3] + R[0 + 1 * 3] * P[1 + i * 3] +                        R[0 + 2 * 3] * P[2 + i * 3] + T[0];         Q[1 + i * 3] = R[1 + 0 * 3] * P[0 + i * 3] + R[1 + 1 * 3] * P[1 + i * 3] +                        R[1 + 2 * 3] * P[2 + i * 3] + T[1];         Q[2 + i * 3] = R[2 + 0 * 3] * P[0 + i * 3] + R[2 + 1 * 3] * P[1 + i * 3] +                        R[2 + 2 * 3] * P[2 + i * 3] + T[2];     }     return (void *)0; }  int main() {          int num_points = 3;     float R[9] = {1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0};     float T[3] = {1.0, 2.0, 3.0};     float P[9] = {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0};     float Q[9];      RyT(R, T, P, Q, 0, num_points);           printf(\"Results after RyT function:\\n\");     for (int i = 0; i < num_points; i++) {         printf(\"Q[%d]: %.2f %.2f %.2f\\n\", i,                Q[0 + i * 3], Q[1 + i * 3], Q[2 + i * 3]);     }      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h>  #define BLOCK_SIZE 256  __global__ void RyT(float *R, float *T, float *P, float *Q, int num_points) {     int i = blockIdx.x * blockDim.x + threadIdx.x;      if (i < num_points) {         Q[0 + i * 3] = R[0 + 0 * 3] * P[0 + i * 3] + R[0 + 1 * 3] * P[1 + i * 3] + R[0 + 2 * 3] * P[2 + i * 3] + T[0];         Q[1 + i * 3] = R[1 + 0 * 3] * P[0 + i * 3] + R[1 + 1 * 3] * P[1 + i * 3] + R[1 + 2 * 3] * P[2 + i * 3] + T[1];         Q[2 + i * 3] = R[2 + 0 * 3] * P[0 + i * 3] + R[2 + 1 * 3] * P[1 + i * 3] + R[2 + 2 * 3] * P[2 + i * 3] + T[2];     } }  int main() {     const int num_points = 1024;       float *R, *T, *P, *Q;           R = (float *)malloc(9 * sizeof(float));     T = (float *)malloc(3 * sizeof(float));     P = (float *)malloc(3 * num_points * sizeof(float));     Q = (float *)malloc(3 * num_points * sizeof(float));           for (int i = 0; i < 9; ++i) {         R[i] = static_cast<float>(i);     }      for (int i = 0; i < 3; ++i) {         T[i] = static_cast<float>(i * 2);     }      for (int i = 0; i < 3 * num_points; ++i) {         P[i] = static_cast<float>(i * 3);     }           float *d_R, *d_T, *d_P, *d_Q;     cudaMalloc((void **)&d_R, 9 * sizeof(float));     cudaMalloc((void **)&d_T, 3 * sizeof(float));     cudaMalloc((void **)&d_P, 3 * num_points * sizeof(float));     cudaMalloc((void **)&d_Q, 3 * num_points * sizeof(float));           cudaMemcpy(d_R, R, 9 * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_T, T, 3 * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_P, P, 3 * num_points * sizeof(float), cudaMemcpyHostToDevice);           dim3 block_size(BLOCK_SIZE);     dim3 grid_size((num_points + block_size.x - 1) / block_size.x);           RyT<<<grid_size, block_size>>>(d_R, d_T, d_P, d_Q, num_points);           cudaMemcpy(Q, d_Q, 3 * num_points * sizeof(float), cudaMemcpyDeviceToHost);           cudaFree(d_R);     cudaFree(d_T);     cudaFree(d_P);     cudaFree(d_Q);     free(R);     free(T);     free(P);     free(Q);      return 0; }   "
    },
    {
        "id": "347",
        "c_code": "#include <stdio.h> #include <stdlib.h> #include <math.h>  void primal_descent(float *y1, float *y2, float *xbar, float sigma, int w, int h, int nc) {     for (int x = 0; x < w; x++) {         for (int y = 0; y < h; y++) {             int i;             float x1, x2, val, norm;             for (int z = 0; z < nc; z++) {                 i = x + w * y + w * h * z;                 val = xbar[i];                 x1 = (x + 1 < w) ? (xbar[(x + 1) + w * y + w * h * z] - val) : 0.f;                 x2 = (y + 1 < h) ? (xbar[x + w * (y + 1) + w * h * z] - val) : 0.f;                 x1 = y1[i] + sigma * x1;                 x2 = y2[i] + sigma * x2;                 norm = sqrtf(x1 * x1 + x2 * x2);                 y1[i] = x1 / fmax(1.f, norm);                 y2[i] = x2 / fmax(1.f, norm);             }         }     } }  int main() {          int w = 3;     int h = 3;     int nc = 2;     float sigma = 0.1;      float y1[w * h * nc];     float y2[w * h * nc];     float xbar[w * h * nc];           for (int i = 0; i < w * h * nc; i++) {         y1[i] = 1.0;         y2[i] = 2.0;         xbar[i] = 3.0;     }           primal_descent(y1, y2, xbar, sigma, w, h, nc);           printf(\"Results after primal_descent function:\\n\");     for (int i = 0; i < w * h * nc; i++) {         printf(\"y1[%d]: %.2f, y2[%d]: %.2f\\n\", i, y1[i], i, y2[i]);     }      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h> #include <math.h>  #define BLOCK_SIZE_X 16 #define BLOCK_SIZE_Y 16  __global__ void primal_descent(float *y1, float *y2, float *xbar, float sigma, int w, int h, int nc) {     int x = threadIdx.x + blockDim.x * blockIdx.x;     int y = threadIdx.y + blockDim.y * blockIdx.y;      if (x < w && y < h) {         int i;         float x1, x2, val, norm;          for (int z = 0; z < nc; z++) {             i = x + w * y + w * h * z;             val = xbar[i];              x1 = (x + 1 < w) ? (xbar[(x + 1) + w * y + w * h * z] - val) : 0.f;             x2 = (y + 1 < h) ? (xbar[x + w * (y + 1) + w * h * z] - val) : 0.f;              x1 = y1[i] + sigma * x1;             x2 = y2[i] + sigma * x2;              norm = sqrtf(x1 * x1 + x2 * x2);              y1[i] = x1 / fmax(1.f, norm);             y2[i] = x2 / fmax(1.f, norm);         }     } }  int main() {     const int w = 512;      const int h = 512;      const int nc = 3;        const float sigma = 0.01;       float *y1, *y2, *xbar;           y1 = (float *)malloc(w * h * nc * sizeof(float));     y2 = (float *)malloc(w * h * nc * sizeof(float));     xbar = (float *)malloc(w * h * nc * sizeof(float));           for (int i = 0; i < w * h * nc; ++i) {         y1[i] = static_cast<float>(i);         y2[i] = static_cast<float>(i * 2);         xbar[i] = static_cast<float>(i * 3);     }           float *d_y1, *d_y2, *d_xbar;     cudaMalloc((void **)&d_y1, w * h * nc * sizeof(float));     cudaMalloc((void **)&d_y2, w * h * nc * sizeof(float));     cudaMalloc((void **)&d_xbar, w * h * nc * sizeof(float));           cudaMemcpy(d_y1, y1, w * h * nc * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_y2, y2, w * h * nc * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_xbar, xbar, w * h * nc * sizeof(float), cudaMemcpyHostToDevice);           dim3 block_size(BLOCK_SIZE_X, BLOCK_SIZE_Y);     dim3 grid_size((w + block_size.x - 1) / block_size.x, (h + block_size.y - 1) / block_size.y);           primal_descent<<<grid_size, block_size>>>(d_y1, d_y2, d_xbar, sigma, w, h, nc);           cudaMemcpy(y1, d_y1, w * h * nc * sizeof(float), cudaMemcpyDeviceToHost);     cudaMemcpy(y2, d_y2, w * h * nc * sizeof(float), cudaMemcpyDeviceToHost);           cudaFree(d_y1);     cudaFree(d_y2);     cudaFree(d_xbar);     free(y1);     free(y2);     free(xbar);      return 0; }   "
    },
    {
        "id": "348",
        "c_code": "#include <stdio.h> #include <stdlib.h> #include <math.h>  void fractal_cpu(const int width, const int frames, unsigned char *const pic) {     for (int i = 0; i < width * width * frames; i++) {         const double Delta = 0.00304;         const double xMid = -0.055846456;         const double yMid = -0.668311119;          const int frame = i / (width * width);         double delta = Delta * pow(0.975, frame);          const int col = i % width;         const double xMin = xMid - delta;          const int row = (i / width) % width;         const double yMin = yMid - delta;          const double dw = 2.0 * delta / width;         const double cy = yMin + row * dw;         const double cx = xMin + col * dw;          double x = cx;         double y = cy;         double x2, y2;         int count = 256;          do {             x2 = x * x;             y2 = y * y;             y = 2.0 * x * y + cy;             x = x2 - y2 + cx;             count--;         } while ((count > 0) && ((x2 + y2) <= 5.0));          pic[frame * width * width + row * width + col] = (unsigned char)count;     } }  int main() {          int width = 512;     int frames = 3;     unsigned char *pic = (unsigned char *)malloc(width * width * frames * sizeof(unsigned char));           fractal_cpu(width, frames, pic);           printf(\"Results after fractal_cpu function:\\n\");     for (int i = 0; i < width * width * frames; i++) {         printf(\"%d \", pic[i]);     }      free(pic);     return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h>  #define BLOCK_SIZE 256  __global__ void fractal(const int width, const int frames, unsigned char *const pic) {     long i = threadIdx.x + blockIdx.x * (long)blockDim.x;      if (i >= width * width * frames) {         return;     }      const double Delta = 0.00304;     const double xMid = -0.055846456;     const double yMid = -0.668311119;      int frame = i / (width * width);     double delta = Delta * pow(0.975, frame);      int col = i % width;     double xMin = xMid - delta;     double yMin = yMid - delta;      double dw = 2.0 * delta / width;     int row = (i / width) % width;      double cy = yMin + row * dw;     double cx = xMin + col * dw;      double x = cx;     double y = cy;     double x2, y2;     int count = 256;      do {         x2 = x * x;         y2 = y * y;         y = 2.0 * x * y + cy;         x = x2 - y2 + cx;         count--;     } while ((count > 0) && ((x2 + y2) <= 5.0));      pic[frame * width * width + row * width + col] = (unsigned char)count; }  int main() {     const int width = 512;      const int frames = 30;       unsigned char *pic;           pic = (unsigned char *)malloc(width * width * frames * sizeof(unsigned char));           unsigned char *d_pic;     cudaMalloc((void **)&d_pic, width * width * frames * sizeof(unsigned char));           dim3 block_size(BLOCK_SIZE);     dim3 grid_size((width * width * frames + block_size.x - 1) / block_size.x);           fractal<<<grid_size, block_size>>>(width, frames, d_pic);           cudaMemcpy(pic, d_pic, width * width * frames * sizeof(unsigned char), cudaMemcpyDeviceToHost);           cudaFree(d_pic);     free(pic);      return 0; }   "
    },
    {
        "id": "349",
        "c_code": "#include <stdio.h> #include <stdlib.h> #include <omp.h>  #define ENDCOM  void Ring_cpu_kernel(float *A, float *BP, int *corrAB, float *M, int ring, int c, int h, int w) {     int ringSize = 2 * ring + 1;     int ringPatch = ringSize * ringSize;     int size = h * w;      #pragma omp parallel for ENDCOM     for (int y1 = 0; y1 < h; y1++) {         for (int x1 = 0; x1 < w; x1++) {             int id = y1 * w + x1;             int x2 = corrAB[2 * id + 0];             int y2 = corrAB[2 * id + 1];              for (int dx = -ring; dx <= ring; dx++) {                 for (int dy = -ring; dy <= ring; dy++) {                     int pIdx = (dy + ring) * ringSize + (dx + ring);                     int _x2 = x2 + dx, _y2 = y2 + dy;                      if (_x2 >= 0 && _x2 < w && _y2 >= 0 && _y2 < h) {                         for (int dc = 0; dc < c; dc++) {                             M[(dc * size + y1 * w) * ringPatch + pIdx * w + x1] = BP[dc * size + _y2 * w + _x2];                         }                     }                 }             }         }     }      return; }  int main() {          int ring = 1;     int c = 3;      int h = 4;      int w = 4;      int size = h * w;     int ringSize = 2 * ring + 1;     int ringPatch = ringSize * ringSize;      float *A = (float *)malloc(c * size * sizeof(float));     float *BP = (float *)malloc(c * size * sizeof(float));     int *corrAB = (int *)malloc(2 * size * sizeof(int));     float *M = (float *)malloc(c * size * ringPatch * sizeof(float));           for (int i = 0; i < c * size; i++) {         A[i] = (float)i;         BP[i] = (float)(i + c * size);     }      for (int i = 0; i < 2 * size; i++) {         corrAB[i] = i % size;     }           Ring_cpu_kernel(A, BP, corrAB, M, ring, c, h, w);           printf(\"Results after Ring_cpu_kernel function:\\n\");     for (int i = 0; i < c * size * ringPatch; i++) {         printf(\"%f \", M[i]);     }      free(A);     free(BP);     free(corrAB);     free(M);      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h>  #define BLOCK_SIZE 256  __global__ void Ring_kernel(float *A, float *BP, int *corrAB, float *M, int ring, int c, int h, int w) {     int id1 = blockIdx.x * blockDim.x + threadIdx.x;     int size = h * w;     int ringSize = 2 * ring + 1;     int ringPatch = ringSize * ringSize;      if (id1 < size) {         int y1 = id1 / w, x1 = id1 % w;         int y2 = corrAB[2 * id1 + 1], x2 = corrAB[2 * id1 + 0];          for (int dx = -ring; dx <= ring; dx++)             for (int dy = -ring; dy <= ring; dy++) {                 int pIdx = (dy + ring) * ringSize + (dx + ring);                 int _x2 = x2 + dx, _y2 = y2 + dy;                  if (_x2 >= 0 && _x2 < w && _y2 >= 0 && _y2 < h) {                     for (int dc = 0; dc < c; dc++) {                         M[(dc * size + y1 * w) * ringPatch + pIdx * w + x1] = BP[dc * size + _y2 * w + _x2];                     }                 }             }     } }  int main() {     const int h = 512;      const int w = 512;      const int c = 3;        const int ring = 2;       float *A, *BP, *M;     int *corrAB;           A = (float *)malloc(h * w * sizeof(float));     BP = (float *)malloc(c * h * w * sizeof(float));     M = (float *)malloc(c * h * w * (2 * ring + 1) * (2 * ring + 1) * sizeof(float));     corrAB = (int *)malloc(2 * h * w * sizeof(int));           for (int i = 0; i < h * w; ++i) {         A[i] = static_cast<float>(i);         corrAB[2 * i] = i % w;         corrAB[2 * i + 1] = i / w;     }      for (int i = 0; i < c * h * w; ++i) {         BP[i] = static_cast<float>(i);     }           float *d_A, *d_BP, *d_M;     int *d_corrAB;     cudaMalloc((void **)&d_A, h * w * sizeof(float));     cudaMalloc((void **)&d_BP, c * h * w * sizeof(float));     cudaMalloc((void **)&d_M, c * h * w * (2 * ring + 1) * (2 * ring + 1) * sizeof(float));     cudaMalloc((void **)&d_corrAB, 2 * h * w * sizeof(int));           cudaMemcpy(d_A, A, h * w * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_BP, BP, c * h * w * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_corrAB, corrAB, 2 * h * w * sizeof(int), cudaMemcpyHostToDevice);           dim3 block_size(BLOCK_SIZE);     dim3 grid_size((h * w + block_size.x - 1) / block_size.x);           Ring_kernel<<<grid_size, block_size>>>(d_A, d_BP, d_corrAB, d_M, ring, c, h, w);           cudaMemcpy(M, d_M, c * h * w * (2 * ring + 1) * (2 * ring + 1) * sizeof(float), cudaMemcpyDeviceToHost);           cudaFree(d_A);     cudaFree(d_BP);     cudaFree(d_M);     cudaFree(d_corrAB);     free(A);     free(BP);     free(M);     free(corrAB);      return 0; }   "
    },
    {
        "id": "350",
        "c_code": "#include <stdio.h> #include <stdlib.h> #include <omp.h>  #define ENDCOM  void convolutionCPU(float *host_outputMatrix, float *host_inputMatrix, float *host_filter, int imageRows, int imageColumns, int filterSize) {     #pragma omp parallel for ENDCOM     for (int eachRowOfImage = 0; eachRowOfImage < (int)imageRows; ++eachRowOfImage) {         for (int eachColumnOfImage = 0; eachColumnOfImage < (int)imageColumns; ++eachColumnOfImage) {             float convolvedValue = 0.f;              for (int eachRowOfFilter = -filterSize / 2; eachRowOfFilter <= filterSize / 2; ++eachRowOfFilter) {                 for (int eachColumnOfFilter = -filterSize / 2; eachColumnOfFilter <= filterSize / 2; ++eachColumnOfFilter) {                     int imageRow = eachRowOfImage + eachRowOfFilter;                     int imageColumn = eachColumnOfImage + eachColumnOfFilter;                      float pixelValue = (imageRow >= 0 && imageRow < imageRows && imageColumn >= 0 && imageColumn < imageColumns)                         ? host_inputMatrix[imageRow * imageColumns + imageColumn]                         : 0.f;                      float filterValue = host_filter[(eachRowOfFilter + filterSize / 2) * filterSize + eachColumnOfFilter + filterSize / 2];                      convolvedValue += pixelValue * filterValue;                 }             }              host_outputMatrix[eachRowOfImage * imageColumns + eachColumnOfImage] = convolvedValue;         }     } }  int main() {          int imageRows = 4;     int imageColumns = 4;     int filterSize = 3;      float *host_outputMatrix = (float *)malloc(imageRows * imageColumns * sizeof(float));     float *host_inputMatrix = (float *)malloc(imageRows * imageColumns * sizeof(float));     float *host_filter = (float *)malloc(filterSize * filterSize * sizeof(float));           for (int i = 0; i < imageRows * imageColumns; i++) {         host_inputMatrix[i] = (float)i;     }      for (int i = 0; i < filterSize * filterSize; i++) {         host_filter[i] = 1.0f;     }           convolutionCPU(host_outputMatrix, host_inputMatrix, host_filter, imageRows, imageColumns, filterSize);           printf(\"Results after convolutionCPU function:\\n\");     for (int i = 0; i < imageRows * imageColumns; i++) {         printf(\"%f \", host_outputMatrix[i]);     }      free(host_outputMatrix);     free(host_inputMatrix);     free(host_filter);      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h>  #define BLOCK_SIZE_X 16 #define BLOCK_SIZE_Y 16  __global__ void convolution_kernel_v1(float *device_outputMatrix, float *device_inputMatrix, float *device_filter, int imageRows, int imageColumns, int filterSize) {     int index_x = blockDim.x * blockIdx.x + threadIdx.x;     int index_y = blockDim.y * blockIdx.y + threadIdx.y;      float convolvedValue = 0.f;      for (int eachFilterRow = -filterSize / 2; eachFilterRow <= filterSize / 2; ++eachFilterRow) {         for (int eachFilterColumn = -filterSize / 2; eachFilterColumn <= filterSize / 2; ++eachFilterColumn) {             int imageRow = index_y + eachFilterRow;             int imageColumn = index_x + eachFilterColumn;              float pixelValue = (imageRow >= 0 && imageRow < imageRows && imageColumn >= 0 && imageColumn < imageColumns)                                    ? device_inputMatrix[imageRow * imageColumns + imageColumn]                                    : 0.f;              float filterValue = device_filter[(eachFilterRow + filterSize / 2) * filterSize + eachFilterColumn + filterSize / 2];              convolvedValue += pixelValue * filterValue;         }     }      device_outputMatrix[index_y * imageColumns + index_x] = convolvedValue; }  int main() {     const int imageRows = 512;         const int imageColumns = 512;      const int filterSize = 3;           float *device_outputMatrix, *device_inputMatrix, *device_filter;           float *host_outputMatrix = (float *)malloc(imageRows * imageColumns * sizeof(float));     float *host_inputMatrix = (float *)malloc(imageRows * imageColumns * sizeof(float));     float *host_filter = (float *)malloc(filterSize * filterSize * sizeof(float));           for (int i = 0; i < imageRows * imageColumns; ++i) {         host_inputMatrix[i] = static_cast<float>(i);     }      for (int i = 0; i < filterSize * filterSize; ++i) {         host_filter[i] = static_cast<float>(i);     }           cudaMalloc((void **)&device_outputMatrix, imageRows * imageColumns * sizeof(float));     cudaMalloc((void **)&device_inputMatrix, imageRows * imageColumns * sizeof(float));     cudaMalloc((void **)&device_filter, filterSize * filterSize * sizeof(float));           cudaMemcpy(device_inputMatrix, host_inputMatrix, imageRows * imageColumns * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(device_filter, host_filter, filterSize * filterSize * sizeof(float), cudaMemcpyHostToDevice);           dim3 block_size(BLOCK_SIZE_X, BLOCK_SIZE_Y);     dim3 grid_size((imageColumns + block_size.x - 1) / block_size.x, (imageRows + block_size.y - 1) / block_size.y);           convolution_kernel_v1<<<grid_size, block_size>>>(device_outputMatrix, device_inputMatrix, device_filter, imageRows, imageColumns, filterSize);           cudaMemcpy(host_outputMatrix, device_outputMatrix, imageRows * imageColumns * sizeof(float), cudaMemcpyDeviceToHost);           cudaFree(device_outputMatrix);     cudaFree(device_inputMatrix);     cudaFree(device_filter);     free(host_outputMatrix);     free(host_inputMatrix);     free(host_filter);      return 0; }   "
    },
    {
        "id": "351",
        "c_code": "#include <stdio.h> #include <stdlib.h>  void opLadj1_cpu(float *vec, float *vec1, float *vec2, float *vec3, long depth, long rows, long cols) {     for (int x = 0; x < cols; x++) {         for (int y = 0; y < rows; y++) {             for (int z = 0; z < depth; z++) {                 unsigned long long i = z * rows * cols + y * cols + x;                 unsigned long long j = z * rows * cols + x;                 unsigned long size2d = cols;                 unsigned long size3d = depth * rows * cols + rows * cols + cols;                  if (i + cols + 1 >= size3d)                     return;                  vec[i + cols] = vec1[i + cols] + 0.25 * (vec2[i + cols] + vec2[i] + vec2[i + cols + 1] + vec2[i + 1]) +                                 0.5 * (vec3[i + cols] + vec3[i + cols + 1]);                  if (j + 1 >= size2d)                     return;                  vec[j] = vec1[j] + (vec2[j] + vec2[j + 1]) / 4 + (vec3[j] + vec3[j + 1]) / 2;             }         }     } }  int main() {          long depth = 3;     long rows = 4;     long cols = 5;      float *vec = (float *)malloc(depth * rows * cols * sizeof(float));     float *vec1 = (float *)malloc(depth * rows * cols * sizeof(float));     float *vec2 = (float *)malloc(depth * rows * cols * sizeof(float));     float *vec3 = (float *)malloc(depth * rows * cols * sizeof(float));           for (int i = 0; i < depth * rows * cols; i++) {         vec[i] = (float)i;         vec1[i] = (float)i;         vec2[i] = (float)i;         vec3[i] = (float)i;     }           opLadj1_cpu(vec, vec1, vec2, vec3, depth, rows, cols);           printf(\"Results after opLadj1_cpu function:\\n\");     for (int i = 0; i < depth * rows * cols; i++) {         printf(\"%f \", vec[i]);     }      free(vec);     free(vec1);     free(vec2);     free(vec3);      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h>  #define BLOCK_SIZE_X 16 #define BLOCK_SIZE_Y 16 #define BLOCK_SIZE_Z 4  __global__ void opLadj1(float *vec, float *vec1, float *vec2, float *vec3, long depth, long rows, long cols) {     unsigned long x = threadIdx.x + blockIdx.x * blockDim.x;     unsigned long y = threadIdx.y + blockIdx.y * blockDim.y;     unsigned long z = threadIdx.z + blockIdx.z * blockDim.z;      unsigned long long i = z * rows * cols + y * cols + x;     unsigned long long j = z * rows * cols + x;      unsigned long size2d = cols;     unsigned long size3d = depth * rows * cols + rows * cols + cols;      if (x >= cols || y >= rows || z >= depth)         return;      if (i + cols + 1 >= size3d)         return;      vec[i + cols] = vec1[i + cols] + 0.25 * (vec2[i + cols] + vec2[i] + vec2[i + cols + 1] + vec2[i + 1]) + 0.5 * (vec3[i + cols] + vec3[i + cols + 1]);      if (j + 1 >= size2d)         return;      vec[j] = vec1[j] + (vec2[j] + vec2[j + 1]) / 4 + (vec3[j] + vec3[j + 1]) / 2; }  int main() {     const long depth = 32;        const long rows = 512;        const long cols = 512;         float *vec, *vec1, *vec2, *vec3;           vec = (float *)malloc(depth * rows * cols * sizeof(float));     vec1 = (float *)malloc(depth * rows * cols * sizeof(float));     vec2 = (float *)malloc(depth * rows * cols * sizeof(float));     vec3 = (float *)malloc(depth * rows * cols * sizeof(float));           for (int i = 0; i < depth * rows * cols; ++i) {         vec[i] = static_cast<float>(i);         vec1[i] = static_cast<float>(i);         vec2[i] = static_cast<float>(i);         vec3[i] = static_cast<float>(i);     }           float *d_vec, *d_vec1, *d_vec2, *d_vec3;     cudaMalloc((void **)&d_vec, depth * rows * cols * sizeof(float));     cudaMalloc((void **)&d_vec1, depth * rows * cols * sizeof(float));     cudaMalloc((void **)&d_vec2, depth * rows * cols * sizeof(float));     cudaMalloc((void **)&d_vec3, depth * rows * cols * sizeof(float));           cudaMemcpy(d_vec, vec, depth * rows * cols * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_vec1, vec1, depth * rows * cols * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_vec2, vec2, depth * rows * cols * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_vec3, vec3, depth * rows * cols * sizeof(float), cudaMemcpyHostToDevice);           dim3 block_size(BLOCK_SIZE_X, BLOCK_SIZE_Y, BLOCK_SIZE_Z);     dim3 grid_size((cols + block_size.x - 1) / block_size.x, (rows + block_size.y - 1) / block_size.y, (depth + block_size.z - 1) / block_size.z);           opLadj1<<<grid_size, block_size>>>(d_vec, d_vec1, d_vec2, d_vec3, depth, rows, cols);           cudaMemcpy(vec, d_vec, depth * rows * cols * sizeof(float), cudaMemcpyDeviceToHost);           cudaFree(d_vec);     cudaFree(d_vec1);     cudaFree(d_vec2);     cudaFree(d_vec3);     free(vec);     free(vec1);     free(vec2);     free(vec3);      return 0; }   "
    },
    {
        "id": "352",
        "c_code": "#include <stdio.h> #include <stdlib.h>  void dual_ascent(float *xn, float *xbar, float *y1, float *y2, float *img, float tau, float lambda, float theta, int w, int h, int nc) {     for (int x = 0; x < w; x++) {         for (int y = 0; y < h; y++) {             int i;             float d1, d2, val;             for (int z = 0; z < nc; z++) {                 i = x + w * y + w * h * z;                 d1 = (x + 1 < w ? y1[i] : 0.f) - (x > 0 ? y1[(x - 1) + w * y + w * h * z] : 0.f);                 d2 = (y + 1 < h ? y2[i] : 0.f) - (y > 0 ? y2[x + w * (y - 1) + w * h * z] : 0.f);                 val = xn[i];                 xn[i] = ((val + tau * (d1 + d2)) + tau * lambda * img[i]) / (1.f + tau * lambda);                 xbar[i] = xn[i] + theta * (xn[i] - val);             }         }     } }  int main() {          int w = 3;     int h = 3;     int nc = 3;      float *xn = (float *)malloc(w * h * nc * sizeof(float));     float *xbar = (float *)malloc(w * h * nc * sizeof(float));     float *y1 = (float *)malloc(w * h * nc * sizeof(float));     float *y2 = (float *)malloc(w * h * nc * sizeof(float));     float *img = (float *)malloc(w * h * nc * sizeof(float));           for (int i = 0; i < w * h * nc; i++) {         xn[i] = (float)i;         xbar[i] = (float)i;         y1[i] = (float)i;         y2[i] = (float)i;         img[i] = (float)i;     }           float tau = 0.1;     float lambda = 0.01;     float theta = 0.05;     dual_ascent(xn, xbar, y1, y2, img, tau, lambda, theta, w, h, nc);           printf(\"Results after dual_ascent function:\\n\");     for (int i = 0; i < w * h * nc; i++) {         printf(\"%f \", xn[i]);     }      free(xn);     free(xbar);     free(y1);     free(y2);     free(img);      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h>  #define BLOCK_SIZE_X 16 #define BLOCK_SIZE_Y 16  __global__ void dual_ascent(float *xn, float *xbar, float *y1, float *y2, float *img, float tau, float lambda, float theta, int w, int h, int nc) {     int x = threadIdx.x + blockDim.x * blockIdx.x;     int y = threadIdx.y + blockDim.y * blockIdx.y;      if (x < w && y < h) {         int i;         float d1, d2, val;          for (int z = 0; z < nc; z++) {             i = x + w * y + w * h * z;             d1 = (x + 1 < w ? y1[i] : 0.f) - (x > 0 ? y1[(x - 1) + w * y + w * h * z] : 0.f);             d2 = (y + 1 < h ? y2[i] : 0.f) - (y > 0 ? y2[x + w * (y - 1) + w * h * z] : 0.f);             val = xn[i];              xn[i] = ((val + tau * (d1 + d2)) + tau * lambda * img[i]) / (1.f + tau * lambda);             xbar[i] = xn[i] + theta * (xn[i] - val);         }     } }  int main() {     const int w = 512;         const int h = 512;         const int nc = 3;          const float tau = 0.1;      const float lambda = 0.5;      const float theta = 0.2;       float *xn, *xbar, *y1, *y2, *img;           xn = (float *)malloc(w * h * nc * sizeof(float));     xbar = (float *)malloc(w * h * nc * sizeof(float));     y1 = (float *)malloc(w * h * nc * sizeof(float));     y2 = (float *)malloc(w * h * nc * sizeof(float));     img = (float *)malloc(w * h * nc * sizeof(float));           for (int i = 0; i < w * h * nc; ++i) {         xn[i] = static_cast<float>(i);         xbar[i] = static_cast<float>(i);         y1[i] = static_cast<float>(i);         y2[i] = static_cast<float>(i);         img[i] = static_cast<float>(i);     }           float *d_xn, *d_xbar, *d_y1, *d_y2, *d_img;     cudaMalloc((void **)&d_xn, w * h * nc * sizeof(float));     cudaMalloc((void **)&d_xbar, w * h * nc * sizeof(float));     cudaMalloc((void **)&d_y1, w * h * nc * sizeof(float));     cudaMalloc((void **)&d_y2, w * h * nc * sizeof(float));     cudaMalloc((void **)&d_img, w * h * nc * sizeof(float));           cudaMemcpy(d_xn, xn, w * h * nc * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_xbar, xbar, w * h * nc * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_y1, y1, w * h * nc * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_y2, y2, w * h * nc * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_img, img, w * h * nc * sizeof(float), cudaMemcpyHostToDevice);           dim3 block_size(BLOCK_SIZE_X, BLOCK_SIZE_Y);     dim3 grid_size((w + block_size.x - 1) / block_size.x, (h + block_size.y - 1) / block_size.y);           dual_ascent<<<grid_size, block_size>>>(d_xn, d_xbar, d_y1, d_y2, d_img, tau, lambda, theta, w, h, nc);           cudaMemcpy(xn, d_xn, w * h * nc * sizeof(float), cudaMemcpyDeviceToHost);           cudaFree(d_xn);     cudaFree(d_xbar);     cudaFree(d_y1);     cudaFree(d_y2);     cudaFree(d_img);     free(xn);     free(xbar);     free(y1);     free(y2);     free(img);      return 0; }   "
    },
    {
        "id": "353",
        "c_code": "#include <stdio.h> #include <stdlib.h>  void convoluteCPU(float *dData, float *hData, int height, int width, float *mask, int masksize) {     for (int row = 0; row < height; row++) {         for (int col = 0; col < width; col++) {             int S = (masksize - 1) / 2;             float sum = 0;             int pixPos = row * width + col;             dData[pixPos] = 0.0;              for (int maskrow = -S; maskrow <= S; maskrow++) {                 for (int maskcol = -S; maskcol <= S; maskcol++) {                     int pixP = (row + maskrow) * width + (col + maskcol);                     int maskP = (maskrow + S) * masksize + (maskcol + S);                      if (pixP < height * width && pixP > 0 && maskP < masksize * masksize) {                         sum += mask[maskP] * hData[pixP];                     }                 }             }              dData[pixPos] = sum;              if (dData[pixPos] < 0) {                 dData[pixPos] = 0;             } else if (dData[pixPos] > 1) {                 dData[pixPos] = 1;             }         }     } }  int main() {          int height = 3;     int width = 3;     int masksize = 3;      float *dData = (float *)malloc(height * width * sizeof(float));     float *hData = (float *)malloc(height * width * sizeof(float));     float *mask = (float *)malloc(masksize * masksize * sizeof(float));           for (int i = 0; i < height * width; i++) {         hData[i] = (float)i;     }           for (int i = 0; i < masksize * masksize; i++) {         mask[i] = 0.1;     }           convoluteCPU(dData, hData, height, width, mask, masksize);           printf(\"Results after convoluteCPU function:\\n\");     for (int i = 0; i < height * width; i++) {         printf(\"%f \", dData[i]);     }      free(dData);     free(hData);     free(mask);      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h>  #define BLOCK_SIZE_X 16 #define BLOCK_SIZE_Y 16  __global__ void convoluteGPU(float *dData, float *hData, int height, int width, float *mask, int masksize) {     int row = threadIdx.x + blockIdx.x * blockDim.x;     int col = threadIdx.y + blockIdx.y * blockDim.y;     int S = (masksize - 1) / 2;     float sum = 0;     int pixPos = row * width + col;      dData[pixPos] = 0.0;      if (row < height && col < width) {         for (int maskrow = -S; maskrow <= S; maskrow++) {             for (int maskcol = -S; maskcol <= S; maskcol++) {                 int pixP = (row + maskrow) * width + (col + maskcol);                 int maskP = (maskrow + S) * masksize + (maskcol + S);                  if (pixP < height * width && pixP > 0 && maskP < masksize * masksize) {                     sum += mask[maskP] * hData[pixP];                 }             }         }          dData[pixPos] = sum;          if (dData[pixPos] < 0) {             dData[pixPos] = 0;         } else if (dData[pixPos] > 1) {             dData[pixPos] = 1;         }     } }  int main() {     const int height = 512;        const int width = 512;         const int masksize = 3;         float *dData, *hData, *mask;           hData = (float *)malloc(height * width * sizeof(float));     dData = (float *)malloc(height * width * sizeof(float));     mask = (float *)malloc(masksize * masksize * sizeof(float));           for (int i = 0; i < height * width; ++i) {         hData[i] = static_cast<float>(i);     }           for (int i = 0; i < masksize * masksize; ++i) {         mask[i] = 1.0;     }           float *d_hData, *d_dData, *d_mask;     cudaMalloc((void **)&d_hData, height * width * sizeof(float));     cudaMalloc((void **)&d_dData, height * width * sizeof(float));     cudaMalloc((void **)&d_mask, masksize * masksize * sizeof(float));           cudaMemcpy(d_hData, hData, height * width * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_mask, mask, masksize * masksize * sizeof(float), cudaMemcpyHostToDevice);           dim3 block_size(BLOCK_SIZE_X, BLOCK_SIZE_Y);     dim3 grid_size((width + block_size.x - 1) / block_size.x, (height + block_size.y - 1) / block_size.y);           convoluteGPU<<<grid_size, block_size>>>(d_dData, d_hData, height, width, d_mask, masksize);           cudaMemcpy(dData, d_dData, height * width * sizeof(float), cudaMemcpyDeviceToHost);           cudaFree(d_hData);     cudaFree(d_dData);     cudaFree(d_mask);     free(hData);     free(dData);     free(mask);      return 0; }   "
    },
    {
        "id": "354",
        "c_code": "#include <stdio.h> #include <stdlib.h>  void opLadj2_cpu(float *vec, float *vec1, float *vec2, float *vec3, long depth, long rows, long cols) {     for (int x = 0; x < cols; x++) {         for (int y = 0; y < rows; y++) {             for (int z = 0; z < depth; z++) {                 unsigned long long i = z * rows * cols + y * cols + x;                 unsigned long long j = z * rows * cols + y * cols;                 unsigned long size2d = z * rows * cols + cols * rows;                 unsigned long size3d = depth * rows * cols + rows * cols + cols;                  if (i + cols + 1 >= size3d) return;                  vec[i + 1] = vec1[i + 1] + 0.25 * (vec2[i + 1] + vec2[i] + vec2[i + cols + 1] + vec2[i + cols]) + 0.5 * (vec3[i + 1] + vec3[i + cols + 1]);                  if (j + cols >= size2d) return;                  vec[j] = vec1[j] + (vec2[j] + vec2[j + cols]) / 4 + (vec3[j] + vec3[j + cols]) / 2;             }         }     } }  int main() {          int depth = 3;     int rows = 3;     int cols = 3;      float *vec = (float *)malloc(depth * rows * cols * sizeof(float));     float *vec1 = (float *)malloc(depth * rows * cols * sizeof(float));     float *vec2 = (float *)malloc(depth * rows * cols * sizeof(float));     float *vec3 = (float *)malloc(depth * rows * cols * sizeof(float));           for (int i = 0; i < depth * rows * cols; i++) {         vec1[i] = (float)i;         vec2[i] = (float)i * 2;         vec3[i] = (float)i * 3;     }           opLadj2_cpu(vec, vec1, vec2, vec3, depth, rows, cols);           printf(\"Results after opLadj2_cpu function:\\n\");     for (int i = 0; i < depth * rows * cols; i++) {         printf(\"%f \", vec[i]);     }      free(vec);     free(vec1);     free(vec2);     free(vec3);      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h>  #define BLOCK_SIZE_X 16 #define BLOCK_SIZE_Y 16 #define BLOCK_SIZE_Z 4  __global__ void opLadj2(float *vec, float *vec1, float *vec2, float *vec3, long depth, long rows, long cols) {     unsigned long x = threadIdx.x + blockIdx.x * blockDim.x;     unsigned long y = threadIdx.y + blockIdx.y * blockDim.y;     unsigned long z = threadIdx.z + blockIdx.z * blockDim.z;     unsigned long long i = z * rows * cols + y * cols + x;     unsigned long long j = z * rows * cols + y * cols;     unsigned long size2d = z * rows * cols + cols * rows;     unsigned long size3d = depth * rows * cols + rows * cols + cols;      if (x >= cols || y >= rows || z >= depth) return;      if (i + cols + 1 >= size3d) return;      vec[i + 1] = vec1[i + 1] + 0.25 * (vec2[i + 1] + vec2[i] + vec2[i + cols + 1] + vec2[i + cols]) + 0.5 * (vec3[i + 1] + vec3[i + cols + 1]);      if (j + cols >= size2d) return;      vec[j] = vec1[j] + (vec2[j] + vec2[j + cols]) / 4 + (vec3[j] + vec3[j + cols]) / 2; }  int main() {     const long depth = 64;        const long rows = 512;        const long cols = 512;         float *vec, *vec1, *vec2, *vec3;           vec = (float *)malloc(depth * rows * cols * sizeof(float));     vec1 = (float *)malloc(depth * rows * cols * sizeof(float));     vec2 = (float *)malloc(depth * rows * cols * sizeof(float));     vec3 = (float *)malloc(depth * rows * cols * sizeof(float));           for (long i = 0; i < depth * rows * cols; ++i) {         vec[i] = static_cast<float>(i);         vec1[i] = static_cast<float>(i);         vec2[i] = static_cast<float>(i);         vec3[i] = static_cast<float>(i);     }           float *d_vec, *d_vec1, *d_vec2, *d_vec3;     cudaMalloc((void **)&d_vec, depth * rows * cols * sizeof(float));     cudaMalloc((void **)&d_vec1, depth * rows * cols * sizeof(float));     cudaMalloc((void **)&d_vec2, depth * rows * cols * sizeof(float));     cudaMalloc((void **)&d_vec3, depth * rows * cols * sizeof(float));           cudaMemcpy(d_vec, vec, depth * rows * cols * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_vec1, vec1, depth * rows * cols * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_vec2, vec2, depth * rows * cols * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(d_vec3, vec3, depth * rows * cols * sizeof(float), cudaMemcpyHostToDevice);           dim3 block_size(BLOCK_SIZE_X, BLOCK_SIZE_Y, BLOCK_SIZE_Z);     dim3 grid_size((cols + block_size.x - 1) / block_size.x, (rows + block_size.y - 1) / block_size.y, (depth + block_size.z - 1) / block_size.z);           opLadj2<<<grid_size, block_size>>>(d_vec, d_vec1, d_vec2, d_vec3, depth, rows, cols);           cudaMemcpy(vec, d_vec, depth * rows * cols * sizeof(float), cudaMemcpyDeviceToHost);           cudaFree(d_vec);     cudaFree(d_vec1);     cudaFree(d_vec2);     cudaFree(d_vec3);     free(vec);     free(vec1);     free(vec2);     free(vec3);      return 0; }   "
    },
    {
        "id": "355",
        "c_code": "#include <stdio.h> #include <stdlib.h> #include <math.h>  void CrossEntropyLoss_forward(float *logits_data, float *logits_grad, float *loss, int *truth, int training, int num_classes, int size, int grad_size) {     float total_loss = 0;     int count = 0;      for (int i = 0; i < size / num_classes; i++) {         if (truth[i] < 0) continue;          count++;         float *logit = &logits_data[i * num_classes];         float max_logit = -1e30, sum_exp = 0;          for (int j = 0; j < num_classes; j++)             max_logit = fmax(max_logit, logit[j]);          for (int j = 0; j < num_classes; j++) {             logit[j] -= max_logit;             sum_exp += expf(logit[j]);         }          total_loss += logf(sum_exp) - logit[truth[i]];          if (training) {             for (int j = 0; j < num_classes; j++) {                 float prob = expf(logit[j]) / sum_exp;                 logits_grad[i * num_classes + j] = prob;             }              logits_grad[i * num_classes + truth[i]] -= 1.0;         }     }      *loss = total_loss / count;      if (training) {         for (int i = 0; i < grad_size; i++)             logits_grad[i] /= count;     } }  int main() {          int num_classes = 3;     int size = 9;       int grad_size = num_classes * (size / num_classes);     int *truth = (int *)malloc(size / num_classes * sizeof(int));      float *logits_data = (float *)malloc(size * sizeof(float));     float *logits_grad = (float *)malloc(grad_size * sizeof(float));     float loss;           for (int i = 0; i < size / num_classes; i++) {         truth[i] = i % num_classes;     }      for (int i = 0; i < size; i++) {         logits_data[i] = i * 0.1;       }           CrossEntropyLoss_forward(logits_data, logits_grad, &loss, truth, 1, num_classes, size, grad_size);           printf(\"Loss: %f\\n\", loss);      printf(\"Gradient:\\n\");     for (int i = 0; i < grad_size; i++) {         printf(\"%f \", logits_grad[i]);     }      free(truth);     free(logits_data);     free(logits_grad);      return 0; }   ",
        "cuda_code": "#include <stdio.h>   __global__ void cuda_Cross_py_forward_A_kernel(float *logits_data, float *logits_grad, bool training, int num_classes, int *truth, int *count, float *thread_loss, int size) {     int i = blockIdx.x * blockDim.x + threadIdx.x;          if (i < size) {         if (truth[i] < 0) {             count[i] = 0;             return;         }          float *logit = &logits_data[i * num_classes];         float max_logit = -1e30, sum_exp = 0;          for (int j = 0; j < num_classes; j++)             max_logit = fmax(max_logit, logit[j]);          for (int j = 0; j < num_classes; j++) {             logit[j] -= max_logit;             sum_exp += expf(logit[j]);         }          if (training) {             for (int j = 0; j < num_classes; j++) {                 float prob = expf(logit[j]) / sum_exp;                 logits_grad[i * num_classes + j] = prob;             }             logits_grad[i * num_classes + truth[i]] -= 1.0;         }          count[i] = 1;         thread_loss[i] = logf(sum_exp) - logit[truth[i]];     } }   void launch_kernel(float *logits_data, float *logits_grad, bool training, int num_classes, int *truth, int *count, float *thread_loss, int size) {          dim3 gridSize(<<<YourGridSizeX>>>, <<<YourGridSizeY>>>);     dim3 blockSize(<<<YourBlockSizeX>>>, <<<YourBlockSizeY>>>;           cuda_Cross_py_forward_A_kernel<<<gridSize, blockSize>>>(logits_data, logits_grad, training, num_classes, truth, count, thread_loss, size);           cudaDeviceSynchronize();           cudaError_t error = cudaGetLastError();     if (error != cudaSuccess) {         fprintf(stderr, \"CUDA error: %s\\n\", cudaGetErrorString(error));     } }  int main() {          int size = 100;       float *logits_data, *logits_grad, *thread_loss;     int *truth, *count;           cudaMalloc((void **)&logits_data, size * sizeof(float));     cudaMalloc((void **)&logits_grad, size * sizeof(float));     cudaMalloc((void **)&truth, size * sizeof(int));     cudaMalloc((void **)&count, size * sizeof(int));     cudaMalloc((void **)&thread_loss, size * sizeof(float));                 launch_kernel(logits_data, logits_grad, true, 10, truth, count, thread_loss, size);           cudaFree(logits_data);     cudaFree(logits_grad);     cudaFree(truth);     cudaFree(count);     cudaFree(thread_loss);      return 0; }   "
    },
    {
        "id": "356",
        "c_code": "#include <stdio.h>  void insert_sort(int a[], int n) {     for (int i = 1; i < n; i++) {         int j = 0;         while ((a[j] < a[i]) && (j < i)) {             j++;         }         if (i != j) {             int temp = a[i];             for (int k = i; k > j; k--) {                 a[k] = a[k - 1];             }             a[j] = temp;         }     } }  int main() {          int n = 5;     int a[] = {5, 3, 1, 4, 2};           printf(\"Original array: \");     for (int i = 0; i < n; i++) {         printf(\"%d \", a[i]);     }     printf(\"\\n\");           insert_sort(a, n);           printf(\"Sorted array: \");     for (int i = 0; i < n; i++) {         printf(\"%d \", a[i]);     }     printf(\"\\n\");      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h>   __global__ void cuInsertionSort(float *dist, long *ind, int width, int height, int k) {     int l, i, j;     float *p_dist;     long *p_ind;     float curr_dist, max_dist;     long curr_row, max_row;     unsigned int xIndex = blockIdx.x * blockDim.x + threadIdx.x;      if (xIndex < width) {         p_dist = dist + xIndex;         p_ind = ind + xIndex;         max_dist = p_dist[0];         p_ind[0] = 1;          for (l = 1; l < k; l++) {             curr_row = l * width;             curr_dist = p_dist[curr_row];              if (curr_dist < max_dist) {                 i = l - 1;                  for (int a = 0; a < l - 1; a++) {                     if (p_dist[a * width] > curr_dist) {                         i = a;                         break;                     }                 }                  for (j = l; j > i; j--) {                     p_dist[j * width] = p_dist[(j - 1) * width];                     p_ind[j * width] = p_ind[(j - 1) * width];                 }                  p_dist[i * width] = curr_dist;                 p_ind[i * width] = l + 1;             } else {                 p_ind[l * width] = l + 1;             }              max_dist = p_dist[curr_row];         }          max_row = (k - 1) * width;          for (l = k; l < height; l++) {             curr_dist = p_dist[l * width];              if (curr_dist < max_dist) {                 i = k - 1;                  for (int a = 0; a < k - 1; a++) {                     if (p_dist[a * width] > curr_dist) {                         i = a;                         break;                     }                 }                  for (j = k - 1; j > i; j--) {                     p_dist[j * width] = p_dist[(j - 1) * width];                     p_ind[j * width] = p_ind[(j - 1) * width];                 }                  p_dist[i * width] = curr_dist;                 p_ind[i * width] = l + 1;                 max_dist = p_dist[max_row];             }         }     } }   void launchKernel(float *dist, long *ind, int width, int height, int k) {     int blockSize = 256;     int numBlocks = (width + blockSize - 1) / blockSize;      cuInsertionSort<<<numBlocks, blockSize>>>(dist, ind, width, height, k);      cudaDeviceSynchronize();      cudaError_t cuda_err = cudaGetLastError();     if (cuda_err != cudaSuccess) {         fprintf(stderr, \"CUDA kernel launch error: %s\\n\", cudaGetErrorString(cuda_err));         exit(EXIT_FAILURE);     } }  int main() {     int width = 1024;     int height = 1024;     int k = 10;           float *hostDist = (float *)malloc(width * height * sizeof(float));     long *hostInd = (long *)malloc(width * height * sizeof(long));                 float *deviceDist;     long *deviceInd;      cudaMalloc((void **)&deviceDist, width * height * sizeof(float));     cudaMalloc((void **)&deviceInd, width * height * sizeof(long));           cudaMemcpy(deviceDist, hostDist, width * height * sizeof(float), cudaMemcpyHostToDevice);     cudaMemcpy(deviceInd, hostInd, width * height * sizeof(long), cudaMemcpyHostToDevice);           launchKernel(deviceDist, deviceInd, width, height, k);           cudaMemcpy(hostDist, deviceDist, width * height * sizeof(float), cudaMemcpyDeviceToHost);     cudaMemcpy(hostInd, deviceInd, width * height * sizeof(long), cudaMemcpyDeviceToHost);           cudaFree(deviceDist);     cudaFree(deviceInd);           free(hostDist);     free(hostInd);      return 0; }   "
    },
    {
        "id": "357",
        "c_code": "#include <stdio.h>  void conv2_cpu(float *A, float *kernel, int inputSize, int depth, int kernelSize, int stride, int pad, float *B, int outputSize) {     for (int i = 0; i < outputSize; i++) {         for (int j = 0; j < outputSize; j++) {             int Ai = i * stride;             int Aj = j * stride;             int startk = (pad - Ai) < 0 ? 0 : pad - Ai;             int endk = kernelSize < (inputSize + pad - Ai) ? kernelSize : (inputSize + pad - Ai);             int startl = (pad - Aj) < 0 ? 0 : pad - Aj;             int endl = kernelSize < (inputSize + pad - Aj) ? kernelSize : (inputSize + pad - Aj);             float sum = 0;             for (int d = 0; d < depth; d++) {                 for (int k = startk; k < endk; k++) {                     for (int l = startl; l < endl; l++) {                         sum += A[d * inputSize * inputSize + (Ai + k - pad) * inputSize + Aj + l - pad] * kernel[d * kernelSize * kernelSize + k * kernelSize + l];                     }                 }                 B[d * outputSize * outputSize + i * outputSize + j] = sum;             }             B[i * outputSize + j] = sum;         }     } }  int main() {          int inputSize = 4;     int depth = 3;     int kernelSize = 3;     int stride = 1;     int pad = 1;     int outputSize = (inputSize - kernelSize + 2 * pad) / stride + 1;      float A[3 * 4 * 4];      float kernel[3 * 3 * 3];      float B[3 * 3 * 3];                  conv2_cpu(A, kernel, inputSize, depth, kernelSize, stride, pad, B, outputSize);           for (int d = 0; d < depth; d++) {         printf(\"Depth %d:\\n\", d);         for (int i = 0; i < outputSize; i++) {             for (int j = 0; j < outputSize; j++) {                 printf(\"%f \", B[d * outputSize * outputSize + i * outputSize + j]);             }             printf(\"\\n\");         }         printf(\"\\n\");     }      return 0; }   ",
        "cuda_code": "#include <stdio.h>  __global__ void conv2(float *A, float *kernel, int inputSize, int depth, int kernelSize, int stride, int pad, float *B, int outputSize) {     int i = threadIdx.x + blockDim.x * blockIdx.x;     int j = threadIdx.y + blockDim.y * blockIdx.y;      if (!(i < outputSize) || !(j < outputSize))         return;      int Ai = i * stride;     int Aj = j * stride;     int startk = (pad - Ai) < 0 ? 0 : pad - Ai;     int endk = kernelSize < (inputSize + pad - Ai) ? kernelSize : (inputSize + pad - Ai);     int startl = (pad - Aj) < 0 ? 0 : pad - Aj;     int endl = kernelSize < (inputSize + pad - Aj) ? kernelSize : (inputSize + pad - Aj);      for (int d = 0; d < depth; d++) {         float sum = 0;          for (int k = startk; k < endk; k++) {             for (int l = startl; l < endl; l++) {                 sum += A[d * inputSize * inputSize + (Ai + k - pad) * inputSize + Aj + l - pad] * kernel[d * kernelSize * kernelSize + k * kernelSize + l];             }         }          B[d * outputSize * outputSize + i * outputSize + j] = sum;     } }  int main() {          int inputSize = 5, depth = 3, kernelSize = 3, stride = 1, pad = 1, outputSize = (inputSize - kernelSize + 2 * pad) / stride + 1;     int A_size = depth * inputSize * inputSize;     int kernel_size = depth * kernelSize * kernelSize;     int B_size = depth * outputSize * outputSize;      float *d_A, *d_kernel, *d_B;           cudaMalloc((void **)&d_A, A_size * sizeof(float));     cudaMalloc((void **)&d_kernel, kernel_size * sizeof(float));     cudaMalloc((void **)&d_B, B_size * sizeof(float));                 dim3 gridSize(<<<YourGridSizeX>>>, <<<YourGridSizeY>>>, 1);     dim3 blockSize(<<<YourBlockSizeX>>>, <<<YourBlockSizeY>>>, 1);           conv2<<<gridSize, blockSize>>>(d_A, d_kernel, inputSize, depth, kernelSize, stride, pad, d_B, outputSize);           cudaDeviceSynchronize();           cudaFree(d_A);     cudaFree(d_kernel);     cudaFree(d_B);      return 0; }   "
    },
    {
        "id": "358",
        "c_code": "#include <stdio.h>  void get_positive_data_cpu(const float *all_box, const float *all_scores, const float *all_conf, const int *conf_inds, float *positive_box, float *positive_scores, float *positive_conf, int dims, int clsNum) {     for (int tid = 0; tid < dims; tid++) {         if (conf_inds[tid] != (-1)) {             positive_box[tid * 4 + 0] = all_box[tid * 4 + 0];             positive_box[tid * 4 + 1] = all_box[tid * 4 + 1];             positive_box[tid * 4 + 2] = all_box[tid * 4 + 2];             positive_box[tid * 4 + 3] = all_box[tid * 4 + 3];             for (int i = 0; i < clsNum; i++) {                 positive_scores[tid * clsNum + i] = all_scores[tid * clsNum + i];             }             positive_conf[tid] = all_conf[tid];         } else {             positive_box[tid * 4 + 0] = 0;             positive_box[tid * 4 + 1] = 0;             positive_box[tid * 4 + 2] = 0;             positive_box[tid * 4 + 3] = 0;             for (int i = 0; i < clsNum; i++) {                 positive_scores[tid * clsNum + i] = (-1);             }             positive_conf[tid] = (-1);         }     } }  int main() {          int dims = 5;     int clsNum = 3;      float all_box[5 * 4];      float all_scores[5 * clsNum];      float all_conf[5];     int conf_inds[5];      float positive_box[5 * 4];     float positive_scores[5 * clsNum];     float positive_conf[5];                 get_positive_data_cpu(all_box, all_scores, all_conf, conf_inds, positive_box, positive_scores, positive_conf, dims, clsNum);           for (int i = 0; i < dims; i++) {         printf(\"Example %d:\\n\", i);         printf(\"Positive Box: [%f, %f, %f, %f]\\n\", positive_box[i * 4 + 0], positive_box[i * 4 + 1], positive_box[i * 4 + 2], positive_box[i * 4 + 3]);         printf(\"Positive Scores: \");         for (int j = 0; j < clsNum; j++) {             printf(\"%f \", positive_scores[i * clsNum + j]);         }         printf(\"\\nPositive Confidence: %f\\n\", positive_conf[i]);         printf(\"\\n\");     }      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h>  #define THREADS_PER_BLOCK 256  __global__ void get_positive_data(const float *all_box, const float *all_scores, const float *all_conf,                                    const int *conf_inds, float *positive_box, float *positive_scores,                                    float *positive_conf, int dims, int clsNum) {     int tid = blockIdx.x * blockDim.x + threadIdx.x;     if (tid >= dims) {         return;     }      if (conf_inds[tid] != (-1)) {         positive_box[tid * 4 + 0] = all_box[tid * 4 + 0];         positive_box[tid * 4 + 1] = all_box[tid * 4 + 1];         positive_box[tid * 4 + 2] = all_box[tid * 4 + 2];         positive_box[tid * 4 + 3] = all_box[tid * 4 + 3];          for (int i = 0; i < clsNum; i++) {             positive_scores[tid * clsNum + i] = all_scores[tid * clsNum + i];         }          positive_conf[tid] = all_conf[tid];     } else {         positive_box[tid * 4 + 0] = 0;         positive_box[tid * 4 + 1] = 0;         positive_box[tid * 4 + 2] = 0;         positive_box[tid * 4 + 3] = 0;          for (int i = 0; i < clsNum; i++) {             positive_scores[tid * clsNum + i] = (-1);         }          positive_conf[tid] = (-1);     } }  int main() {          int dims = 1000;     int clsNum = 5;           float *all_box_host = (float *)malloc(dims * 4 * sizeof(float));     float *all_scores_host = (float *)malloc(dims * clsNum * sizeof(float));     float *all_conf_host = (float *)malloc(dims * sizeof(float));     int *conf_inds_host = (int *)malloc(dims * sizeof(int));      float *positive_box_host = (float *)malloc(dims * 4 * sizeof(float));     float *positive_scores_host = (float *)malloc(dims * clsNum * sizeof(float));     float *positive_conf_host = (float *)malloc(dims * sizeof(float));                 float *all_box_device, *all_scores_device, *all_conf_device;     int *conf_inds_device;     float *positive_box_device, *positive_scores_device, *positive_conf_device;      cudaMalloc((void **)&all_box_device, dims * 4 * sizeof(float));     cudaMalloc((void **)&all_scores_device, dims * clsNum * sizeof(float));     cudaMalloc((void **)&all_conf_device, dims * sizeof(float));     cudaMalloc((void **)&conf_inds_device, dims * sizeof(int));      cudaMalloc((void **)&positive_box_device, dims * 4 * sizeof(float));     cudaMalloc((void **)&positive_scores_device, dims * clsNum * sizeof(float));     cudaMalloc((void **)&positive_conf_device, dims * sizeof(float));                 dim3 gridDim((dims + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK, 1, 1);     dim3 blockDim(THREADS_PER_BLOCK, 1, 1);      get_positive_data<<<gridDim, blockDim>>>(all_box_device, all_scores_device, all_conf_device,                                              conf_inds_device, positive_box_device,                                              positive_scores_device, positive_conf_device, dims,                                              clsNum);                  free(all_box_host);     free(all_scores_host);     free(all_conf_host);     free(conf_inds_host);      free(positive_box_host);     free(positive_scores_host);     free(positive_conf_host);      cudaFree(all_box_device);     cudaFree(all_scores_device);     cudaFree(all_conf_device);     cudaFree(conf_inds_device);      cudaFree(positive_box_device);     cudaFree(positive_scores_device);     cudaFree(positive_conf_device);      return 0; }   "
    },
    {
        "id": "359",
        "c_code": "#include <stdio.h> #include <math.h>  void cpuChoiLee(float *xi, float *xq, float *sr, float *si, int N, float *L);  int main() {          int N = 10;     float xi[N], xq[N], sr[N], si[N], L[N];           for (int i = 0; i < N; ++i) {         xi[i] = i + 1;         xq[i] = i + 2;         sr[i] = i + 3;         si[i] = i + 4;     }           cpuChoiLee(xi, xq, sr, si, N, L);           printf(\"Results:\\n\");     for (int i = 0; i < N; ++i) {         printf(\"L[%d] = %f\\n\", i, L[i]);     }      return 0; }  void cpuChoiLee(float *xi, float *xq, float *sr, float *si, int N, float *L) {     for (int u = 0; u < N; u++) {         float uSum = 0;         float r_i, r_q, rconj_i, rconj_q;         float s_i, s_q, sconj_i, sconj_q;         float rsum_i, rsum_q, ssum_i, ssum_q;         float ksum_i, ksum_q;          for (int i = 0; i < N; i++) {             ksum_i = 0;             ksum_q = 0;              for (int k = 0; k < N - i; k++) {                 r_i = xi[u + k + i];                 r_q = xq[u + k + i];                 rconj_i = xi[u + k];                 rconj_q = xq[u + k] * (-1);                 s_i = sr[k];                 s_q = si[k];                 sconj_i = sr[k + i];                 sconj_q = si[k + i] * (-1);                 rsum_i = (r_i * rconj_i) - (r_q * rconj_q);                 rsum_q = (r_i * rconj_q) + (r_q * rconj_i);                 ssum_i = (s_i * sconj_i) - (s_q * sconj_q);                 ssum_q = (s_i * sconj_q) + (s_q * sconj_i);                 ksum_i += (rsum_i * ssum_i) - (rsum_q * ssum_q);                 ksum_q += (rsum_i * ssum_q) + (rsum_q * ssum_i);             }              uSum += sqrt((ksum_i * ksum_i) + (ksum_q * ksum_q));         }          L[u] = uSum;     } } ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h> #include <math.h> #include <cuda_runtime.h>  #define THREADS_PER_BLOCK 256  __global__ void cudaChoiLee(float *xi, float *xq, float *sr, float *si, int N, float *L) {     int u = (blockIdx.x * blockDim.x) + threadIdx.x;     if (u >= N)         return;      float uSum = 0;     float r_i, r_q, rconj_i, rconj_q;     float s_i, s_q, sconj_i, sconj_q;     float rsum_i, rsum_q, ssum_i, ssum_q;     float ksum_i, ksum_q;      for (int i = 0; i < N; i++) {         ksum_i = 0;         ksum_q = 0;          for (int k = 0; k < N - i; k++) {             r_i = xi[u + k + i];             r_q = xq[u + k + i];             rconj_i = xi[u + k];             rconj_q = xq[u + k] * (-1);              s_i = sr[k];             s_q = si[k];             sconj_i = sr[k + i];             sconj_q = si[k + i] * (-1);              rsum_i = (r_i * rconj_i) - (r_q * rconj_q);             rsum_q = (r_i * rconj_q) + (r_q * rconj_i);             ssum_i = (s_i * sconj_i) - (s_q * sconj_q);             ssum_q = (s_i * sconj_q) + (s_q * sconj_i);              ksum_i += (rsum_i * ssum_i) - (rsum_q * ssum_q);             ksum_q += (rsum_i * ssum_q) + (rsum_q * ssum_i);         }          uSum += sqrt((ksum_i * ksum_i) + (ksum_q * ksum_q));     }      L[u] = uSum; }  int main() {          int N = 1000;           float *xi_host = (float *)malloc(N * sizeof(float));     float *xq_host = (float *)malloc(N * sizeof(float));     float *sr_host = (float *)malloc(N * sizeof(float));     float *si_host = (float *)malloc(N * sizeof(float));     float *L_host = (float *)malloc(N * sizeof(float));                 float *xi_device, *xq_device, *sr_device, *si_device, *L_device;      cudaMalloc((void **)&xi_device, N * sizeof(float));     cudaMalloc((void **)&xq_device, N * sizeof(float));     cudaMalloc((void **)&sr_device, N * sizeof(float));     cudaMalloc((void **)&si_device, N * sizeof(float));     cudaMalloc((void **)&L_device, N * sizeof(float));                 dim3 gridDim((N + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK, 1, 1);     dim3 blockDim(THREADS_PER_BLOCK, 1, 1);      cudaChoiLee<<<gridDim, blockDim>>>(xi_device, xq_device, sr_device, si_device, N, L_device);                  free(xi_host);     free(xq_host);     free(sr_host);     free(si_host);     free(L_host);      cudaFree(xi_device);     cudaFree(xq_device);     cudaFree(sr_device);     cudaFree(si_device);     cudaFree(L_device);      return 0; }   "
    },
    {
        "id": "360",
        "c_code": "#include <stdio.h> #include <math.h>   void calculateOuterSumsNew_cpu(float *innerSums, float *L, int uLength) {     for (int u = 0; u < uLength; u++) {         float real, imag, u_sum;         int realIdx = 2 * u;         int imagIdx = realIdx + 1;          real = innerSums[realIdx];         imag = innerSums[imagIdx];         u_sum = (real * real) + (imag * imag);          realIdx += 64;         imagIdx += 64;         real = innerSums[realIdx];         imag = innerSums[imagIdx];         u_sum += (real * real) + (imag * imag);          realIdx += 64;         imagIdx += 64;         real = innerSums[realIdx];         imag = innerSums[imagIdx];         u_sum += (real * real) + (imag * imag);          realIdx += 64;         imagIdx += 64;         real = innerSums[realIdx];         imag = innerSums[imagIdx];         u_sum += (real * real) + (imag * imag);          realIdx += 64;         imagIdx += 64;         real = innerSums[realIdx];         imag = innerSums[imagIdx];         u_sum += (real * real) + (imag * imag);          realIdx += 64;         imagIdx += 64;         real = innerSums[realIdx];         imag = innerSums[imagIdx];         u_sum += (real * real) + (imag * imag);          realIdx += 64;         imagIdx += 64;         real = innerSums[realIdx];         imag = innerSums[imagIdx];         u_sum += (real * real) + (imag * imag);          realIdx += 64;         imagIdx += 64;         real = innerSums[realIdx];         imag = innerSums[imagIdx];         u_sum += (real * real) + (imag * imag);          L[u] = u_sum;     } }   int main() {     const int uLength = 10;       float innerSums[uLength * 2];             for (int i = 0; i < uLength * 2; ++i) {         innerSums[i] = (float)i;     }      float L[uLength];           calculateOuterSumsNew_cpu(innerSums, L, uLength);           printf(\"Result: \");     for (int u = 0; u < uLength; ++u) {         printf(\"%f \", L[u]);     }     printf(\"\\n\");      return 0; }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h> #include <cuda_runtime.h>  #define THREADS_PER_BLOCK 256  __global__ void calculateOuterSumsNew(float *innerSums, float *L, int uLength) {     int u = blockDim.x * blockIdx.x + threadIdx.x;     if (u >= uLength)         return;      float real, imag, u_sum = 0.0f;      for (int i = 0; i < 8; i++) {         int realIdx = 2 * (u + i * 64);         int imagIdx = realIdx + 1;          real = innerSums[realIdx];         imag = innerSums[imagIdx];          u_sum += (real * real) + (imag * imag);     }      L[u] = u_sum; }  int main() {          int uLength = 1000;           float *innerSums_host = (float *)malloc(2 * uLength * sizeof(float));     float *L_host = (float *)malloc(uLength * sizeof(float));                 float *innerSums_device, *L_device;      cudaMalloc((void **)&innerSums_device, 2 * uLength * sizeof(float));     cudaMalloc((void **)&L_device, uLength * sizeof(float));                 dim3 gridDim((uLength + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK, 1, 1);     dim3 blockDim(THREADS_PER_BLOCK, 1, 1);      calculateOuterSumsNew<<<gridDim, blockDim>>>(innerSums_device, L_device, uLength);                  free(innerSums_host);     free(L_host);      cudaFree(innerSums_device);     cudaFree(L_device);      return 0; }   "
    },
    {
        "id": "361",
        "c_code": " #include <stdio.h>  void nlf_right_forward_cpu(const int n, const float *filters, const int channel, const int height, const int width, const int wsize, float *top_data);  int main() {          int n = 1;     int channel = 1;     int height = 3;     int width = 3;     int wsize = 5;      float filters[] = {              };      float top_data[n * height * width];             nlf_right_forward_cpu(n, filters, channel, height, width, wsize, top_data);           printf(\"Results:\\n\");     for (int i = 0; i < n; ++i) {         for (int j = 0; j < height; ++j) {             for (int k = 0; k < width; ++k) {                 printf(\"%f \", top_data[i * height * width + j * width + k]);             }             printf(\"\\n\");         }         printf(\"\\n\");     }      return 0; }  void nlf_right_forward_cpu(const int n, const float *filters, const int channel, const int height, const int width, const int wsize, float *top_data) {     for (int index = 0; index < n; index++) {         int step = height * width;         int base = index * step;         int fbase = index / channel * wsize * step;          for (int col = 0; col < width; col++) {             for (int row = 0; row < height; row++) {                 float temp = 0;                 int r, c, shift;                  r = row;                 c = col;                 shift = 0 * step + row * width + col;                 temp += top_data[base + r * width + c] * filters[fbase + shift];                  r = row;                 c = col - 1;                 shift = 1 * step + row * width + col;                 if (c >= 0)                     temp += top_data[base + r * width + c] * filters[fbase + shift];                 else                     temp += top_data[base + row * width + col] * filters[fbase + shift];                  r = row - 1;                 c = col - 1;                 shift = 2 * step + row * width + col;                 if (c >= 0 && r >= 0)                     temp += top_data[base + r * width + c] * filters[fbase + shift];                 else                     temp += top_data[base + row * width + col] * filters[fbase + shift];                  r = row + 1;                 c = col - 1;                 shift = 3 * step + row * width + col;                 if (c >= 0 && r < height)                     temp += top_data[base + r * width + c] * filters[fbase + shift];                 else                     temp += top_data[base + row * width + col] * filters[fbase + shift];                  r = row - 1;                 c = col;                 shift = 4 * step + row * width + col;                 if (r >= 0)                     temp += top_data[base + r * width + c] * filters[fbase + shift];                 else                     temp += top_data[base + row * width + col] * filters[fbase + shift];                  top_data[base + row * width + col] = temp;             }         }     } }  ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h> #include <cuda_runtime.h>  #define THREADS_PER_BLOCK 256  __global__ void nlf_right_forward(const int n, const float *filters, const int channel,                                    const int height, const int width, const int wsize,                                    float *top_data) {     int index = blockIdx.x * blockDim.x + threadIdx.x;     if (index >= n) {         return;     }      int step = height * width;     int base = index * step;     int fbase = index / channel * wsize * step;      for (int col = 0; col < width; col++) {         for (int row = 0; row < height; row++) {             float temp = 0;             int r, c, shift;                           r = row;             c = col;             shift = 0 * step + row * width + col;             temp += top_data[base + r * width + c] * filters[fbase + shift];                           r = row;             c = col - 1;             shift = 1 * step + row * width + col;             if (c >= 0)                 temp += top_data[base + r * width + c] * filters[fbase + shift];             else                 temp += top_data[base + row * width + col] * filters[fbase + shift];                           r = row - 1;             c = col - 1;             shift = 2 * step + row * width + col;             if (c >= 0 && r >= 0)                 temp += top_data[base + r * width + c] * filters[fbase + shift];             else                 temp += top_data[base + row * width + col] * filters[fbase + shift];                           r = row + 1;             c = col - 1;             shift = 3 * step + row * width + col;             if (c >= 0 && r < height)                 temp += top_data[base + r * width + c] * filters[fbase + shift];             else                 temp += top_data[base + row * width + col] * filters[fbase + shift];                           r = row - 1;             c = col;             shift = 4 * step + row * width + col;             if (r >= 0)                 temp += top_data[base + r * width + c] * filters[fbase + shift];             else                 temp += top_data[base + row * width + col] * filters[fbase + shift];              top_data[base + row * width + col] = temp;         }     } }  int main() {          int n = 1000;     int channel = 3;     int height = 32;     int width = 32;     int wsize = 5;           float *filters_host = (float *)malloc(n * channel * wsize * sizeof(float));     float *top_data_host = (float *)malloc(n * height * width * sizeof(float));                 float *filters_device, *top_data_device;      cudaMalloc((void **)&filters_device, n * channel * wsize * sizeof(float));     cudaMalloc((void **)&top_data_device, n * height * width * sizeof(float));                 dim3 gridDim((n + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK, 1, 1);     dim3 blockDim(THREADS_PER_BLOCK, 1, 1);      nlf_right_forward<<<gridDim, blockDim>>>(n, filters_device, channel, height, width, wsize,                                              top_data_device);                  free(filters_host);     free(top_data_host);      cudaFree(filters_device);     cudaFree(top_data_device);      return 0; }   "
    },
    {
        "id": "362",
        "c_code": "#include <stdio.h>  void nlf_filter_right_backward_cpu(const int n, const float *bottom_data, const float *top_data, const float *temp_diff, const int channel, const int height, const int width, const int wsize, float *filters_diff);  int main() {          int n = 1;     int channel = 1;     int height = 3;     int width = 3;     int wsize = 5;      float bottom_data[n * channel * height * width];       float top_data[n * channel * height * width];          float temp_diff[n * channel * height * width];         float filters_diff[n * channel * wsize * height * width];             nlf_filter_right_backward_cpu(n, bottom_data, top_data, temp_diff, channel, height, width, wsize, filters_diff);           printf(\"Results:\\n\");     for (int i = 0; i < n; ++i) {         for (int j = 0; j < channel; ++j) {             for (int k = 0; k < wsize; ++k) {                 for (int m = 0; m < height; ++m) {                     for (int n = 0; n < width; ++n) {                         printf(\"%f \", filters_diff[i * channel * wsize * height * width + j * wsize * height * width + k * height * width + m * width + n]);                     }                     printf(\"\\n\");                 }                 printf(\"\\n\");             }         }     }      return 0; }  void nlf_filter_right_backward_cpu(const int n, const float *bottom_data, const float *top_data, const float *temp_diff, const int channel, const int height, const int width, const int wsize, float *filters_diff) {     for (int index = 0; index < n; index++) {         int step = height * width;         int base = index / step * step * channel + index % step;         int fbase = index / step * step * wsize + index % step;         int row = index % step / width;         int col = index % step % width;          for (int i = 0; i < channel; i++) {             filters_diff[fbase] += temp_diff[base + i * step] * bottom_data[base + i * step];              if (col - 1 >= 0)                 filters_diff[fbase + step] += temp_diff[base + i * step] * top_data[base - 1 + i * step];             else                 filters_diff[fbase + step] += temp_diff[base + i * step] * bottom_data[base + i * step];              if (col - 1 >= 0 && row - 1 >= 0)                 filters_diff[fbase + 2 * step] += temp_diff[base + i * step] * top_data[base - width - 1 + i * step];             else                 filters_diff[fbase + 2 * step] += temp_diff[base + i * step] * bottom_data[base + i * step];              if (col - 1 >= 0 && row + 1 < height)                 filters_diff[fbase + 3 * step] += temp_diff[base + i * step] * top_data[base + width - 1 + i * step];             else                 filters_diff[fbase + 3 * step] += temp_diff[base + i * step] * bottom_data[base + i * step];              if (row - 1 >= 0)                 filters_diff[fbase + 4 * step] += temp_diff[base + i * step] * top_data[base - width + i * step];             else                 filters_diff[fbase + 4 * step] += temp_diff[base + i * step] * bottom_data[base + i * step];         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h> #include <cuda_runtime.h>  #define THREADS_PER_BLOCK 256  __global__ void nlf_filter_right_backward(const int n, const float *bottom_data, const float *top_data,                                            const float *temp_diff, const int channel,                                            const int height, const int width, const int wsize,                                            float *filters_diff) {     int index = blockIdx.x * blockDim.x + threadIdx.x;     if (index >= n) {         return;     }      int step = height * width;     int base = index / step * step * channel + index % step;     int fbase = index / step * step * wsize + index % step;     int row = index % step / width;     int col = index % step % width;      for (int i = 0; i < channel; i++) {         filters_diff[fbase] += temp_diff[base + i * step] * bottom_data[base + i * step];          if (col - 1 >= 0)             filters_diff[fbase + step] += temp_diff[base + i * step] * top_data[base - 1 + i * step];         else             filters_diff[fbase + step] += temp_diff[base + i * step] * bottom_data[base + i * step];          if (col - 1 >= 0 && row - 1 >= 0)             filters_diff[fbase + 2 * step] +=                 temp_diff[base + i * step] * top_data[base - width - 1 + i * step];         else             filters_diff[fbase + 2 * step] += temp_diff[base + i * step] * bottom_data[base + i * step];          if (col - 1 >= 0 && row + 1 < height)             filters_diff[fbase + 3 * step] +=                 temp_diff[base + i * step] * top_data[base + width - 1 + i * step];         else             filters_diff[fbase + 3 * step] += temp_diff[base + i * step] * bottom_data[base + i * step];          if (row - 1 >= 0)             filters_diff[fbase + 4 * step] +=                 temp_diff[base + i * step] * top_data[base - width + i * step];         else             filters_diff[fbase + 4 * step] += temp_diff[base + i * step] * bottom_data[base + i * step];     } }  int main() {          int n = 1000;     int channel = 3;     int height = 32;     int width = 32;     int wsize = 5;           float *bottom_data_host = (float *)malloc(n * channel * height * width * sizeof(float));     float *top_data_host = (float *)malloc(n * channel * height * width * sizeof(float));     float *temp_diff_host = (float *)malloc(n * channel * height * width * sizeof(float));     float *filters_diff_host = (float *)malloc(n * channel * wsize * sizeof(float));                 float *bottom_data_device, *top_data_device, *temp_diff_device, *filters_diff_device;      cudaMalloc((void **)&bottom_data_device, n * channel * height * width * sizeof(float));     cudaMalloc((void **)&top_data_device, n * channel * height * width * sizeof(float));     cudaMalloc((void **)&temp_diff_device, n * channel * height * width * sizeof(float));     cudaMalloc((void **)&filters_diff_device, n * channel * wsize * sizeof(float));                 dim3 gridDim((n + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK, 1, 1);     dim3 blockDim(THREADS_PER_BLOCK, 1, 1);      nlf_filter_right_backward<<<gridDim, blockDim>>>(n, bottom_data_device, top_data_device,                                                      temp_diff_device, channel, height, width,                                                      wsize, filters_diff_device);                  free(bottom_data_host);     free(top_data_host);     free(temp_diff_host);     free(filters_diff_host);      cudaFree(bottom_data_device);     cudaFree(top_data_device);     cudaFree(temp_diff_device);     cudaFree(filters_diff_device);      return 0; }   "
    },
    {
        "id": "363",
        "c_code": "#include <stdio.h>  void nlf_filter_up_backward_cpu(const int n, const float *bottom_data, const float *top_data, const float *temp_diff, const int channel, const int height, const int width, const int wsize, float *filters_diff);  int main() {          int n = 1;     int channel = 1;     int height = 3;     int width = 3;     int wsize = 5;      float bottom_data[n * channel * height * width];       float top_data[n * channel * height * width];          float temp_diff[n * channel * height * width];         float filters_diff[n * channel * wsize * height * width];             nlf_filter_up_backward_cpu(n, bottom_data, top_data, temp_diff, channel, height, width, wsize, filters_diff);           printf(\"Results:\\n\");     for (int i = 0; i < n; ++i) {         for (int j = 0; j < channel; ++j) {             for (int k = 0; k < wsize; ++k) {                 for (int m = 0; m < height; ++m) {                     for (int n = 0; n < width; ++n) {                         printf(\"%f \", filters_diff[i * channel * wsize * height * width + j * wsize * height * width + k * height * width + m * width + n]);                     }                     printf(\"\\n\");                 }                 printf(\"\\n\");             }         }     }      return 0; }  void nlf_filter_up_backward_cpu(const int n, const float *bottom_data, const float *top_data, const float *temp_diff, const int channel, const int height, const int width, const int wsize, float *filters_diff) {     for (int index = 0; index < n; index++) {         int step = height * width;         int base = index / step * step * channel + index % step;         int fbase = index / step * step * wsize + index % step;         int row = index % step / width;         int col = index % step % width;          for (int i = 0; i < channel; i++) {             filters_diff[fbase] += temp_diff[base + i * step] * bottom_data[base + i * step];              if (row + 1 < height)                 filters_diff[fbase + step] += temp_diff[base + i * step] * top_data[base + width + i * step];             else                 filters_diff[fbase + step] += temp_diff[base + i * step] * bottom_data[base + i * step];              if (row + 1 < height && col - 1 >= 0)                 filters_diff[fbase + 2 * step] += temp_diff[base + i * step] * top_data[base + width - 1 + i * step];             else                 filters_diff[fbase + 2 * step] += temp_diff[base + i * step] * bottom_data[base + i * step];              if (row + 1 < height && col + 1 < width)                 filters_diff[fbase + 3 * step] += temp_diff[base + i * step] * top_data[base + width + 1 + i * step];             else                 filters_diff[fbase + 3 * step] += temp_diff[base + i * step] * bottom_data[base + i * step];              if (col + 1 < width)                 filters_diff[fbase + 4 * step] += temp_diff[base + i * step] * top_data[base + 1 + i * step];             else                 filters_diff[fbase + 4 * step] += temp_diff[base + i * step] * bottom_data[base + i * step];         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h> #include <cuda_runtime.h>  #define THREADS_PER_BLOCK 256  __global__ void nlf_filter_up_backward(const int n, const float *bottom_data, const float *top_data,                                         const float *temp_diff, const int channel,                                         const int height, const int width, const int wsize,                                         float *filters_diff) {     int index = blockIdx.x * blockDim.x + threadIdx.x;     if (index >= n) {         return;     }      int step = height * width;     int base = index / step * step * channel + index % step;     int fbase = index / step * step * wsize + index % step;     int row = index % step / width;     int col = index % step % width;      for (int i = 0; i < channel; i++) {         filters_diff[fbase] += temp_diff[base + i * step] * bottom_data[base + i * step];          if (row + 1 < height)             filters_diff[fbase + step] += temp_diff[base + i * step] * top_data[base + width + i * step];         else             filters_diff[fbase + step] += temp_diff[base + i * step] * bottom_data[base + i * step];          if (row + 1 < height && col - 1 >= 0)             filters_diff[fbase + 2 * step] +=                 temp_diff[base + i * step] * top_data[base + width - 1 + i * step];         else             filters_diff[fbase + 2 * step] += temp_diff[base + i * step] * bottom_data[base + i * step];          if (row + 1 < height && col + 1 < width)             filters_diff[fbase + 3 * step] +=                 temp_diff[base + i * step] * top_data[base + width + 1 + i * step];         else             filters_diff[fbase + 3 * step] += temp_diff[base + i * step] * bottom_data[base + i * step];          if (col + 1 < width)             filters_diff[fbase + 4 * step] += temp_diff[base + i * step] * top_data[base + 1 + i * step];         else             filters_diff[fbase + 4 * step] += temp_diff[base + i * step] * bottom_data[base + i * step];     } }  int main() {          int n = 1000;     int channel = 3;     int height = 32;     int width = 32;     int wsize = 5;           float *bottom_data_host = (float *)malloc(n * channel * height * width * sizeof(float));     float *top_data_host = (float *)malloc(n * channel * height * width * sizeof(float));     float *temp_diff_host = (float *)malloc(n * channel * height * width * sizeof(float));     float *filters_diff_host = (float *)malloc(n * channel * wsize * sizeof(float));                 float *bottom_data_device, *top_data_device, *temp_diff_device, *filters_diff_device;      cudaMalloc((void **)&bottom_data_device, n * channel * height * width * sizeof(float));     cudaMalloc((void **)&top_data_device, n * channel * height * width * sizeof(float));     cudaMalloc((void **)&temp_diff_device, n * channel * height * width * sizeof(float));     cudaMalloc((void **)&filters_diff_device, n * channel * wsize * sizeof(float));                 dim3 gridDim((n + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK, 1, 1);     dim3 blockDim(THREADS_PER_BLOCK, 1, 1);      nlf_filter_up_backward<<<gridDim, blockDim>>>(n, bottom_data_device, top_data_device,                                                    temp_diff_device, channel, height, width,                                                    wsize, filters_diff_device);                  free(bottom_data_host);     free(top_data_host);     free(temp_diff_host);     free(filters_diff_host);      cudaFree(bottom_data_device);     cudaFree(top_data_device);     cudaFree(temp_diff_device);     cudaFree(filters_diff_device);      return 0; }   "
    },
    {
        "id": "364",
        "c_code": "#include <stdio.h>  void nlf_left_forward_cpu(const int n, const float *filters, const int channel, const int height, const int width, const int wsize, float *top_data);  int main() {          int n = 1;     int channel = 1;     int height = 3;     int width = 3;     int wsize = 5;      float filters[] = {              };      float top_data[n * height * width];             nlf_left_forward_cpu(n, filters, channel, height, width, wsize, top_data);           printf(\"Results:\\n\");     for (int i = 0; i < n; ++i) {         for (int j = 0; j < height; ++j) {             for (int k = 0; k < width; ++k) {                 printf(\"%f \", top_data[i * height * width + j * width + k]);             }             printf(\"\\n\");         }         printf(\"\\n\");     }      return 0; }  void nlf_left_forward_cpu(const int n, const float *filters, const int channel, const int height, const int width, const int wsize, float *top_data) {     for (int index = 0; index < n; index++) {         int step = height * width;         int base = index * step;         int fbase = index / channel * wsize * step;          for (int col = width - 1; col >= 0; col--) {             for (int row = height - 1; row >= 0; row--) {                 float temp = 0;                 int r, c, shift;                  r = row;                 c = col;                 shift = 0 * step + row * width + col;                 temp += top_data[base + r * width + c] * filters[fbase + shift];                  r = row;                 c = col + 1;                 shift = 1 * step + row * width + col;                 if (c < width)                     temp += top_data[base + r * width + c] * filters[fbase + shift];                 else                     temp += top_data[base + row * width + col] * filters[fbase + shift];                  r = row - 1;                 c = col + 1;                 shift = 2 * step + row * width + col;                 if (c < width && r >= 0)                     temp += top_data[base + r * width + c] * filters[fbase + shift];                 else                     temp += top_data[base + row * width + col] * filters[fbase + shift];                  r = row + 1;                 c = col + 1;                 shift = 3 * step + row * width + col;                 if (c < width && r < height)                     temp += top_data[base + r * width + c] * filters[fbase + shift];                 else                     temp += top_data[base + row * width + col] * filters[fbase + shift];                  r = row + 1;                 c = col;                 shift = 4 * step + row * width + col;                 if (r < height)                     temp += top_data[base + r * width + c] * filters[fbase + shift];                 else                     temp += top_data[base + row * width + col] * filters[fbase + shift];                  top_data[base + row * width + col] = temp;             }         }     } }   ",
        "cuda_code": "#include <stdio.h> #include <stdlib.h> #include <cuda_runtime.h>  #define THREADS_PER_BLOCK 256  __global__ void nlf_left_forward(const int n, const float *filters, const int channel,                                  const int height, const int width, const int wsize,                                  float *top_data) {     int index = blockIdx.x * blockDim.x + threadIdx.x;     if (index >= n) {         return;     }      int step = height * width;     int base = index * step;     int fbase = index / channel * wsize * step;      for (int col = width - 1; col >= 0; col--) {         for (int row = height - 1; row >= 0; row--) {             float temp = 0;             int r, c, shift;                           r = row;             c = col;             shift = 0 * step + row * width + col;             temp += top_data[base + r * width + c] * filters[fbase + shift];                           r = row;             c = col + 1;             shift = 1 * step + row * width + col;             if (c < width)                 temp += top_data[base + r * width + c] * filters[fbase + shift];             else                 temp += top_data[base + row * width + col] * filters[fbase + shift];                           r = row - 1;             c = col + 1;             shift = 2 * step + row * width + col;             if (c < width && r >= 0)                 temp += top_data[base + r * width + c] * filters[fbase + shift];             else                 temp += top_data[base + row * width + col] * filters[fbase + shift];                           r = row + 1;             c = col + 1;             shift = 3 * step + row * width + col;             if (c < width && r < height)                 temp += top_data[base + r * width + c] * filters[fbase + shift];             else                 temp += top_data[base + row * width + col] * filters[fbase + shift];                           r = row + 1;             c = col;             shift = 4 * step + row * width + col;             if (r < height)                 temp += top_data[base + r * width + c] * filters[fbase + shift];             else                 temp += top_data[base + row * width + col] * filters[fbase + shift];              top_data[base + row * width + col] = temp;         }     } }  int main() {          int n = 1000;     int channel = 3;     int height = 32;     int width = 32;     int wsize = 5;           float *filters_host = (float *)malloc(n * channel * wsize * sizeof(float));     float *top_data_host = (float *)malloc(n * height * width * sizeof(float));                 float *filters_device, *top_data_device;      cudaMalloc((void **)&filters_device, n * channel * wsize * sizeof(float));     cudaMalloc((void **)&top_data_device, n * height * width * sizeof(float));                 dim3 gridDim((n + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK, 1, 1);     dim3 blockDim(THREADS_PER_BLOCK, 1, 1);      nlf_left_forward<<<gridDim, blockDim>>>(n, filters_device, channel, height, width,                                             wsize, top_data_device);                  free(filters_host);     free(top_data_host);      cudaFree(filters_device);     cudaFree(top_data_device);      return 0; }   "
    }
]